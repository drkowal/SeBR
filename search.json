[{"path":"https://drkowal.github.io/SeBR/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 SeBR authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://drkowal.github.io/SeBR/articles/SeBR.html","id":"background-semiparametric-regression-via-data-transformations","dir":"Articles","previous_headings":"","what":"Background: semiparametric regression via data transformations","title":"Introduction to SeBR","text":"Data transformations useful companion parametric regression models. well-chosen learned transformation can greatly enhance applicability given model, especially data irregular marginal features (e.g., multimodality, skewness) various data domains (e.g., real-valued, positive, compactly-supported data). interested providing fully Bayesian inference semiparametric regression models incorporate (1) unknown data transformation (2) useful parametric regression model. paired data \\(\\{x_i, y_i\\}_{=1}^n\\) \\(x_i \\\\mathbb{R}^p\\) \\(y \\\\mathcal{Y} \\subseteq \\mathbb{R}\\), consider following class models: \\[ g(y_i) = z_i \\] \\[ z_i  \\stackrel{indep}{\\sim} P_{Z \\mid \\theta, X = x_i} \\] , \\(g\\) (monotone increasing) data transformation learned, \\(P_{Z \\mid \\theta, X}\\) may considered core parametric regression model indexed unknown parameters \\(\\theta\\). Examples. focus following important special cases \\(P_{Z \\mid \\theta, X}\\): linear model natural starting point: \\[ z_i = x_i'\\theta + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma_\\epsilon^2) \\] transformation \\(g\\) broadens applicability useful class models, including positive compactly-supported data (see ), \\(P_{Z \\mid \\theta, X=x} = N(x'\\theta, \\sigma_\\epsilon^2)\\). quantile regression model replaces Gaussian assumption linear model asymmetric Laplace distribution (ALD) target \\(\\tau\\)th quantile \\(z\\) \\(x\\), equivalently, \\(g^{-1}(\\tau)\\)th quantile \\(y\\) \\(x\\). ALD quite often poor model real data, especially \\(\\tau\\) near zero one. transformation \\(g\\) offers pathway significantly improve model adequacy, still targeting desired quantile data. Gaussian process (GP) model generalizes linear model include nonparametric regression function, \\[ z_i = f_\\theta(x_i) + \\epsilon_i, \\quad  \\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma_\\epsilon^2) \\] \\(f_\\theta\\) GP \\(\\theta\\) parameterizes mean covariance functions. Although GPs offer substantial flexibility regression function \\(f_\\theta\\), model may inadequate \\(y\\) irregular marginal features restricted domain (e.g., positive compact). Challenges: goal provide fully Bayesian posterior inference unknowns \\((g, \\theta)\\) posterior predictive inference future/unobserved data \\(\\tilde y(x)\\). prefer model algorithm offer () flexible modeling \\(g\\) (ii) efficient posterior predictive computations. Innovations: approach (https://arxiv.org/abs/2306.05498) specifies nonparametric model \\(g\\), yet also provides Monte Carlo (MCMC) sampling posterior predictive distributions. result, control approximation accuracy via number simulations, require lengthy runs, burn-periods, convergence diagnostics, inefficiency factors accompany MCMC. Monte Carlo sampling typically quite fast.","code":""},{"path":"https://drkowal.github.io/SeBR/articles/SeBR.html","id":"using-sebr","dir":"Articles","previous_headings":"","what":"Using SeBR","title":"Introduction to SeBR","text":"R package SeBR installed loaded follows: main functions SeBR : sblm(): Monte Carlo sampling posterior predictive inference semiparametric Bayesian linear model; sbsm(): Monte Carlo sampling posterior predictive inference semiparametric Bayesian spline model, replaces linear model spline nonlinear modeling \\(x \\\\mathbb{R}\\); sbqr(): blocked Gibbs sampling posterior predictive inference semiparametric Bayesian quantile regression; sbgp(): Monte Carlo sampling predictive inference semiparametric Bayesian Gaussian process model. function returns point estimate \\(\\theta\\) (coefficients), posterior samples transformation \\(g\\) (post_g), posterior predictive samples \\(\\tilde y(x)\\) specified testing points \\(X_{test}\\) (post_ytilde), well function-specific quantities (e.g., posterior draws \\(\\theta\\), post_theta). Note: package also includes Box-Cox variants functions, .e., restricting \\(g\\) (signed) Box-Cox parametric family \\(g(t; \\lambda) = \\{\\mbox{sign}(t) \\vert t \\vert^\\lambda - 1\\}/\\lambda\\) known unknown \\(\\lambda\\). parametric transformation less flexible, especially irregular marginals restricted domains, requires MCMC sampling. functions (e.g., blm_bc(), etc.) primarily benchmarking.","code":"# install.packages(\"devtools\") # devtools::install_github(\"drkowal/SeBR\") library(SeBR)"},{"path":"https://drkowal.github.io/SeBR/articles/SeBR.html","id":"semiparametric-bayesian-linear-models-with-sblm","dir":"Articles","previous_headings":"","what":"Semiparametric Bayesian linear models with sblm","title":"Introduction to SeBR","text":"simulate data transformed linear model: sblm() quickly produces Monte Carlo samples \\((\\theta, g, \\tilde y(X_{test}))\\) semiparametric Bayesian linear model: Monte Carlo (MCMC) samples, require algorithm diagnostics. First, check model adequacy using posterior predictive diagnostics. Specifically, compute empirical CDF y_test (black) simulated testing predictive dataset post_ytilde (gray):  Despite challenging features marginal distribution, proposed model appears adequate. Remark: Posterior predictive diagnostics require training/testing splits typically performed -sample. X_test left unspecified sblm, posterior predictive draws given X can compared y. Naturally, --sample diagnostics offer rigorous check. Next, evaluate predictive ability testing dataset computing plotting --sample prediction intervals X_test comparing y_test. built-function :  --sample predictive distributions well-calibrated. Finally, summarize posterior inference transformation \\(g\\) regression coefficients \\(\\theta\\) compare ground truth values. First, plot posterior draws \\(g\\) (gray), posterior mean \\(g\\) (black), true transformation (triangles): posterior distribution \\(g\\) accurately matches true transformation. regression coefficients also recovered: Remark: location-scale data-generating process model may match exactly. Thus, use correlations compare regression coefficients \\(\\theta\\) (omitting intercept) apply location-scale shifts transformations \\(g\\) ensure comparability. byproduct simulated data setting matter real data analysis. Note: Try repeating exercise blm_bc() place sblm(). Box-Cox transformation recover transformation accurately, model diagnostics alarming, predictions deteriorate substantially.","code":"set.seed(123) # for reproducibility  # Simulate data from a transformed linear model: dat = simulate_tlm(n = 200,  # number of observations                    p = 10,   # number of covariates                     g_type = 'step' # type of transformation (here, positive data)                    ) # Training data: y = dat$y; X = dat$X   # Testing data: y_test = dat$y_test; X_test = dat$X_test # Fit the semiparametric Bayesian linear model: fit = sblm(y = y,             X = X,             X_test = X_test) #> [1] \"7 seconds remaining\" #> [1] \"6 seconds remaining\" #> [1] \"3 seconds remaining\" #> [1] \"Total time:  8 seconds\"  names(fit) # what is returned #>  [1] \"coefficients\"  \"post_theta\"    \"post_ytilde\"   \"post_g\"        #>  [5] \"model\"         \"y\"             \"X\"             \"X_test\"        #>  [9] \"psi\"           \"approx_g\"      \"sigma_epsilon\" # Evaluate posterior predictive means and intervals on the testing data: plot_pptest(fit$post_ytilde,              y_test,              alpha_level = 0.10) # coverage should be >= 90% #> [1] 0.929 # Check: correlation with true coefficients cor(dat$beta_true[-1],     coef(fit)[-1]) # excluding the intercept #> [1] 0.9431343"},{"path":"https://drkowal.github.io/SeBR/articles/SeBR.html","id":"semiparametric-bayesian-quantile-regression-with-sbqr","dir":"Articles","previous_headings":"","what":"Semiparametric Bayesian quantile regression with sbqr","title":"Introduction to SeBR","text":"now consider Bayesian quantile regression, specifies linear model ALD errors. First, simulate data heteroskedastic linear model. Heteroskedasticity often produces conclusions differ traditional mean regression. , include transformation, data-generating process implicitly favor approach traditional Bayesian quantile regression (.e., \\(g(t) = t\\) identity). Now, fit two Bayesian quantile regression models: traditional version without transformation (bqr()) proposed alternative (sbqr()). target \\(\\tau = 0.05\\) quantile. model fits, evaluate posterior predictive diagnostics . Specifically, compute empirical CDF y_test (black) simulated testing predictive dataset post_ytilde sbqr (gray) bqr (red): Without transformation, Bayesian quantile regression model good model data. learned transformation completely resolves model inadequacy—even though transformation present data-generating process. Finally, can asses quantile estimates testing data. First, consider bqr:  Recall quantile regression models \\(\\tau\\), expect asymmetric y_test. --sample empirical quantile 0.026 (target \\(\\tau = 0.05\\)) 90% prediction interval coverage 0.978. Repeat evaluation sbqr:  Now --sample empirical quantile 0.034 90% prediction interval coverage 0.968. sbqr better calibrated \\(\\tau\\), methods slightly overconservative prediction interval coverage. However, sbqr produce significantly smaller prediction intervals maintaining conservative coverage, thus provides powerful precise inference. Note: try quantiles, \\(\\tau \\\\{0.25, 0.5\\}\\). \\(\\tau\\) approaches 0.5 (.e., median regression), problem becomes easier models better calibrated.","code":"# Simulate data from a heteroskedastic linear model (no transformation): dat = simulate_tlm(n = 200,  # number of observations                    p = 10,   # number of covariates                     g_type = 'box-cox', lambda = 1, # no transformation                    heterosked = TRUE # heteroskedastic errors                    ) # Training data: y = dat$y; X = dat$X   # Testing data: y_test = dat$y_test; X_test = dat$X_test # Quantile to target: tau = 0.05  # (Traditional) Bayesian quantile regression: fit_bqr = bqr(y = y,             X = X,             tau = tau,             X_test = X_test,            verbose = FALSE  # omit printout )  # Semiparametric Bayesian quantile regression: fit = sbqr(y = y,             X = X,             tau = tau,             X_test = X_test,            verbose = FALSE # omit printout )        names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ytilde\"   #>  [5] \"post_qtau\"     \"post_g\"        \"model\"         \"y\"             #>  [9] \"X\"             \"X_test\"        \"psi\"           \"approx_g\"      #> [13] \"tau\" # Quantile point estimates: q_hat_bqr = fitted(fit_bqr)   # Empirical quantiles on testing data: (emp_quant_bqr = mean(q_hat_bqr >= y_test)) #> [1] 0.026  # Evaluate posterior predictive means and intervals on the testing data: (emp_cov_bqr = plot_pptest(fit_bqr$post_ytilde,                             y_test,                             alpha_level = 0.10)) #> [1] 0.978 # Quantile point estimates: q_hat = fitted(fit)   # Empirical quantiles on testing data: (emp_quant_sbqr = mean(q_hat >= y_test)) #> [1] 0.034  # Evaluate posterior predictive means and intervals on the testing data: (emp_cov_sbqr = plot_pptest(fit$post_ytilde,                              y_test,                              alpha_level = 0.10)) #> [1] 0.968"},{"path":"https://drkowal.github.io/SeBR/articles/SeBR.html","id":"semiparametric-bayesian-gaussian-processes-with-sbgp","dir":"Articles","previous_headings":"","what":"Semiparametric Bayesian Gaussian processes with sbgp","title":"Introduction to SeBR","text":"Consider challenging scenario () nonlinear regression function \\(x \\\\mathbb{R}\\) (ii) Beta marginals, support \\(\\mathcal{Y} = [0,1]\\). Simulate data accordingly:  highlight challenges problem, first consider Box-Cox-transformed GP evaluate --sample predictive performance:  Box-Cox transformation adds flexibility GP, insufficient data. prediction intervals unnecessarily wide respect support \\(\\mathcal{Y} = [0,1]\\), estimated mean function fully capture trend data. Now fit semiparametric Bayesian GP model: Evaluate --sample predictive performance testing data:  Unlike Box-Cox version, sbgp respects support data \\(\\mathcal{Y} = [0,1]\\), captures trend, provides narrower intervals (average widths 0.215 compared 0.267) better coverage (0.955 sbgp 0.894 Box-Cox). Despite significant complexities data, sbgp performs quite well ---box: nonlinearity modeled adequately; support data enforced automatically; --sample prediction intervals sharp calibrated; computations fast. Note: sbgp also applies \\(x \\\\mathbb{R}^p\\) \\(p >1\\), spatial spatio-temporal data. cases may require careful consideration mean covariance functions: default mean function linear regression intercept , default covariance function isotropic Matern function. However, many options available (inherited GpGp package).","code":"# Training data: n = 200 # sample size x = seq(0, 1, length = n) # observation points  # Testing data: n_test = 1000  x_test = seq(0, 1, length = n_test)   # True inverse transformation: g_inv_true = function(z)    qbeta(pnorm(z),          shape1 = 0.5,          shape2 = 0.1) # approx Beta(0.5, 0.1) marginals  # Training observations: y = g_inv_true(   sin(2*pi*x) + sin(4*pi*x) + .25*rnorm(n)              )   # Testing observations: y_test = g_inv_true(   sin(2*pi*x_test) + sin(4*pi*x_test) + .25*rnorm(n)              )   plot(x_test, y_test,       xlab = 'x', ylab = 'y',      main = \"Training (gray) and testing (black) data\") lines(x, y, type='p', col='gray', pch = 2) # Fit the Box-Cox Gaussian process model: fit_bc = bgp_bc(y = y,             locs = x,            locs_test = x_test) #> [1] \"Initial GP fit...\" #> [1] \"Updated GP fit...\"  # Fitted values and on the testing data: y_hat_bc = colMeans(fit_bc$post_ytilde)  # 90% prediction intervals on the testing data: pi_y_bc = t(apply(fit_bc$post_ytilde, 2, quantile, c(0.05, .95)))   # Average PI width: (width_bc = mean(pi_y_bc[,2] - pi_y_bc[,1])) #> [1] 0.2668766  # Empirical PI coverage: (emp_cov_bc = mean((pi_y_bc[,1] <= y_test)*(pi_y_bc[,2] >= y_test))) #> [1] 0.894  # Plot these together with the actual testing points: plot(x_test, y_test, type='n',       ylim = range(pi_y_bc, y_test), xlab = 'x', ylab = 'y',       main = paste('Fitted values and prediction intervals: \\n Box-Cox Gaussian process'))  # Add the intervals: polygon(c(x_test, rev(x_test)),         c(pi_y_bc[,2], rev(pi_y_bc[,1])),         col='gray', border=NA) lines(x_test, y_test, type='p') # actual values lines(x_test, y_hat_bc, lwd = 3) # fitted values # Fit the semiparametric Gaussian process model: fit = sbgp(y = y,             locs = x,            locs_test = x_test) #> [1] \"Initial GP fit...\" #> [1] \"Updated GP fit...\" #> [1] \"Sampling...\" #> [1] \"Done!\"  names(fit) # what is returned #> [1] \"coefficients\"  \"fit_gp\"        \"post_ytilde\"   \"post_g\"        #> [5] \"model\"         \"y\"             \"X\"             \"approx_g\"      #> [9] \"sigma_epsilon\"  coef(fit) # estimated regression coefficients (here, just an intercept) #> [1] 0.02256625 # Fitted values and on the testing data: y_hat = colMeans(fit$post_ytilde)  # 90% prediction intervals on the testing data: pi_y = t(apply(fit$post_ytilde, 2, quantile, c(0.05, .95)))   # Average PI width: (width = mean(pi_y[,2] - pi_y[,1])) #> [1] 0.2150783  # Empirical PI coverage: (emp_cov = mean((pi_y[,1] <= y_test)*(pi_y[,2] >= y_test))) #> [1] 0.955  # Plot these together with the actual testing points: plot(x_test, y_test, type='n',       ylim = range(pi_y, y_test), xlab = 'x', ylab = 'y',       main = paste('Fitted values and prediction intervals: \\n semiparametric Gaussian process'))  # Add the intervals: polygon(c(x_test, rev(x_test)),         c(pi_y[,2], rev(pi_y[,1])),         col='gray', border=NA) lines(x_test, y_test, type='p') # actual values lines(x_test, y_hat, lwd = 3) # fitted values"},{"path":"https://drkowal.github.io/SeBR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Dan Kowal. Maintainer.","code":""},{"path":"https://drkowal.github.io/SeBR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Kowal D (2023). SeBR: Semiparametric Bayesian Regression Analysis. https://github.com/drkowal/SeBR, https://drkowal.github.io/SeBR/.","code":"@Manual{,   title = {SeBR: Semiparametric Bayesian Regression Analysis},   author = {Dan Kowal},   year = {2023},   note = {https://github.com/drkowal/SeBR, https://drkowal.github.io/SeBR/}, }"},{"path":"https://drkowal.github.io/SeBR/index.html","id":"sebr-semiparametric-bayesian-regression","dir":"","previous_headings":"","what":"Semiparametric Bayesian Regression Analysis","title":"Semiparametric Bayesian Regression Analysis","text":"Overview. Data transformations useful companion parametric regression models. well-chosen learned transformation can greatly enhance applicability given model, especially data irregular marginal features (e.g., multimodality, skewness) various data domains (e.g., real-valued, positive, compactly-supported data). Given paired data (xi,yi)  = 1, …, n, SeBR implements efficient fully Bayesian inference semiparametric regression models incorporate (1) unknown data transformation g(yi) = zi (2) useful parametric regression model $$ z_i  \\stackrel{indep}{\\sim} P_{Z \\mid \\theta, X = x_i} $$ unknown parameters θ. Examples. focus following important special cases PZ ∣ θ, X: linear model natural starting point: $$ z_i = x_i'\\theta + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma_\\epsilon^2) $$ transformation g broadens applicability useful class models, including positive compactly-supported data, PZ ∣ θ, X = x = N(x′θ,σϵ2). quantile regression model replaces Gaussian assumption linear model asymmetric Laplace distribution (ALD) $$ z_i = x_i'\\theta + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} ALD(\\tau) $$ target τth quantile z x, equivalently, g−1(τ)th quantile y x. ALD quite often poor model real data, especially τ near zero one. transformation g offers pathway significantly improve model adequacy, still targeting desired quantile data. Gaussian process (GP) model generalizes linear model include nonparametric regression function, $$ z_i = f_\\theta(x_i) + \\epsilon_i, \\quad  \\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma_\\epsilon^2) $$ fθ GP θ parameterizes mean covariance functions. Although GPs offer substantial flexibility regression function fθ, model may inadequate y irregular marginal features restricted domain (e.g., positive compact). Challenges: goal provide fully Bayesian posterior inference unknowns (g,θ) posterior predictive inference future/unobserved data ỹ(x). prefer model algorithm offer () flexible modeling g (ii) efficient posterior predictive computations. Innovations: approach (https://arxiv.org/abs/2306.05498) specifies nonparametric model g, yet also provides Monte Carlo (MCMC) sampling posterior predictive distributions. result, control approximation accuracy via number simulations, require lengthy runs, burn-periods, convergence diagnostics, inefficiency factors accompany MCMC. Monte Carlo sampling typically quite fast.","code":""},{"path":"https://drkowal.github.io/SeBR/index.html","id":"using-sebr","dir":"","previous_headings":"","what":"Using SeBR","title":"Semiparametric Bayesian Regression Analysis","text":"package SeBR installed loaded follows: main functions SeBR : sblm(): Monte Carlo sampling posterior predictive inference semiparametric Bayesian linear model; sbsm(): Monte Carlo sampling posterior predictive inference semiparametric Bayesian spline model, replaces linear model spline nonlinear modeling x ∈ ℝ; sbqr(): blocked Gibbs sampling posterior predictive inference semiparametric Bayesian quantile regression; sbgp(): Monte Carlo sampling predictive inference semiparametric Bayesian Gaussian process model. function returns point estimate θ (coefficients), posterior samples transformation g (post_g), posterior predictive samples ỹ(x) specified testing points Xtest (post_ytilde), well function-specific quantities (e.g., posterior draws θ, post_theta). Note: package also includes Box-Cox variants functions, .e., restricting g (signed) Box-Cox parametric family g(t;λ) = {sign(t)|t|λ − 1}/λ known unknown λ. parametric transformation less flexible, especially irregular marginals restricted domains, requires MCMC sampling. functions (e.g., blm_bc(), etc.) primarily benchmarking.","code":"# install.packages(\"devtools\") # devtools::install_github(\"drkowal/SeBR\") library(SeBR)"},{"path":"https://drkowal.github.io/SeBR/reference/Fz_fun.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the latent data CDF — Fz_fun","title":"Compute the latent data CDF — Fz_fun","text":"Assuming Gaussian latent data distribution (given x), compute CDF grid points","code":""},{"path":"https://drkowal.github.io/SeBR/reference/Fz_fun.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the latent data CDF — Fz_fun","text":"","code":"Fz_fun(z, weights = NULL, mean_vec = NULL, sd_vec)"},{"path":"https://drkowal.github.io/SeBR/reference/Fz_fun.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the latent data CDF — Fz_fun","text":"z vector points CDF z evaluated weights n-dimensional vector weights; NULL, assume 1/n mean_vec n-dimensional vector means; NULL, assume mean zero sd_vec n-dimensional vector standard deviations","code":""},{"path":"https://drkowal.github.io/SeBR/reference/Fz_fun.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the latent data CDF — Fz_fun","text":"CDF z evaluated z","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian Gaussian processes with a Box-Cox transformation — bgp_bc","title":"Bayesian Gaussian processes with a Box-Cox transformation — bgp_bc","text":"MCMC sampling Bayesian Gaussian process regression (known unknown) Box-Cox transformation.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian Gaussian processes with a Box-Cox transformation — bgp_bc","text":"","code":"bgp_bc(   y,   locs,   X = NULL,   covfun_name = \"matern_isotropic\",   locs_test = locs,   X_test = NULL,   nn = 30,   emp_bayes = TRUE,   lambda = NULL,   sample_lambda = TRUE,   nsave = 1000,   nburn = 1000,   nskip = 0 )"},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian Gaussian processes with a Box-Cox transformation — bgp_bc","text":"y n x 1 response vector locs n x d matrix locations X n x p design matrix; unspecified, use intercept covfun_name string name covariance function; see ?GpGp locs_test n_test x d matrix locations predictions needed; default locs X_test n_test x p design matrix test data; default X nn number nearest neighbors use; default 30 (larger values improve approximation increase computing cost) emp_bayes logical; TRUE, use (faster!) empirical Bayes approach estimating mean function lambda Box-Cox transformation; NULL, estimate parameter sample_lambda logical; TRUE, sample lambda, otherwise use fixed value lambda MLE (lambda unspecified) nsave number MCMC iterations save nburn number MCMC iterations discard nskip number MCMC iterations skip saving iterations, .e., save every (nskip + 1)th draw","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian Gaussian processes with a Box-Cox transformation — bgp_bc","text":"list following elements: coefficients posterior mean regression coefficients fit_gp fitted GpGp_fit object, includes covariance parameter estimates model information post_ytilde: nsave x n_test samples posterior predictive distribution locs_test post_g: nsave posterior samples transformation evaluated unique y values post_lambda nsave posterior samples lambda model: model fit (, sbgp_bc) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian Gaussian processes with a Box-Cox transformation — bgp_bc","text":"function provides Bayesian inference transformed Gaussian processes. transformation parametric Box-Cox family, one parameter lambda. parameter may fixed advanced learned data. computational efficiency, Gaussian process parameters fixed point estimates, latent Gaussian process sampled emp_bayes = FALSE.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bayesian Gaussian processes with a Box-Cox transformation — bgp_bc","text":"Box-Cox transformations may useful cases, general recommend nonparametric transformation (Monte Carlo, MCMC sampling) sbgp.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian Gaussian processes with a Box-Cox transformation — bgp_bc","text":"","code":"# Simulate some data: n = 200 # sample size x = seq(0, 1, length = n) # observation points  # Transform a noisy, periodic function: y = g_inv_bc(   sin(2*pi*x) + sin(4*pi*x) + rnorm(n, sd = .5),              lambda = .5) # Signed square-root transformation  # Fit a Bayesian Gaussian process with Box-Cox transformation: fit = bgp_bc(y = y, locs = x) #> [1] \"Initial GP fit...\" #> [1] \"Updated GP fit...\" names(fit) # what is returned #> [1] \"coefficients\" \"fit_gp\"       \"post_ytilde\"  \"post_g\"       \"post_lambda\"  #> [6] \"model\"        \"y\"            \"X\"            coef(fit) # estimated regression coefficients (here, just an intercept) #> [1] 0.1972943 class(fit$fit_gp) # the GpGp object is also returned #> [1] \"GpGp_fit\" round(quantile(fit$post_lambda), 3) # summary of unknown Box-Cox parameter #>    0%   25%   50%   75%  100%  #> 0.732 0.809 0.833 0.857 0.938   # Plot the model predictions (point and interval estimates): pi_y = t(apply(fit$post_ytilde, 2, quantile, c(0.05, .95))) # 90% PI plot(x, y, type='n', ylim = range(pi_y,y),      xlab = 'x', ylab = 'y', main = paste('Fitted values and prediction intervals')) polygon(c(x, rev(x)),c(pi_y[,2], rev(pi_y[,1])),col='gray', border=NA) lines(x, y, type='p') lines(x, colMeans(fit$post_ytilde), lwd = 3)"},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian linear model with a Box-Cox transformation — blm_bc","title":"Bayesian linear model with a Box-Cox transformation — blm_bc","text":"MCMC sampling Bayesian linear regression (known unknown) Box-Cox transformation. g-prior assumed regression coefficients.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian linear model with a Box-Cox transformation — blm_bc","text":"","code":"blm_bc(   y,   X,   X_test = X,   psi = length(y),   lambda = NULL,   sample_lambda = TRUE,   nsave = 1000,   nburn = 1000,   nskip = 0,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian linear model with a Box-Cox transformation — blm_bc","text":"y n x 1 vector observed counts X n x p matrix predictors X_test n_test x p matrix predictors test data; default observed covariates X psi prior variance (g-prior) lambda Box-Cox transformation; NULL, estimate parameter sample_lambda logical; TRUE, sample lambda, otherwise use fixed value lambda MLE (lambda unspecified) nsave number MCMC iterations save nburn number MCMC iterations discard nskip number MCMC iterations skip saving iterations, .e., save every (nskip + 1)th draw verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian linear model with a Box-Cox transformation — blm_bc","text":"list following elements: coefficients posterior mean regression coefficients post_theta: nsave x p samples posterior distribution regression coefficients post_ytilde: nsave x n_test samples posterior predictive distribution test points X_test post_g: nsave posterior samples transformation evaluated unique y values post_lambda nsave posterior samples lambda post_sigma nsave posterior samples sigma model: model fit (, sblm_bc) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian linear model with a Box-Cox transformation — blm_bc","text":"function provides fully Bayesian inference transformed linear model via MCMC sampling. transformation parametric Box-Cox family, one parameter lambda. parameter may fixed advanced learned data.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bayesian linear model with a Box-Cox transformation — blm_bc","text":"Box-Cox transformations may useful cases, general recommend nonparametric transformation (Monte Carlo, MCMC sampling) sblm.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian linear model with a Box-Cox transformation — blm_bc","text":"","code":"# Simulate some data: dat = simulate_tlm(n = 200, p = 10, g_type = 'step') y = dat$y; X = dat$X # training data y_test = dat$y_test; X_test = dat$X_test # testing data  hist(y, breaks = 25) # marginal distribution   # Fit the Bayesian linear model with a Box-Cox transformation: fit = blm_bc(y = y, X = X, X_test = X_test) #> [1] \"1 seconds remaining\" #> [1] \"Total time:  1 seconds\" names(fit) # what is returned #>  [1] \"coefficients\" \"post_theta\"   \"post_ytilde\"  \"post_g\"       \"post_lambda\"  #>  [6] \"post_sigma\"   \"model\"        \"y\"            \"X\"            \"X_test\"       #> [11] \"psi\"          round(quantile(fit$post_lambda), 3) # summary of unknown Box-Cox parameter #>    0%   25%   50%   75%  100%  #> 0.000 0.001 0.003 0.005 0.031"},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian quantile regression — bqr","title":"Bayesian quantile regression — bqr","text":"MCMC sampling Bayesian quantile regression. asymmetric Laplace distribution assumed errors, regression models targets specified quantile. g-prior assumed regression coefficients.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian quantile regression — bqr","text":"","code":"bqr(   y,   X,   tau = 0.5,   X_test = X,   psi = length(y),   nsave = 1000,   nburn = 1000,   nskip = 0,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian quantile regression — bqr","text":"y n x 1 vector observed counts X n x p matrix predictors tau target quantile (zero one) X_test n_test x p matrix predictors test data; default observed covariates X psi prior variance (g-prior) nsave number MCMC iterations save nburn number MCMC iterations discard nskip number MCMC iterations skip saving iterations, .e., save every (nskip + 1)th draw verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian quantile regression — bqr","text":"list following elements: coefficients posterior mean regression coefficients fitted.values estimated tauth quantile test points X_test post_theta: nsave x p samples posterior distribution regression coefficients post_ytilde: nsave x n_test samples posterior predictive distribution test points X_test model: model fit (, bqr) well arguments passed","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bayesian quantile regression — bqr","text":"asymmetric Laplace distribution advantageous links regression model (X%*%theta) pre-specified quantile (tau). However, often poor model observed data, semiparametric version sbqr recommended general.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian quantile regression — bqr","text":"","code":"# Simulate some heteroskedastic data (no transformation): dat = simulate_tlm(n = 200, p = 10, g_type = 'box-cox', heterosked = TRUE, lambda = 1) y = dat$y; X = dat$X # training data y_test = dat$y_test; X_test = dat$X_test # testing data  # Target this quantile: tau = 0.05  # Fit the Bayesian quantile regression model: fit = bqr(y = y, X = X, tau = tau, X_test = X_test) #> [1] \"1 seconds remaining\" #> [1] \"Total time:  1 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ytilde\"   #>  [5] \"model\"         \"y\"             \"X\"             \"X_test\"        #>  [9] \"psi\"           \"tau\"            # Posterior predictive checks on testing data: empirical CDF y0 = sort(unique(y_test)) plot(y0, y0, type='n', ylim = c(0,1),      xlab='y', ylab='F_y', main = 'Posterior predictive ECDF') temp = sapply(1:nrow(fit$post_ytilde), function(s)   lines(y0, ecdf(fit$post_ytilde[s,])(y0), # ECDF of posterior predictive draws         col='gray', type ='s')) lines(y0, ecdf(y_test)(y0),  # ECDF of testing data      col='black', type = 's', lwd = 3)   # The posterior predictive checks usually do not pass! # try ?sbqr instead..."},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian spline model with a Box-Cox transformation — bsm_bc","title":"Bayesian spline model with a Box-Cox transformation — bsm_bc","text":"MCMC sampling Bayesian spline regression (known unknown) Box-Cox transformation.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian spline model with a Box-Cox transformation — bsm_bc","text":"","code":"bsm_bc(   y,   x = NULL,   x_test = NULL,   psi = NULL,   lambda = NULL,   sample_lambda = TRUE,   nsave = 1000,   nburn = 1000,   nskip = 0,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian spline model with a Box-Cox transformation — bsm_bc","text":"y n x 1 vector observed counts x n x 1 vector observation points; NULL, assume equally-spaced [0,1] x_test n_test x 1 vector testing points; NULL, assume equal x psi prior variance (inverse smoothing parameter); NULL, sample parameter lambda Box-Cox transformation; NULL, estimate parameter sample_lambda logical; TRUE, sample lambda, otherwise use fixed value lambda MLE (lambda unspecified) nsave number MCMC iterations save nburn number MCMC iterations discard nskip number MCMC iterations skip saving iterations, .e., save every (nskip + 1)th draw verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian spline model with a Box-Cox transformation — bsm_bc","text":"list following elements: coefficients posterior mean regression coefficients post_theta: nsave x p samples posterior distribution regression coefficients post_ytilde: nsave x n_test samples posterior predictive distribution x_test post_g: nsave posterior samples transformation evaluated unique y values post_lambda nsave posterior samples lambda model: model fit (, sbsm_bc) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian spline model with a Box-Cox transformation — bsm_bc","text":"function provides fully Bayesian inference transformed spline model via MCMC sampling. transformation parametric Box-Cox family, one parameter lambda. parameter may fixed advanced learned data.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bayesian spline model with a Box-Cox transformation — bsm_bc","text":"Box-Cox transformations may useful cases, general recommend nonparametric transformation (Monte Carlo, MCMC sampling) sbsm.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian spline model with a Box-Cox transformation — bsm_bc","text":"","code":"# Simulate some data: n = 500 # sample size x = sort(runif(n)) # observation points  # Transform a noisy, periodic function: y = g_inv_bc(   sin(2*pi*x) + sin(4*pi*x) + rnorm(n, sd = .5),              lambda = .5) # Signed square-root transformation  # Fit the Bayesian spline model with a Box-Cox transformation: fit = bsm_bc(y = y, x = x) #> [1] \"1 seconds remaining\" #> [1] \"Total time:  1 seconds\" names(fit) # what is returned #> [1] \"coefficients\" \"post_theta\"   \"post_ytilde\"  \"post_g\"       \"post_lambda\"  #> [6] \"model\"        \"y\"            \"X\"            \"psi\"          round(quantile(fit$post_lambda), 3) # summary of unknown Box-Cox parameter #>    0%   25%   50%   75%  100%  #> 0.554 0.604 0.620 0.638 0.715   # Plot the model predictions (point and interval estimates): pi_y = t(apply(fit$post_ytilde, 2, quantile, c(0.05, .95))) # 90% PI plot(x, y, type='n', ylim = range(pi_y,y),      xlab = 'x', ylab = 'y', main = paste('Fitted values and prediction intervals')) polygon(c(x, rev(x)),c(pi_y[,2], rev(pi_y[,1])),col='gray', border=NA) lines(x, y, type='p') lines(x, colMeans(fit$post_ytilde), lwd = 3)"},{"path":"https://drkowal.github.io/SeBR/reference/computeTimeRemaining.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate the remaining time in the MCMC based on previous samples — computeTimeRemaining","title":"Estimate the remaining time in the MCMC based on previous samples — computeTimeRemaining","text":"Estimate remaining time MCMC based previous samples","code":""},{"path":"https://drkowal.github.io/SeBR/reference/computeTimeRemaining.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate the remaining time in the MCMC based on previous samples — computeTimeRemaining","text":"","code":"computeTimeRemaining(nsi, timer0, nsims, nrep = 1000)"},{"path":"https://drkowal.github.io/SeBR/reference/computeTimeRemaining.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate the remaining time in the MCMC based on previous samples — computeTimeRemaining","text":"nsi Current iteration timer0 Initial timer value, returned proc.time()[3] nsims Total number simulations nrep Print estimated time remaining every nrep iterations","code":""},{"path":"https://drkowal.github.io/SeBR/reference/computeTimeRemaining.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate the remaining time in the MCMC based on previous samples — computeTimeRemaining","text":"Table summary statistics using function summary","code":""},{"path":"https://drkowal.github.io/SeBR/reference/contract_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Grid contraction — contract_grid","title":"Grid contraction — contract_grid","text":"Contract grid evaluation points exceed threshold. removes corresponding z values. can add points back achieve (approximate) length.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/contract_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Grid contraction — contract_grid","text":"","code":"contract_grid(z, Fz, lower, upper, add_back = TRUE, monotone = TRUE)"},{"path":"https://drkowal.github.io/SeBR/reference/contract_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Grid contraction — contract_grid","text":"z grid points (ordered) Fz function evaluated grid points lower lower threshold check Fz upper upper threshold check Fz add_back logical; true, expand grid () original size monotone logical; true, enforce monotonicity expanded grid","code":""},{"path":"https://drkowal.github.io/SeBR/reference/contract_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Grid contraction — contract_grid","text":"list containing grid points z (interpolated) function Fz points","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":null,"dir":"Reference","previous_headings":"","what":"Box-Cox transformation — g_bc","title":"Box-Cox transformation — g_bc","text":"Evaluate Box-Cox transformation, scaled power transformation preserve continuity index lambda zero. Negative values permitted.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Box-Cox transformation — g_bc","text":"","code":"g_bc(t, lambda)"},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Box-Cox transformation — g_bc","text":"t argument(s) evaluate function lambda Box-Cox parameter","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Box-Cox transformation — g_bc","text":"evaluation(s) Box-Cox function given input(s) t.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Box-Cox transformation — g_bc","text":"Special cases include identity transformation (lambda = 1), square-root transformation (lambda = 1/2), log transformation (lambda = 0).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Box-Cox transformation — g_bc","text":"","code":"# Log-transformation: g_bc(1:5, lambda = 0); log(1:5) #> [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 #> [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379  # Square-root transformation: note the shift and scaling g_bc(1:5, lambda = 1/2); sqrt(1:5) #> [1] 0.0000000 0.8284271 1.4641016 2.0000000 2.4721360 #> [1] 1.000000 1.414214 1.732051 2.000000 2.236068"},{"path":"https://drkowal.github.io/SeBR/reference/g_fun.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the transformation — g_fun","title":"Compute the transformation — g_fun","text":"Given CDFs z y, compute smoothed function evaluate transformation","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_fun.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the transformation — g_fun","text":"","code":"g_fun(y, Fy_eval, z, Fz_eval)"},{"path":"https://drkowal.github.io/SeBR/reference/g_fun.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the transformation — g_fun","text":"y vector points CDF y evaluated Fy_eval CDF y evaluated y z vector points CDF z evaluated Fz_eval CDF z evaluated z","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_fun.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the transformation — g_fun","text":"smooth monotone function can used evaluations transformation.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_approx.html","id":null,"dir":"Reference","previous_headings":"","what":"Approximate inverse transformation — g_inv_approx","title":"Approximate inverse transformation — g_inv_approx","text":"Compute inverse function transformation g based grid search.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_approx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Approximate inverse transformation — g_inv_approx","text":"","code":"g_inv_approx(g, t_grid)"},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_approx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Approximate inverse transformation — g_inv_approx","text":"g transformation function t_grid grid arguments evaluate transformation function","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_approx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Approximate inverse transformation — g_inv_approx","text":"function can used evaluations (approximate) inverse transformation function.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_bc.html","id":null,"dir":"Reference","previous_headings":"","what":"Inverse Box-Cox transformation — g_inv_bc","title":"Inverse Box-Cox transformation — g_inv_bc","text":"Evaluate inverse Box-Cox transformation. Negative values permitted.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_bc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inverse Box-Cox transformation — g_inv_bc","text":"","code":"g_inv_bc(s, lambda)"},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_bc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inverse Box-Cox transformation — g_inv_bc","text":"s argument(s) evaluate function lambda Box-Cox parameter","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_bc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Inverse Box-Cox transformation — g_inv_bc","text":"evaluation(s) inverse Box-Cox function given input(s) s.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_bc.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Inverse Box-Cox transformation — g_inv_bc","text":"Special cases include identity transformation (lambda = 1), square-root transformation (lambda = 1/2), log transformation (lambda = 0). #' @examples # (Inverse) log-transformation: g_inv_bc(1:5, lambda = 0); exp(1:5) # (Inverse) square-root transformation: note shift scaling g_inv_bc(1:5, lambda = 1/2); (1:5)^2","code":""},{"path":"https://drkowal.github.io/SeBR/reference/getEffSize.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize effective sample size — getEffSize","title":"Summarize effective sample size — getEffSize","text":"Compute summary statistics effective sample size (ESS) across posterior samples possibly many variables","code":""},{"path":"https://drkowal.github.io/SeBR/reference/getEffSize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize effective sample size — getEffSize","text":"","code":"getEffSize(postX)"},{"path":"https://drkowal.github.io/SeBR/reference/getEffSize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize effective sample size — getEffSize","text":"postX array arbitrary dimension (nsims x ... x ...), nsims number posterior samples","code":""},{"path":"https://drkowal.github.io/SeBR/reference/getEffSize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize effective sample size — getEffSize","text":"Table summary statistics using function summary().","code":""},{"path":"https://drkowal.github.io/SeBR/reference/getEffSize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize effective sample size — getEffSize","text":"","code":"# ESS for iid simulations: rand_iid = rnorm(n = 10^4) getEffSize(rand_iid) #>  var1  #> 10000   # ESS for several AR(1) simulations with coefficients 0.1, 0.2,...,0.9: rand_ar1 = sapply(seq(0.1, 0.9, by = 0.1), function(x) arima.sim(n = 10^4, list(ar = x))) getEffSize(rand_ar1) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>   530.9  1766.3  3279.2  3752.9  5476.0  8218.3"},{"path":"https://drkowal.github.io/SeBR/reference/plot_pptest.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot point and interval predictions on testing data — plot_pptest","title":"Plot point and interval predictions on testing data — plot_pptest","text":"Given posterior predictive samples X_test, plot point interval estimates compare actual testing data y_test.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/plot_pptest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot point and interval predictions on testing data — plot_pptest","text":"","code":"plot_pptest(post_ytilde, y_test, alpha_level = 0.1)"},{"path":"https://drkowal.github.io/SeBR/reference/plot_pptest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot point and interval predictions on testing data — plot_pptest","text":"post_ytilde nsave x n_test samples posterior predictive distribution test points X_test y_test n_test testing points alpha_level alpha-level prediction intervals","code":""},{"path":"https://drkowal.github.io/SeBR/reference/plot_pptest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot point and interval predictions on testing data — plot_pptest","text":"plot testing data, point interval predictions, summary empirical coverage","code":""},{"path":"https://drkowal.github.io/SeBR/reference/rank_approx.html","id":null,"dir":"Reference","previous_headings":"","what":"Rank-based estimation of the linear regression coefficients — rank_approx","title":"Rank-based estimation of the linear regression coefficients — rank_approx","text":"transformed Gaussian linear model, compute point estimates regression coefficients.  approach uses ranks data require transformation, must expand sample size n^2 thus can slow.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/rank_approx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rank-based estimation of the linear regression coefficients — rank_approx","text":"","code":"rank_approx(y, X)"},{"path":"https://drkowal.github.io/SeBR/reference/rank_approx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rank-based estimation of the linear regression coefficients — rank_approx","text":"y n x 1 response vector X n x p matrix predictors (include intercept!)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/rank_approx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rank-based estimation of the linear regression coefficients — rank_approx","text":"estimated linear coefficients","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":null,"dir":"Reference","previous_headings":"","what":"Semiparametric Bayesian Gaussian processes — sbgp","title":"Semiparametric Bayesian Gaussian processes — sbgp","text":"Monte Carlo sampling Bayesian Gaussian process regression unknown (nonparametric) transformation.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Semiparametric Bayesian Gaussian processes — sbgp","text":"","code":"sbgp(   y,   locs,   X = NULL,   covfun_name = \"matern_isotropic\",   locs_test = locs,   X_test = NULL,   nn = 30,   emp_bayes = TRUE,   approx_g = FALSE,   nsave = 1000,   ngrid = 100 )"},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Semiparametric Bayesian Gaussian processes — sbgp","text":"y n x 1 response vector locs n x d matrix locations X n x p design matrix; unspecified, use intercept covfun_name string name covariance function; see ?GpGp locs_test n_test x d matrix locations predictions needed; default locs X_test n_test x p design matrix test data; default X nn number nearest neighbors use; default 30 (larger values improve approximation increase computing cost) emp_bayes logical; TRUE, use (faster!) empirical Bayes approach estimating mean function approx_g logical; TRUE, apply large-sample approximation transformation nsave number Monte Carlo simulations ngrid number grid points inverse approximations","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Semiparametric Bayesian Gaussian processes — sbgp","text":"list following elements: coefficients estimated regression coefficients fit_gp fitted GpGp_fit object, includes covariance parameter estimates model information post_ytilde: nsave x ntest samples posterior predictive distribution locs_test post_g: nsave posterior samples transformation evaluated unique y values model: model fit (, sbgp) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Semiparametric Bayesian Gaussian processes — sbgp","text":"function provides Bayesian inference transformed Gaussian process model using Monte Carlo (MCMC) sampling. transformation modeled unknown learned jointly regression function (unless approx_g = TRUE, uses point approximation). model applies real-valued data, positive data, compactly-supported data (support automatically deduced observed y values). results typically unchanged whether laplace_approx TRUE/FALSE; setting TRUE may reduce sensitivity prior, setting FALSE may speed computations large datasets. computational efficiency, Gaussian process parameters fixed point estimates, latent Gaussian process sampled emp_bayes = FALSE. However, uncertainty term often negligible compared observation errors, transformation serves additional layer robustness.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Semiparametric Bayesian Gaussian processes — sbgp","text":"","code":"# Simulate some data: n = 200 # sample size x = seq(0, 1, length = n) # observation points  # Transform a noisy, periodic function: y = g_inv_bc(   sin(2*pi*x) + sin(4*pi*x) + rnorm(n, sd = .5),              lambda = .5) # Signed square-root transformation  # Fit the semiparametric Bayesian Gaussian process: fit = sbgp(y = y, locs = x) #> [1] \"Initial GP fit...\" #> [1] \"Updated GP fit...\" #> [1] \"Sampling...\" #> [1] \"Done!\" names(fit) # what is returned #> [1] \"coefficients\"  \"fit_gp\"        \"post_ytilde\"   \"post_g\"        #> [5] \"model\"         \"y\"             \"X\"             \"approx_g\"      #> [9] \"sigma_epsilon\" coef(fit) # estimated regression coefficients (here, just an intercept) #> [1] 0.09347218 class(fit$fit_gp) # the GpGp object is also returned #> [1] \"GpGp_fit\"  # Plot the model predictions (point and interval estimates): pi_y = t(apply(fit$post_ytilde, 2, quantile, c(0.05, .95))) # 90% PI plot(x, y, type='n', ylim = range(pi_y,y),      xlab = 'x', ylab = 'y', main = paste('Fitted values and prediction intervals')) polygon(c(x, rev(x)),c(pi_y[,2], rev(pi_y[,1])),col='gray', border=NA) lines(x, y, type='p') lines(x, colMeans(fit$post_ytilde), lwd = 3)"},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":null,"dir":"Reference","previous_headings":"","what":"Semiparametric Bayesian linear model — sblm","title":"Semiparametric Bayesian linear model — sblm","text":"Monte Carlo sampling Bayesian linear regression unknown (nonparametric) transformation. g-prior assumed regression coefficients.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Semiparametric Bayesian linear model — sblm","text":"","code":"sblm(   y,   X,   X_test = X,   psi = length(y),   laplace_approx = TRUE,   approx_g = FALSE,   nsave = 1000,   ngrid = 100,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Semiparametric Bayesian linear model — sblm","text":"y n x 1 response vector X n x p matrix predictors X_test n_test x p matrix predictors test data; default observed covariates X psi prior variance (g-prior) laplace_approx logical; TRUE, use normal approximation posterior definition transformation; otherwise prior used approx_g logical; TRUE, apply large-sample approximation transformation nsave number Monte Carlo simulations ngrid number grid points inverse approximations verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Semiparametric Bayesian linear model — sblm","text":"list following elements: coefficients posterior mean regression coefficients post_theta: nsave x p samples posterior distribution regression coefficients post_ytilde: nsave x n_test samples posterior predictive distribution test points X_test post_g: nsave posterior samples transformation evaluated unique y values model: model fit (, sblm) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Semiparametric Bayesian linear model — sblm","text":"function provides fully Bayesian inference transformed linear model using Monte Carlo (MCMC) sampling. transformation modeled unknown learned jointly regression coefficients (unless approx_g = TRUE, uses point approximation). model applies real-valued data, positive data, compactly-supported data (support automatically deduced observed y values). results typically unchanged whether laplace_approx TRUE/FALSE; setting TRUE may reduce sensitivity prior, setting FALSE may speed computations large datasets.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Semiparametric Bayesian linear model — sblm","text":"","code":"# Simulate some data: dat = simulate_tlm(n = 200, p = 10, g_type = 'step') y = dat$y; X = dat$X # training data y_test = dat$y_test; X_test = dat$X_test # testing data  hist(y, breaks = 25) # marginal distribution   # Fit the semiparametric Bayesian linear model: fit = sblm(y = y, X = X, X_test = X_test) #> [1] \"7 seconds remaining\" #> [1] \"5 seconds remaining\" #> [1] \"2 seconds remaining\" #> [1] \"Total time:  7 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"post_theta\"    \"post_ytilde\"   \"post_g\"        #>  [5] \"model\"         \"y\"             \"X\"             \"X_test\"        #>  [9] \"psi\"           \"approx_g\"      \"sigma_epsilon\"  # Note: this is Monte Carlo sampling, so no need for MCMC diagnostics!  # Evaluate posterior predictive means and intervals on the testing data: plot_pptest(fit$post_ytilde, y_test,             alpha_level = 0.10) # coverage should be about 90%  #> [1] 0.916  # Check: correlation with true coefficients cor(dat$beta_true[-1],     coef(fit)[-1]) # excluding the intercept #> [1] 0.9780206  # Summarize the transformation: y0 = sort(unique(y)) # posterior draws of g are evaluated at the unique y observations plot(y0, fit$post_g[1,], type='n', ylim = range(fit$post_g),      xlab = 'y', ylab = 'g(y)', main = \"Posterior draws of the transformation\") temp = sapply(1:nrow(fit$post_g), function(s)   lines(y0, fit$post_g[s,], col='gray')) # posterior draws lines(y0, colMeans(fit$post_g), lwd = 3) # posterior mean  # Add the true transformation, rescaled for easier comparisons: lines(y,       scale(dat$g_true)*sd(colMeans(fit$post_g)) + mean(colMeans(fit$post_g)), type='p', pch=2) legend('bottomright', c('Truth'), pch = 2) # annotate the true transformation   # Posterior predictive checks on testing data: empirical CDF y0 = sort(unique(y_test)) plot(y0, y0, type='n', ylim = c(0,1),      xlab='y', ylab='F_y', main = 'Posterior predictive ECDF') temp = sapply(1:nrow(fit$post_ytilde), function(s)   lines(y0, ecdf(fit$post_ytilde[s,])(y0), # ECDF of posterior predictive draws         col='gray', type ='s')) lines(y0, ecdf(y_test)(y0),  # ECDF of testing data      col='black', type = 's', lwd = 3)"},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":null,"dir":"Reference","previous_headings":"","what":"Semiparametric Bayesian quantile regression — sbqr","title":"Semiparametric Bayesian quantile regression — sbqr","text":"MCMC sampling Bayesian quantile regression unknown (nonparametric) transformation. Like traditional Bayesian quantile regression, asymmetric Laplace distribution assumed errors, regression models targets specified quantile. However, models often woefully inadequate describing observed data. introduce nonparametric transformation improve model adequacy still providing inference regression coefficients specified quantile. g-prior assumed regression coefficients.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Semiparametric Bayesian quantile regression — sbqr","text":"","code":"sbqr(   y,   X,   tau = 0.5,   X_test = X,   psi = length(y),   laplace_approx = TRUE,   approx_g = FALSE,   nsave = 1000,   nburn = 100,   ngrid = 100,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Semiparametric Bayesian quantile regression — sbqr","text":"y n x 1 response vector X n x p matrix predictors tau target quantile (zero one) X_test n_test x p matrix predictors test data; default observed covariates X psi prior variance (g-prior) laplace_approx logical; TRUE, use normal approximation posterior definition transformation; otherwise prior used approx_g logical; TRUE, apply large-sample approximation transformation nsave number MCMC iterations save nburn number MCMC iterations discard ngrid number grid points inverse approximations verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Semiparametric Bayesian quantile regression — sbqr","text":"list following elements: coefficients posterior mean regression coefficients fitted.values estimated tauth quantile test points X_test post_theta: nsave x p samples posterior distribution regression coefficients post_ytilde: nsave x n_test samples posterior predictive distribution test points X_test post_qtau: nsave x n_test samples tauth conditional quantile test points X_test post_g: nsave posterior samples transformation evaluated unique y values model: model fit (, sbqr) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Semiparametric Bayesian quantile regression — sbqr","text":"function provides fully Bayesian inference transformed quantile linear model. transformation modeled unknown learned jointly regression coefficients (unless approx_g = TRUE, uses point approximation). model applies real-valued data, positive data, compactly-supported data (support automatically deduced observed y values). results typically unchanged whether laplace_approx TRUE/FALSE; setting TRUE may reduce sensitivity prior, setting FALSE may speed computations large datasets.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Semiparametric Bayesian quantile regression — sbqr","text":"","code":"if (FALSE) { # Simulate some heteroskedastic data (no transformation): dat = simulate_tlm(n = 200, p = 10, g_type = 'box-cox', heterosked = TRUE, lambda = 1) y = dat$y; X = dat$X # training data y_test = dat$y_test; X_test = dat$X_test # testing data  # Target this quantile: tau = 0.05  # Fit the semiparametric Bayesian quantile regression model: fit = sbqr(y = y, X = X, tau = tau, X_test = X_test) names(fit) # what is returned  # Posterior predictive checks on testing data: empirical CDF y0 = sort(unique(y_test)) plot(y0, y0, type='n', ylim = c(0,1),      xlab='y', ylab='F_y', main = 'Posterior predictive ECDF') temp = sapply(1:nrow(fit$post_ytilde), function(s)   lines(y0, ecdf(fit$post_ytilde[s,])(y0), # ECDF of posterior predictive draws         col='gray', type ='s')) lines(y0, ecdf(y_test)(y0),  # ECDF of testing data      col='black', type = 's', lwd = 3) }"},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":null,"dir":"Reference","previous_headings":"","what":"Semiparametric Bayesian spline model — sbsm","title":"Semiparametric Bayesian spline model — sbsm","text":"Monte Carlo sampling Bayesian spline regression unknown (nonparametric) transformation.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Semiparametric Bayesian spline model — sbsm","text":"","code":"sbsm(   y,   x = NULL,   x_test = NULL,   psi = NULL,   laplace_approx = TRUE,   approx_g = FALSE,   nsave = 1000,   ngrid = 100,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Semiparametric Bayesian spline model — sbsm","text":"y n x 1 response vector x n x 1 vector observation points; NULL, assume equally-spaced [0,1] x_test n_test x 1 vector testing points; NULL, assume equal x psi prior variance (inverse smoothing parameter); NULL, sample parameter laplace_approx logical; TRUE, use normal approximation posterior definition transformation; otherwise prior used approx_g logical; TRUE, apply large-sample approximation transformation nsave number Monte Carlo simulations ngrid number grid points inverse approximations verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Semiparametric Bayesian spline model — sbsm","text":"list following elements: coefficients posterior mean regression coefficients post_theta: nsave x p samples posterior distribution regression coefficients post_ytilde: nsave x n_test samples posterior predictive distribution x_test post_g: nsave posterior samples transformation evaluated unique y values model: model fit (, sbsm) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Semiparametric Bayesian spline model — sbsm","text":"function provides fully Bayesian inference transformed spline regression model using Monte Carlo (MCMC) sampling. transformation modeled unknown learned jointly regression function (unless approx_g = TRUE, uses point approximation). model applies real-valued data, positive data, compactly-supported data (support automatically deduced observed y values). results typically unchanged whether laplace_approx TRUE/FALSE; setting TRUE may reduce sensitivity prior, setting FALSE may speed computations large datasets.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Semiparametric Bayesian spline model — sbsm","text":"","code":"# Simulate some data: n = 500 # sample size x = sort(runif(n)) # observation points  # Transform a noisy, periodic function: y = g_inv_bc(   sin(2*pi*x) + sin(4*pi*x) + rnorm(n, sd = .5),              lambda = .5) # Signed square-root transformation  # Fit the semiparametric Bayesian spline model: fit = sbsm(y = y, x = x) #> [1] \"13 seconds remaining\" #> [1] \"7 seconds remaining\" #> [1] \"0 seconds remaining\" #> [1] \"Total time:  14 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"post_theta\"    \"post_ytilde\"   \"post_g\"        #>  [5] \"post_psi\"      \"model\"         \"y\"             \"X\"             #>  [9] \"psi\"           \"approx_g\"      \"sigma_epsilon\"  # Note: this is Monte Carlo sampling, so no need for MCMC diagnostics!  # Plot the model predictions (point and interval estimates): pi_y = t(apply(fit$post_ytilde, 2, quantile, c(0.05, .95))) # 90% PI plot(x, y, type='n', ylim = range(pi_y,y),      xlab = 'x', ylab = 'y', main = paste('Fitted values and prediction intervals')) polygon(c(x, rev(x)),c(pi_y[,2], rev(pi_y[,1])),col='gray', border=NA) lines(x, y, type='p') lines(x, colMeans(fit$post_ytilde), lwd = 3)"},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate a transformed linear model — simulate_tlm","title":"Simulate a transformed linear model — simulate_tlm","text":"Generate training data (X, y) testing data (X_test, y_test) transformed linear model. covariates correlated Gaussian variables. Half true regression coefficients zero half one. multiple options transformation, define support data (see ).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate a transformed linear model — simulate_tlm","text":"","code":"simulate_tlm(   n,   p,   g_type = \"beta\",   n_test = 1000,   heterosked = FALSE,   lambda = 1 )"},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate a transformed linear model — simulate_tlm","text":"n number observations training data p number covariates g_type type transformation; must one beta, step, box-cox n_test number observations testing data heterosked logical; TRUE, simulate latent data heteroskedasticity lambda Box-Cox parameter (applies g_type = 'box-cox')","code":""},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate a transformed linear model — simulate_tlm","text":"list following elements: y: response variable training data X: covariates training data y_test: response variable testing data X_test: covariates testing data beta_true: true regression coefficients g_true: true transformation, evaluated y","code":""},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulate a transformed linear model — simulate_tlm","text":"transformations vary complexity support observed data, include following options: beta yields marginally Beta(0.1, 0.5) data supported [0,1]; step generates locally-linear inverse transformation produces positive data; box-cox refers signed Box-Cox family indexed lambda, generates real-valued data examples including identity, square-root, log transformations.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate a transformed linear model — simulate_tlm","text":"","code":"# Simulate data: dat = simulate_tlm(n = 100, p = 10, g_type = 'beta') names(dat) # what is returned #> [1] \"y\"         \"X\"         \"y_test\"    \"X_test\"    \"beta_true\" \"g_true\"    hist(dat$y, breaks = 25) # marginal distribution"},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":null,"dir":"Reference","previous_headings":"","what":"Post-processing with importance sampling — sir_adjust","title":"Post-processing with importance sampling — sir_adjust","text":"Given Monte Carlo draws surrogate posterior, apply sampling importance reweighting (SIR) correct true model likelihood.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Post-processing with importance sampling — sir_adjust","text":"","code":"sir_adjust(fit, sir_frac = 0.3, nsims_prior = 100, verbose = TRUE)"},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Post-processing with importance sampling — sir_adjust","text":"fit fitted model object includes coefficients posterior mean regression coefficients post_theta: nsave x p samples posterior distribution regression coefficients post_ytilde: nsave x n_test samples posterior predictive distribution test points X_test post_g: nsave posterior samples transformation evaluated unique y values model: model fit (sblm sbsm) sir_frac fraction draws sample SIR nsims_prior number draws prior verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Post-processing with importance sampling — sir_adjust","text":"fitted model object posterior draws subsampled based SIR adjustment","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Post-processing with importance sampling — sir_adjust","text":"Monte Carlo sampling sblm sbsm uses surrogate likelihood posterior inference, enables much faster easier computing. SIR provides correction actual (specified) likelihood. However, correction step quite slow typically produce noticeable discrepancies, even small sample sizes.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Post-processing with importance sampling — sir_adjust","text":"SIR sampling done WITHOUT replacement, sir_frac typically 0.1 0.5. nsims_priors draws used approximate prior expectation, larger values can significantly slow function.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Post-processing with importance sampling — sir_adjust","text":"","code":"if (FALSE) { # Simulate some data: dat = simulate_tlm(n = 50, p = 5, g_type = 'step') y = dat$y; X = dat$X # training data y_test = dat$y_test; X_test = dat$X_test # testing data  hist(y, breaks = 10) # marginal distribution  # Fit the semiparametric Bayesian linear model: fit = sblm(y = y, X = X, X_test = X_test) names(fit) # what is returned  # Update with SIR: fit_sir = sir_adjust(fit)  # Prediction: unadjusted vs. adjusted?  # Point estimates: y_hat = colMeans(fit$post_ytilde) y_hat_sir = colMeans(fit_sir$post_ytilde) cor(y_hat, y_hat_sir) # similar  # Interval estimates: pi_y = t(apply(fit$post_ytilde, 2, quantile, c(0.05, .95))) # 90% PI pi_y_sir = t(apply(fit_sir$post_ytilde, 2, quantile, c(0.05, .95))) # 90% PI  # PI overlap (%): overlaps = 100*sapply(1:length(y_test), function(i){   # innermost part   (min(pi_y[i,2], pi_y_sir[i,2]) - max(pi_y[i,1], pi_y_sir[i,1]))/     # outermost part     (max(pi_y[i,2], pi_y_sir[i,2]) - min(pi_y[i,1], pi_y_sir[i,1])) }) summary(overlaps) # mostly close to 100%  # Coverage of PIs on testing data (should be ~ 90%) mean((pi_y[,1] <= y_test)*(pi_y[,2] >= y_test)) # unadjusted mean((pi_y_sir[,1] <= y_test)*(pi_y_sir[,2] >= y_test)) # adjusted  # Plot together with testing data: plot(y_test, y_test, type='n', ylim = range(pi_y, pi_y_sir, y_test),      xlab = 'y_test', ylab = 'y_hat', main = paste('Prediction intervals: testing data')) abline(0,1) # reference line suppressWarnings(   arrows(y_test, pi_y[,1], y_test, pi_y[,2],          length=0.15, angle=90, code=3, col='gray', lwd=2) ) # plot the PIs (unadjusted) suppressWarnings(   arrows(y_test, pi_y_sir[,1], y_test, pi_y_sir[,2],          length=0.15, angle=90, code=3, col='darkgray', lwd=2) ) # plot the PIs (adjusted) lines(y_test, y_hat, type='p', pch=2) # plot the means (unadjusted) lines(y_test, y_hat_sir, type='p', pch=3) # plot the means (adjusted) }"},{"path":"https://drkowal.github.io/SeBR/reference/uni.slice.html","id":null,"dir":"Reference","previous_headings":"","what":"Univariate Slice Sampler from Neal (2008) — uni.slice","title":"Univariate Slice Sampler from Neal (2008) — uni.slice","text":"Compute draw univariate distribution using code provided Radford M. Neal. documentation also reproduced Neal (2008).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/uni.slice.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Univariate Slice Sampler from Neal (2008) — uni.slice","text":"","code":"uni.slice(x0, g, w = 1, m = Inf, lower = -Inf, upper = +Inf, gx0 = NULL)"},{"path":"https://drkowal.github.io/SeBR/reference/uni.slice.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Univariate Slice Sampler from Neal (2008) — uni.slice","text":"x0 Initial point g Function returning log probability density (plus constant) w Size steps creating interval (default 1) m Limit steps (default infinite) lower Lower bound support distribution (default -Inf) upper Upper bound support distribution (default +Inf) gx0 Value g(x0), known (default known)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/uni.slice.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Univariate Slice Sampler from Neal (2008) — uni.slice","text":"point sampled, log density attached attribute.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/uni.slice.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Univariate Slice Sampler from Neal (2008) — uni.slice","text":"log density function may return -Inf points outside support distribution.  lower /upper bound specified support, log density function called outside limits.","code":""},{"path":"https://drkowal.github.io/SeBR/news/index.html","id":"sebr-010","dir":"Changelog","previous_headings":"","what":"SeBR 0.1.0","title":"SeBR 0.1.0","text":"Initial CRAN submission.","code":""}]
