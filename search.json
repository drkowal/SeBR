[{"path":"https://drkowal.github.io/SeBR/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 SeBR authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://drkowal.github.io/SeBR/articles/SeBR.html","id":"background-semiparametric-regression-via-data-transformations","dir":"Articles","previous_headings":"","what":"Background: semiparametric regression via data transformations","title":"Introduction to SeBR","text":"Data transformations useful companion parametric regression models. well-chosen learned transformation can greatly enhance applicability given model, especially data irregular marginal features (e.g., multimodality, skewness) various data domains (e.g., real-valued, positive, compactly-supported data). interested providing fully Bayesian inference semiparametric regression models incorporate (1) unknown data transformation (2) useful parametric regression model. paired data {xi,yi}=1n\\{x_i, y_i\\}_{=1}^n xi∈ℝpx_i \\\\mathbb{R}^p y∈𝒴⊆ℝy \\\\mathcal{Y} \\subseteq \\mathbb{R}, consider following class models: g(yi)=zi g(y_i) = z_i zi=fθ(xi)+σϵi z_i  = f_\\theta(x_i) + \\sigma \\epsilon_i  , gg (monotone increasing) data transformation learned, fθf_\\theta unknown regression function parametrized θ\\theta, ϵi\\epsilon_i independent errors. Location scale restrictions (e.g., fθ(0)=0f_\\theta(0) = 0 σ=1\\sigma =1) usually applied identifiability. Examples. focus following important special cases: linear model natural starting point: zi=xi′θ+σϵi,ϵi∼iidN(0,1) z_i = x_i'\\theta + \\sigma\\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0, 1)  transformation gg broadens applicability useful class models, including positive compactly-supported data (see ). quantile regression model replaces Gaussian assumption linear model asymmetric Laplace distribution (ALD) zi=xi′θ+σϵi,ϵi∼iidALD(τ) z_i = x_i'\\theta + \\sigma\\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} ALD(\\tau)  target τ\\tauth quantile zz xx, equivalently, g−1(τ)g^{-1}(\\tau)th quantile yy xx. ALD quite often poor model real data, especially τ\\tau near zero one. transformation gg offers pathway significantly improve model adequacy, still targeting desired quantile data. Gaussian process (GP) model generalizes linear model include nonparametric regression function, zi=fθ(xi)+σϵi,ϵi∼iidN(0,1) z_i = f_\\theta(x_i) + \\sigma \\epsilon_i, \\quad  \\epsilon_i \\stackrel{iid}{\\sim} N(0, 1)  fθf_\\theta GP θ\\theta parameterizes mean covariance functions. Although GPs offer substantial flexibility regression function fθf_\\theta, default approach (without transformation) may inadequate yy irregular marginal features restricted domain (e.g., positive compact). Challenges: goal provide fully Bayesian posterior inference unknowns (g,θ)(g, \\theta) posterior predictive inference future/unobserved data ỹ(x)\\tilde y(x). prefer model algorithm offer () flexible modeling gg (ii) efficient posterior predictive computations. Innovations: approach (https://doi.org/10.1080/01621459.2024.2395586) specifies nonparametric model gg, yet also provides Monte Carlo (MCMC) sampling posterior predictive distributions. result, control approximation accuracy via number simulations, require lengthy runs, burn-periods, convergence diagnostics, inefficiency factors accompany MCMC. Monte Carlo sampling typically quite fast.","code":""},{"path":"https://drkowal.github.io/SeBR/articles/SeBR.html","id":"using-sebr","dir":"Articles","previous_headings":"","what":"Using SeBR","title":"Introduction to SeBR","text":"R package SeBR installed loaded follows: main functions SeBR : sblm(): Monte Carlo sampling posterior predictive inference semiparametric Bayesian linear model; sbsm(): Monte Carlo sampling posterior predictive inference semiparametric Bayesian spline model, replaces linear model spline nonlinear modeling x∈ℝx \\\\mathbb{R}; sbqr(): blocked Gibbs sampling posterior predictive inference semiparametric Bayesian quantile regression; sbgp(): Monte Carlo sampling predictive inference semiparametric Bayesian Gaussian process model. function returns point estimate θ\\theta (coefficients), point predictions specified testing points (fitted.values), posterior samples transformation gg (post_g), posterior predictive samples ỹ(x)\\tilde y(x) testing points (post_ypred), well function-specific quantities (e.g., posterior draws θ\\theta, post_theta). calls coef() fitted() extract point estimates point predictions, respectively. Note: package also includes Box-Cox variants functions, .e., restricting gg (signed) Box-Cox parametric family g(t;λ)={sign(t)|t|λ−1}/λg(t; \\lambda) = \\{\\mbox{sign}(t) \\vert t \\vert^\\lambda - 1\\}/\\lambda known unknown λ\\lambda. parametric transformation less flexible, especially irregular marginals restricted domains, requires MCMC sampling. functions (e.g., blm_bc(), etc.) primarily benchmarking.","code":"# install.packages(\"devtools\") # devtools::install_github(\"drkowal/SeBR\") library(SeBR)"},{"path":"https://drkowal.github.io/SeBR/articles/SeBR.html","id":"semiparametric-bayesian-linear-models-with-sblm","dir":"Articles","previous_headings":"","what":"Semiparametric Bayesian linear models with sblm","title":"Introduction to SeBR","text":"simulate data transformed linear model: sblm() quickly produces Monte Carlo samples (θ,g,ỹ(Xtest))(\\theta, g, \\tilde y(X_{test})) semiparametric Bayesian linear model: Monte Carlo (MCMC) samples, need perform MCMC diagnostics (e.g., verify convergence, inspect autocorrelations, discard burn-, re-run multiple chains, etc.). First, check model adequacy using posterior predictive diagnostics. Specifically, compute empirical CDF y_test (black) simulated testing predictive dataset post_ypred (gray):  Despite challenging features marginal distribution, proposed model appears adequate. Although gray lines clearly visible zero one, posterior predictive distribution indeed match support observed data. Remark: Posterior predictive diagnostics require training/testing splits typically performed -sample. X_test left unspecified sblm, posterior predictive draws given X can compared y. example uses --sample checks, rigorous less common. Next, evaluate predictive ability testing dataset computing plotting --sample prediction intervals X_test comparing y_test. built-function :  --sample predictive distributions well-calibrated. Finally, summarize posterior inference transformation gg regression coefficients θ\\theta compare ground truth values. First, plot posterior draws gg (gray), posterior mean gg (black), true transformation (triangles): posterior distribution gg accurately matches true transformation. Next, compute point interval summaries θ\\theta compare ground truth: point estimates θ\\theta closely track ground truth, inference based 95% credible intervals correctly selects truly nonzero regression coefficients. Remark: location-scale data-generating process model may match exactly. Thus, use correlations compare regression coefficients θ\\theta (omitting intercept) apply location-scale shifts transformations gg ensure comparability. byproduct simulated data setting matter real data analysis (variable selection). Note: Try repeating exercise blm_bc() place sblm(). Box-Cox transformation recover transformation gg coefficients θ\\theta accurately, model diagnostics alarming, predictions deteriorate substantially.","code":"set.seed(123) # for reproducibility  # Simulate data from a transformed linear model: dat = simulate_tlm(n = 200,  # number of observations                    p = 10,   # number of covariates                     g_type = 'step' # type of transformation (here, positive data)                    ) # Training data: y = dat$y; X = dat$X   # Testing data: y_test = dat$y_test; X_test = dat$X_test # Fit the semiparametric Bayesian linear model: fit = sblm(y = y,             X = X,             X_test = X_test) #> [1] \"7 seconds remaining\" #> [1] \"5 seconds remaining\" #> [1] \"3 seconds remaining\" #> [1] \"Total time:  8 seconds\"  names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_g\"        \"model\"         \"y\"             \"X\"             #>  [9] \"X_test\"        \"psi\"           \"approx_g\"      \"sigma_epsilon\" # Evaluate posterior predictive means and intervals on the testing data: plot_pptest(fit$post_ypred,              y_test,              alpha_level = 0.10) # coverage should be >= 90% #> [1] 0.93 # Summarize the parameters (regression coefficients):  # Posterior means: coef(fit) #>  [1]  0.07314744  0.47686070  0.44598444  0.48627809  0.34035855  0.39385554 #>  [7]  0.08038143  0.12864699 -0.04873447 -0.08244957  0.08042865  # Check: correlation with true coefficients cor(dat$beta_true[-1],     coef(fit)[-1]) # excluding the intercept #> [1] 0.9435131  # 95% credible intervals: theta_ci = t(apply(fit$post_theta, 2, quantile, c(.025, 0.975)))  # Check: agreement on nonzero coefficients? which(theta_ci[,1] >= 0 | theta_ci[,2] <=0) # 95% CI excludes zero #> [1] 2 3 4 5 6 which(dat$beta_true != 0) # truly nonzero #> [1] 2 3 4 5 6"},{"path":"https://drkowal.github.io/SeBR/articles/SeBR.html","id":"semiparametric-bayesian-quantile-regression-with-sbqr","dir":"Articles","previous_headings":"","what":"Semiparametric Bayesian quantile regression with sbqr","title":"Introduction to SeBR","text":"now consider Bayesian quantile regression, specifies linear model ALD errors. First, simulate data heteroskedastic linear model. Heteroskedasticity often produces conclusions differ traditional mean regression. , include transformation, data-generating process implicitly favor approach traditional Bayesian quantile regression (.e., g(t)=tg(t) = t identity). Next, load two packages ’ll need: Now, fit two Bayesian quantile regression models: traditional version without transformation (bqr()) proposed alternative (sbqr()). target τ=0.05\\tau = 0.05 quantile. model fits, evaluate posterior predictive diagnostics . Specifically, compute empirical CDF y_test (black) simulated testing predictive dataset post_ypred sbqr (gray) bqr (red): Without transformation, Bayesian quantile regression model good model data. learned transformation completely resolves model inadequacy—even though transformation present data-generating process. Finally, can asses quantile estimates testing data. First, consider bqr:  Recall quantile regression models τ\\tau, expect asymmetric y_test. --sample empirical quantile 0.026 (target τ=0.05\\tau = 0.05) 90% prediction interval coverage 0.98. Repeat evaluation sbqr:  Now --sample empirical quantile 0.034 90% prediction interval coverage 0.97. sbqr better calibrated τ\\tau, methods slightly overconservative prediction interval coverage. However, sbqr produce significantly smaller prediction intervals maintaining conservative coverage, thus provides powerful precise inference. Remark: point interval estimates quantile regression coefficients θ\\theta may computed exactly sblm() example. Note: try quantiles, τ∈{0.25,0.5}\\tau \\\\{0.25, 0.5\\}. τ\\tau approaches 0.5 (.e., median regression), problem becomes easier models better calibrated.","code":"# Simulate data from a heteroskedastic linear model (no transformation): dat = simulate_tlm(n = 200,  # number of observations                    p = 10,   # number of covariates                     g_type = 'box-cox', lambda = 1, # no transformation                    heterosked = TRUE # heteroskedastic errors                    ) # Training data: y = dat$y; X = dat$X   # Testing data: y_test = dat$y_test; X_test = dat$X_test library(quantreg) # traditional QR for initialization library(statmod) # for rinvgauss sampling # Quantile to target: tau = 0.05  # (Traditional) Bayesian quantile regression: fit_bqr = bqr(y = y,             X = X,             tau = tau,             X_test = X_test,            verbose = FALSE  # omit printout )  # Semiparametric Bayesian quantile regression: fit = sbqr(y = y,             X = X,             tau = tau,             X_test = X_test,            verbose = FALSE # omit printout )        names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_qtau\"     \"post_g\"        \"model\"         \"y\"             #>  [9] \"X\"             \"X_test\"        \"psi\"           \"approx_g\"      #> [13] \"tau\" # Quantile point estimates: q_hat_bqr = fitted(fit_bqr)   # Empirical quantiles on testing data: (emp_quant_bqr = mean(q_hat_bqr >= y_test)) #> [1] 0.026  # Evaluate posterior predictive means and intervals on the testing data: (emp_cov_bqr = plot_pptest(fit_bqr$post_ypred,                             y_test,                             alpha_level = 0.10)) #> [1] 0.98 # Quantile point estimates: q_hat = fitted(fit)   # Empirical quantiles on testing data: (emp_quant_sbqr = mean(q_hat >= y_test)) #> [1] 0.034  # Evaluate posterior predictive means and intervals on the testing data: (emp_cov_sbqr = plot_pptest(fit$post_ypred,                              y_test,                              alpha_level = 0.10)) #> [1] 0.97"},{"path":"https://drkowal.github.io/SeBR/articles/SeBR.html","id":"semiparametric-bayesian-gaussian-processes-with-sbgp","dir":"Articles","previous_headings":"","what":"Semiparametric Bayesian Gaussian processes with sbgp","title":"Introduction to SeBR","text":"Consider challenging scenario () nonlinear regression function x∈ℝx \\\\mathbb{R} (ii) Beta marginals, support 𝒴=[0,1]\\mathcal{Y} = [0,1]. Simulate data accordingly:  highlight challenges , first consider Box-Cox-transformed GP. well proposed model, require package: Now fit Box-Cox GP evaluate --sample predictive performance:  Box-Cox transformation adds flexibility GP, insufficient data. prediction intervals unnecessarily wide respect support 𝒴=[0,1]\\mathcal{Y} = [0,1], estimated mean function fully capture trend data. Now fit semiparametric Bayesian GP model: Evaluate --sample predictive performance testing data:  Unlike Box-Cox version, sbgp respects support data 𝒴=[0,1]\\mathcal{Y} = [0,1], captures trend, provides narrower intervals (average widths 0.213 compared 0.267) better coverage (0.953 sbgp 0.894 Box-Cox). Despite significant complexities data, sbgp performs quite well ---box: nonlinearity modeled adequately; support data enforced automatically; --sample prediction intervals sharp calibrated; computations fast. Note: sbgp also applies x∈ℝpx \\\\mathbb{R}^p p>1p >1, spatial spatio-temporal data. cases may require careful consideration mean covariance functions: default mean function linear regression intercept , default covariance function isotropic Matern function. However, many options available (inherited GpGp package).","code":"# Training data: n = 200 # sample size x = seq(0, 1, length = n) # observation points  # Testing data: n_test = 1000  x_test = seq(0, 1, length = n_test)   # True inverse transformation: g_inv_true = function(z)    qbeta(pnorm(z),          shape1 = 0.5,          shape2 = 0.1) # approx Beta(0.5, 0.1) marginals  # Training observations: y = g_inv_true(   sin(2*pi*x) + sin(4*pi*x) + .25*rnorm(n)              )   # Testing observations: y_test = g_inv_true(   sin(2*pi*x_test) + sin(4*pi*x_test) + .25*rnorm(n)              )   plot(x_test, y_test,       xlab = 'x', ylab = 'y',      main = \"Training (gray) and testing (black) data\") lines(x, y, type='p', col='gray', pch = 2) library(GpGp) # fast GP computing library(fields) # accompanies GpGp # Fit the Box-Cox Gaussian process model: fit_bc = bgp_bc(y = y,             locs = x,            locs_test = x_test) #> [1] \"Initial GP fit...\" #> [1] \"Updated GP fit...\"  # Fitted values on the testing data: y_hat_bc = fitted(fit_bc)  # 90% prediction intervals on the testing data: pi_y_bc = t(apply(fit_bc$post_ypred, 2, quantile, c(0.05, .95)))   # Average PI width: (width_bc = mean(pi_y_bc[,2] - pi_y_bc[,1])) #> [1] 0.2668767  # Empirical PI coverage: (emp_cov_bc = mean((pi_y_bc[,1] <= y_test)*(pi_y_bc[,2] >= y_test))) #> [1] 0.894  # Plot these together with the actual testing points: plot(x_test, y_test, type='n',       ylim = range(pi_y_bc, y_test), xlab = 'x', ylab = 'y',       main = paste('Fitted values and prediction intervals: \\n Box-Cox Gaussian process'))  # Add the intervals: polygon(c(x_test, rev(x_test)),         c(pi_y_bc[,2], rev(pi_y_bc[,1])),         col='gray', border=NA) lines(x_test, y_test, type='p') # actual values lines(x_test, y_hat_bc, lwd = 3) # fitted values # library(GpGp) # loaded above  # Fit the semiparametric Gaussian process model: fit = sbgp(y = y,             locs = x,            locs_test = x_test) #> [1] \"Initial GP fit...\" #> [1] \"Updated GP fit...\" #> [1] \"Sampling...\" #> [1] \"Done!\"  names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"fit_gp\"        \"post_ypred\"    #>  [5] \"post_g\"        \"model\"         \"y\"             \"X\"             #>  [9] \"approx_g\"      \"sigma_epsilon\"  coef(fit) # estimated regression coefficients (here, just an intercept) #> [1] 0.02256615 # Fitted values on the testing data: y_hat = fitted(fit)  # 90% prediction intervals on the testing data: pi_y = t(apply(fit$post_ypred, 2, quantile, c(0.05, .95)))   # Average PI width: (width = mean(pi_y[,2] - pi_y[,1])) #> [1] 0.2130283  # Empirical PI coverage: (emp_cov = mean((pi_y[,1] <= y_test)*(pi_y[,2] >= y_test))) #> [1] 0.953  # Plot these together with the actual testing points: plot(x_test, y_test, type='n',       ylim = range(pi_y, y_test), xlab = 'x', ylab = 'y',       main = paste('Fitted values and prediction intervals: \\n semiparametric Gaussian process'))  # Add the intervals: polygon(c(x_test, rev(x_test)),         c(pi_y[,2], rev(pi_y[,1])),         col='gray', border=NA) lines(x_test, y_test, type='p') # actual values lines(x_test, y_hat, lwd = 3) # fitted values"},{"path":"https://drkowal.github.io/SeBR/articles/SeBR.html","id":"references","dir":"Articles","previous_headings":"Semiparametric Bayesian Gaussian processes with sbgp","what":"References","title":"Introduction to SeBR","text":"Kowal, D. Wu, B. (2024). Monte Carlo inference semiparametric Bayesian regression. JASA. https://doi.org/10.1080/01621459.2024.2395586","code":""},{"path":"https://drkowal.github.io/SeBR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Dan Kowal. Author, maintainer, copyright holder.","code":""},{"path":"https://drkowal.github.io/SeBR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Kowal D (2025). SeBR: Semiparametric Bayesian Regression Analysis. R package version 1.0.0, https://drkowal.github.io/SeBR/, https://github.com/drkowal/SeBR.","code":"@Manual{,   title = {SeBR: Semiparametric Bayesian Regression Analysis},   author = {Dan Kowal},   year = {2025},   note = {R package version 1.0.0, https://drkowal.github.io/SeBR/},   url = {https://github.com/drkowal/SeBR}, }"},{"path":"https://drkowal.github.io/SeBR/index.html","id":"sebr-semiparametric-bayesian-regression","dir":"","previous_headings":"","what":"Semiparametric Bayesian Regression Analysis","title":"Semiparametric Bayesian Regression Analysis","text":"Overview. Data transformations useful companion parametric regression models. well-chosen learned transformation can greatly enhance applicability given model, especially data irregular marginal features (e.g., multimodality, skewness) various data domains (e.g., real-valued, positive, compactly-supported data). Given paired data (xi,yi)(x_i,y_i) =1,…,ni=1,\\ldots,n, SeBR implements efficient fully Bayesian inference semiparametric regression models incorporate (1) unknown data transformation g(yi)=zi g(y_i) = z_i (2) useful parametric regression model zi=fθ(xi)+σϵi z_i  = f_\\theta(x_i) + \\sigma \\epsilon_i unknown parameters θ\\theta independent errors ϵi\\epsilon_i. Examples. focus following important special cases: linear model natural starting point: zi=xi′θ+σϵi,ϵi∼iidN(0,1) z_i = x_i'\\theta + \\sigma\\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0, 1) transformation gg broadens applicability useful class models, including positive compactly-supported data. quantile regression model replaces Gaussian assumption linear model asymmetric Laplace distribution (ALD) zi=xi′θ+σϵi,ϵi∼iidALD(τ) z_i = x_i'\\theta + \\sigma\\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} ALD(\\tau) target τ\\tauth quantile zz xx, equivalently, g−1(τ)g^{-1}(\\tau)th quantile yy xx. ALD quite often poor model real data, especially τ\\tau near zero one. transformation gg offers pathway significantly improve model adequacy, still targeting desired quantile data. Gaussian process (GP) model generalizes linear model include nonparametric regression function, zi=fθ(xi)+σϵi,ϵi∼iidN(0,1) z_i = f_\\theta(x_i) + \\sigma \\epsilon_i, \\quad  \\epsilon_i \\stackrel{iid}{\\sim} N(0, 1) fθf_\\theta GP θ\\theta parameterizes mean covariance functions. Although GPs offer substantial flexibility regression function fθf_\\theta, model may inadequate yy irregular marginal features restricted domain (e.g., positive compact). Challenges: goal provide fully Bayesian posterior inference unknowns (g,θ)(g, \\theta) posterior predictive inference future/unobserved data ỹ(x)\\tilde y(x). prefer model algorithm offer () flexible modeling gg (ii) efficient posterior predictive computations. Innovations: approach (https://doi.org/10.1080/01621459.2024.2395586) specifies nonparametric model gg, yet also provides Monte Carlo (MCMC) sampling posterior predictive distributions. result, control approximation accuracy via number simulations, require lengthy runs, burn-periods, convergence diagnostics, inefficiency factors accompany MCMC. Monte Carlo sampling typically quite fast.","code":""},{"path":"https://drkowal.github.io/SeBR/index.html","id":"using-sebr","dir":"","previous_headings":"","what":"Using SeBR","title":"Semiparametric Bayesian Regression Analysis","text":"package SeBR installed loaded follows: main functions SeBR : sblm(): Monte Carlo sampling posterior predictive inference semiparametric Bayesian linear model; sbsm(): Monte Carlo sampling posterior predictive inference semiparametric Bayesian spline model, replaces linear model spline nonlinear modeling x∈ℝx \\\\mathbb{R}; sbqr(): blocked Gibbs sampling posterior predictive inference semiparametric Bayesian quantile regression; sbgp(): Monte Carlo sampling predictive inference semiparametric Bayesian Gaussian process model. function returns point estimate θ\\theta (coefficients), point predictions specified testing points (fitted.values), posterior samples transformation gg (post_g), posterior predictive samples ỹ(x)\\tilde y(x) testing points (post_ypred), well function-specific quantities (e.g., posterior draws θ\\theta, post_theta). calls coef() fitted() extract point estimates point predictions, respectively. Note: package also includes Box-Cox variants functions, .e., restricting gg (signed) Box-Cox parametric family g(t;λ)={sign(t)|t|λ−1}/λg(t; \\lambda) = \\{\\mbox{sign}(t) \\vert t \\vert^\\lambda - 1\\}/\\lambda known unknown λ\\lambda. parametric transformation less flexible, especially irregular marginals restricted domains, requires MCMC sampling. functions (e.g., blm_bc(), etc.) primarily benchmarking. Detailed documentation examples available https://drkowal.github.io/SeBR/.","code":"# CRAN version: # install.packages(\"SeBR\")  # Development version:  # devtools::install_github(\"drkowal/SeBR\") library(SeBR)"},{"path":"https://drkowal.github.io/SeBR/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Semiparametric Bayesian Regression Analysis","text":"Kowal, D. Wu, B. (2024). Monte Carlo inference semiparametric Bayesian regression. JASA. https://doi.org/10.1080/01621459.2024.2395586","code":""},{"path":"https://drkowal.github.io/SeBR/reference/Fz_fun.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the latent data CDF — Fz_fun","title":"Compute the latent data CDF — Fz_fun","text":"Assuming Gaussian latent data distribution (given x), compute CDF grid points","code":""},{"path":"https://drkowal.github.io/SeBR/reference/Fz_fun.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the latent data CDF — Fz_fun","text":"","code":"Fz_fun(z, weights = NULL, mean_vec = NULL, sd_vec)"},{"path":"https://drkowal.github.io/SeBR/reference/Fz_fun.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the latent data CDF — Fz_fun","text":"z vector points CDF z evaluated weights n-dimensional vector weights; NULL, assume 1/n mean_vec n-dimensional vector means; NULL, assume mean zero sd_vec n-dimensional vector standard deviations","code":""},{"path":"https://drkowal.github.io/SeBR/reference/Fz_fun.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the latent data CDF — Fz_fun","text":"CDF z evaluated z","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bb.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian bootstrap posterior sampler for the CDF — bb","title":"Bayesian bootstrap posterior sampler for the CDF — bb","text":"Compute one Monte Carlo draw Bayesian bootstrap (BB) posterior distribution cumulative distribution function (CDF).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian bootstrap posterior sampler for the CDF — bb","text":"","code":"bb(y)"},{"path":"https://drkowal.github.io/SeBR/reference/bb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian bootstrap posterior sampler for the CDF — bb","text":"y data infer CDF (preferably sorted)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian bootstrap posterior sampler for the CDF — bb","text":"function can evaluate sampled CDF argument(s)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian bootstrap posterior sampler for the CDF — bb","text":"Assuming data y iid unknown distribution, Bayesian bootstrap (BB) nonparametric model distribution. BB limiting case Dirichlet process prior (without hyperparameters) admits direct Monte Carlo (MCMC) sampling. function computes one draw BB posterior distribution CDF Fy.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bb.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bayesian bootstrap posterior sampler for the CDF — bb","text":"code inspired ggdist::weighted_ecdf.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian bootstrap posterior sampler for the CDF — bb","text":"","code":"# Simulate data: y = rnorm(n = 100)  # One draw from the BB posterior: Fy = bb(y)  class(Fy) # this is a function #> [1] \"function\" Fy(0) # some example use (for this one draw) #> [1] 0.5173514 Fy(c(.5, 1.2)) #> [1] 0.6692355 0.8765828  # Plot several draws from the BB posterior distribution: ys = seq(-3, 3, length.out=1000) plot(ys, ys, type='n', ylim = c(0,1),      main = 'Draws from BB posterior', xlab = 'y', ylab = 'F(y)') for(s in 1:50) lines(ys, bb(y)(ys), col='gray')  # Add ECDF for reference: lines(ys, ecdf(y)(ys), lty=2)"},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian Gaussian processes with a Box-Cox transformation — bgp_bc","title":"Bayesian Gaussian processes with a Box-Cox transformation — bgp_bc","text":"MCMC sampling Bayesian Gaussian process regression (known unknown) Box-Cox transformation.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian Gaussian processes with a Box-Cox transformation — bgp_bc","text":"","code":"bgp_bc(   y,   locs,   X = NULL,   covfun_name = \"matern_isotropic\",   locs_test = locs,   X_test = NULL,   nn = 30,   emp_bayes = TRUE,   lambda = NULL,   sample_lambda = TRUE,   nsave = 1000,   nburn = 1000,   nskip = 0 )"},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian Gaussian processes with a Box-Cox transformation — bgp_bc","text":"y n x 1 response vector locs n x d matrix locations X n x p design matrix; unspecified, use intercept covfun_name string name covariance function; see ?GpGp locs_test n_test x d matrix locations predictions needed; default locs X_test n_test x p design matrix test data; default X nn number nearest neighbors use; default 30 (larger values improve approximation increase computing cost) emp_bayes logical; TRUE, use (faster!) empirical Bayes approach estimating mean function lambda Box-Cox transformation; NULL, estimate parameter sample_lambda logical; TRUE, sample lambda, otherwise use fixed value lambda MLE (lambda unspecified) nsave number MCMC iterations save nburn number MCMC iterations discard nskip number MCMC iterations skip saving iterations, .e., save every (nskip + 1)th draw","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian Gaussian processes with a Box-Cox transformation — bgp_bc","text":"list following elements: coefficients posterior mean regression coefficients fitted.values posterior predictive mean test points locs_test fit_gp fitted GpGp_fit object, includes covariance parameter estimates model information post_ypred: nsave x n_test samples posterior predictive distribution locs_test post_g: nsave posterior samples transformation evaluated unique y values post_lambda nsave posterior samples lambda model: model fit (, bgp_bc) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian Gaussian processes with a Box-Cox transformation — bgp_bc","text":"function provides Bayesian inference transformed Gaussian processes. transformation parametric Box-Cox family, one parameter lambda. parameter may fixed advanced learned data. computational efficiency, Gaussian process parameters fixed point estimates, latent Gaussian process sampled emp_bayes = FALSE.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bayesian Gaussian processes with a Box-Cox transformation — bgp_bc","text":"Box-Cox transformations may useful cases, general recommend nonparametric transformation (Monte Carlo, MCMC sampling) sbgp.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian Gaussian processes with a Box-Cox transformation — bgp_bc","text":"","code":"# \\donttest{ # Simulate some data: n = 200 # sample size x = seq(0, 1, length = n) # observation points  # Transform a noisy, periodic function: y = g_inv_bc(   sin(2*pi*x) + sin(4*pi*x) + rnorm(n, sd = .5),              lambda = .5) # Signed square-root transformation  # Package we use for fast computing w/ Gaussian processes: library(GpGp)  # Fit a Bayesian Gaussian process with Box-Cox transformation: fit = bgp_bc(y = y, locs = x) #> [1] \"Initial GP fit...\" #> [1] \"Updated GP fit...\" names(fit) # what is returned #> [1] \"coefficients\"  \"fitted.values\" \"fit_gp\"        \"post_ypred\"    #> [5] \"post_g\"        \"post_lambda\"   \"model\"         \"y\"             #> [9] \"X\"             coef(fit) # estimated regression coefficients (here, just an intercept) #> [1] 0.3167962 class(fit$fit_gp) # the GpGp object is also returned #> [1] \"GpGp_fit\" round(quantile(fit$post_lambda), 3) # summary of unknown Box-Cox parameter #>    0%   25%   50%   75%  100%  #> 0.693 0.786 0.813 0.841 0.934   # Plot the model predictions (point and interval estimates): pi_y = t(apply(fit$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI plot(x, y, type='n', ylim = range(pi_y,y),      xlab = 'x', ylab = 'y', main = paste('Fitted values and prediction intervals')) polygon(c(x, rev(x)),c(pi_y[,2], rev(pi_y[,1])),col='gray', border=NA) lines(x, y, type='p') lines(x, fitted(fit), lwd = 3)  # }"},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian linear model with a Box-Cox transformation — blm_bc","title":"Bayesian linear model with a Box-Cox transformation — blm_bc","text":"MCMC sampling Bayesian linear regression (known unknown) Box-Cox transformation. g-prior assumed regression coefficients.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian linear model with a Box-Cox transformation — blm_bc","text":"","code":"blm_bc(   y,   X,   X_test = X,   psi = length(y),   lambda = NULL,   sample_lambda = TRUE,   nsave = 1000,   nburn = 1000,   nskip = 0,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian linear model with a Box-Cox transformation — blm_bc","text":"y n x 1 vector observed counts X n x p matrix predictors X_test n_test x p matrix predictors test data; default observed covariates X psi prior variance (g-prior) lambda Box-Cox transformation; NULL, estimate parameter sample_lambda logical; TRUE, sample lambda, otherwise use fixed value lambda MLE (lambda unspecified) nsave number MCMC iterations save nburn number MCMC iterations discard nskip number MCMC iterations skip saving iterations, .e., save every (nskip + 1)th draw verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian linear model with a Box-Cox transformation — blm_bc","text":"list following elements: coefficients posterior mean regression coefficients fitted.values posterior predictive mean test points X_test post_theta: nsave x p samples posterior distribution regression coefficients post_ypred: nsave x n_test samples posterior predictive distribution test points X_test post_g: nsave posterior samples transformation evaluated unique y values post_lambda nsave posterior samples lambda post_sigma nsave posterior samples sigma model: model fit (, blm_bc) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian linear model with a Box-Cox transformation — blm_bc","text":"function provides fully Bayesian inference transformed linear model via MCMC sampling. transformation parametric Box-Cox family, one parameter lambda. parameter may fixed advanced learned data.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bayesian linear model with a Box-Cox transformation — blm_bc","text":"Box-Cox transformations may useful cases, general recommend nonparametric transformation (Monte Carlo, MCMC sampling) sblm.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian linear model with a Box-Cox transformation — blm_bc","text":"","code":"# Simulate some data: dat = simulate_tlm(n = 100, p = 5, g_type = 'step') y = dat$y; X = dat$X # training data y_test = dat$y_test; X_test = dat$X_test # testing data  hist(y, breaks = 25) # marginal distribution   # Fit the Bayesian linear model with a Box-Cox transformation: fit = blm_bc(y = y, X = X, X_test = X_test) #> [1] \"0 seconds remaining\" #> [1] \"Total time:  0 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_g\"        \"post_lambda\"   \"post_sigma\"    \"model\"         #>  [9] \"y\"             \"X\"             \"X_test\"        \"psi\"           round(quantile(fit$post_lambda), 3) # summary of unknown Box-Cox parameter #>    0%   25%   50%   75%  100%  #> 0.000 0.002 0.006 0.012 0.062"},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian quantile regression — bqr","title":"Bayesian quantile regression — bqr","text":"MCMC sampling Bayesian quantile regression. asymmetric Laplace distribution assumed errors, regression models targets specified quantile. g-prior assumed regression coefficients.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian quantile regression — bqr","text":"","code":"bqr(   y,   X,   tau = 0.5,   X_test = X,   psi = length(y),   nsave = 1000,   nburn = 1000,   nskip = 0,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian quantile regression — bqr","text":"y n x 1 vector observed counts X n x p matrix predictors tau target quantile (zero one) X_test n_test x p matrix predictors test data; default observed covariates X psi prior variance (g-prior) nsave number MCMC iterations save nburn number MCMC iterations discard nskip number MCMC iterations skip saving iterations, .e., save every (nskip + 1)th draw verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian quantile regression — bqr","text":"list following elements: coefficients posterior mean regression coefficients fitted.values estimated tauth quantile test points X_test post_theta: nsave x p samples posterior distribution regression coefficients post_ypred: nsave x n_test samples posterior predictive distribution test points X_test model: model fit (, bqr) well arguments passed","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bayesian quantile regression — bqr","text":"asymmetric Laplace distribution advantageous links regression model (X%*%theta) pre-specified quantile (tau). However, often poor model observed data, semiparametric version sbqr recommended general.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian quantile regression — bqr","text":"","code":"# Simulate some heteroskedastic data (no transformation): dat = simulate_tlm(n = 100, p = 5, g_type = 'box-cox', heterosked = TRUE, lambda = 1) y = dat$y; X = dat$X # training data y_test = dat$y_test; X_test = dat$X_test # testing data  # Target this quantile: tau = 0.05  # Fit the Bayesian quantile regression model: fit = bqr(y = y, X = X, tau = tau, X_test = X_test) #> [1] \"0 seconds remaining\" #> [1] \"Total time:  0 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"model\"         \"y\"             \"X\"             \"X_test\"        #>  [9] \"psi\"           \"tau\"            # Posterior predictive checks on testing data: empirical CDF y0 = sort(unique(y_test)) plot(y0, y0, type='n', ylim = c(0,1),      xlab='y', ylab='F_y', main = 'Posterior predictive ECDF') temp = sapply(1:nrow(fit$post_ypred), function(s)   lines(y0, ecdf(fit$post_ypred[s,])(y0), # ECDF of posterior predictive draws         col='gray', type ='s')) lines(y0, ecdf(y_test)(y0),  # ECDF of testing data      col='black', type = 's', lwd = 3)   # The posterior predictive checks usually do not pass! # try ?sbqr instead..."},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian spline model with a Box-Cox transformation — bsm_bc","title":"Bayesian spline model with a Box-Cox transformation — bsm_bc","text":"MCMC sampling Bayesian spline regression (known unknown) Box-Cox transformation.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian spline model with a Box-Cox transformation — bsm_bc","text":"","code":"bsm_bc(   y,   x = NULL,   x_test = NULL,   psi = NULL,   lambda = NULL,   sample_lambda = TRUE,   nsave = 1000,   nburn = 1000,   nskip = 0,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian spline model with a Box-Cox transformation — bsm_bc","text":"y n x 1 vector observed counts x n x 1 vector observation points; NULL, assume equally-spaced [0,1] x_test n_test x 1 vector testing points; NULL, assume equal x psi prior variance (inverse smoothing parameter); NULL, sample parameter lambda Box-Cox transformation; NULL, estimate parameter sample_lambda logical; TRUE, sample lambda, otherwise use fixed value lambda MLE (lambda unspecified) nsave number MCMC iterations save nburn number MCMC iterations discard nskip number MCMC iterations skip saving iterations, .e., save every (nskip + 1)th draw verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian spline model with a Box-Cox transformation — bsm_bc","text":"list following elements: coefficients posterior mean regression coefficients fitted.values posterior predictive mean test points x_test post_theta: nsave x p samples posterior distribution regression coefficients post_ypred: nsave x n_test samples posterior predictive distribution x_test post_g: nsave posterior samples transformation evaluated unique y values post_lambda nsave posterior samples lambda model: model fit (, sbsm_bc) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian spline model with a Box-Cox transformation — bsm_bc","text":"function provides fully Bayesian inference transformed spline model via MCMC sampling. transformation parametric Box-Cox family, one parameter lambda. parameter may fixed advanced learned data.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bayesian spline model with a Box-Cox transformation — bsm_bc","text":"Box-Cox transformations may useful cases, general recommend nonparametric transformation (Monte Carlo, MCMC sampling) sbsm.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian spline model with a Box-Cox transformation — bsm_bc","text":"","code":"# Simulate some data: n = 100 # sample size x = sort(runif(n)) # observation points  # Transform a noisy, periodic function: y = g_inv_bc(   sin(2*pi*x) + sin(4*pi*x) + rnorm(n, sd = .5),              lambda = .5) # Signed square-root transformation  # Fit the Bayesian spline model with a Box-Cox transformation: fit = bsm_bc(y = y, x = x) #> [1] \"0 seconds remaining\" #> [1] \"Total time:  0 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_g\"        \"post_lambda\"   \"model\"         \"y\"             #>  [9] \"X\"             \"psi\"           round(quantile(fit$post_lambda), 3) # summary of unknown Box-Cox parameter #>    0%   25%   50%   75%  100%  #> 0.418 0.544 0.579 0.616 0.749   # Plot the model predictions (point and interval estimates): pi_y = t(apply(fit$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI plot(x, y, type='n', ylim = range(pi_y,y),      xlab = 'x', ylab = 'y', main = paste('Fitted values and prediction intervals')) polygon(c(x, rev(x)),c(pi_y[,2], rev(pi_y[,1])),col='gray', border=NA) lines(x, y, type='p') lines(x, fitted(fit), lwd = 3)"},{"path":"https://drkowal.github.io/SeBR/reference/computeTimeRemaining.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate the remaining time in the MCMC based on previous samples — computeTimeRemaining","title":"Estimate the remaining time in the MCMC based on previous samples — computeTimeRemaining","text":"Estimate remaining time MCMC based previous samples","code":""},{"path":"https://drkowal.github.io/SeBR/reference/computeTimeRemaining.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate the remaining time in the MCMC based on previous samples — computeTimeRemaining","text":"","code":"computeTimeRemaining(nsi, timer0, nsims, nrep = 1000)"},{"path":"https://drkowal.github.io/SeBR/reference/computeTimeRemaining.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate the remaining time in the MCMC based on previous samples — computeTimeRemaining","text":"nsi Current iteration timer0 Initial timer value, returned proc.time()[3] nsims Total number simulations nrep Print estimated time remaining every nrep iterations","code":""},{"path":"https://drkowal.github.io/SeBR/reference/computeTimeRemaining.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate the remaining time in the MCMC based on previous samples — computeTimeRemaining","text":"Table summary statistics using function summary","code":""},{"path":"https://drkowal.github.io/SeBR/reference/concen_hbb.html","id":null,"dir":"Reference","previous_headings":"","what":"Posterior sampling algorithm for the HBB concentration hyperparameters — concen_hbb","title":"Posterior sampling algorithm for the HBB concentration hyperparameters — concen_hbb","text":"Compute Monte Carlo draws (marginal) posterior distribution concentration hyperparameters hierarchical Bayesian bootstrap (hbb). HBB nonparametric model group-specific distributions; group concentration parameter, larger values encourage shrinkage toward common distribution.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/concen_hbb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Posterior sampling algorithm for the HBB concentration hyperparameters — concen_hbb","text":"","code":"concen_hbb(   groups,   shape_alphas = NULL,   rate_alphas = NULL,   nsave = 1000,   ngrid = 500 )"},{"path":"https://drkowal.github.io/SeBR/reference/concen_hbb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Posterior sampling algorithm for the HBB concentration hyperparameters — concen_hbb","text":"groups group assignments observed data shape_alphas (optional) shape parameter Gamma prior rate_alphas (optional) rate parameter Gamma prior nsave (optional) number Monte Carlo simulations ngrid (optional) number grid points","code":""},{"path":"https://drkowal.github.io/SeBR/reference/concen_hbb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Posterior sampling algorithm for the HBB concentration hyperparameters — concen_hbb","text":"nsave x K samples concentration hyperparameters corresponding K groups","code":""},{"path":"https://drkowal.github.io/SeBR/reference/concen_hbb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Posterior sampling algorithm for the HBB concentration hyperparameters — concen_hbb","text":"concentration hyperparameters assigned independent Gamma(shape_alphas, rate_alphas) priors. function uses grid approximation marginal posterior goal producing simple algorithm. *marginal* posterior sampler, can used hbb sampler (conditions alphas) provide joint Monte Carlo (MCMC) sampling algorithm concentration hyperparameters, group-specific CDFs, common CDF. Note diffuse priors alphas tend put posterior mass large values, leads aggressive shrinkage toward common distribution (complete pooling). moderate shrinkage, use default values shape_alphas = 30*K rate_alphas = 1, K number groups.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/concen_hbb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Posterior sampling algorithm for the HBB concentration hyperparameters — concen_hbb","text":"Oganisian et al. (https://doi.org/10.1515/ijb-2022-0051)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/concen_hbb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Posterior sampling algorithm for the HBB concentration hyperparameters — concen_hbb","text":"","code":"# Dimensions: n = 500 # number of observations K = 3 # number of groups  # Assign groups w/ unequal probabilities: ugroups = paste('g', 1:K, sep='') # groups groups = sample(ugroups,                 size = n,                 replace = TRUE,                 prob = 1:K) # unequally weighted (unnormalized)  # Summarize: table(groups)/n #> groups #>    g1    g2    g3  #> 0.196 0.336 0.468   # Marginal posterior sampling for alpha: post_alpha = concen_hbb(groups)  # Summarize: posterior distributions for(c in 1:K) {   hist(post_alpha[,c],        main = paste(\"Concentration parameter: group\", ugroups[c]),        xlim = range(post_alpha))   abline(v = mean(post_alpha[,c]), lwd=3) # posterior mean }"},{"path":"https://drkowal.github.io/SeBR/reference/contract_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Grid contraction — contract_grid","title":"Grid contraction — contract_grid","text":"Contract grid evaluation points exceed threshold. removes corresponding z values. can add points back achieve (approximate) length.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/contract_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Grid contraction — contract_grid","text":"","code":"contract_grid(z, Fz, lower, upper, add_back = TRUE, monotone = TRUE)"},{"path":"https://drkowal.github.io/SeBR/reference/contract_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Grid contraction — contract_grid","text":"z grid points (ordered) Fz function evaluated grid points lower lower threshold check Fz upper upper threshold check Fz add_back logical; true, expand grid () original size monotone logical; true, enforce monotonicity expanded grid","code":""},{"path":"https://drkowal.github.io/SeBR/reference/contract_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Grid contraction — contract_grid","text":"list containing grid points z (interpolated) function Fz points","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":null,"dir":"Reference","previous_headings":"","what":"Box-Cox transformation — g_bc","title":"Box-Cox transformation — g_bc","text":"Evaluate Box-Cox transformation, scaled power transformation preserve continuity index lambda zero. Negative values permitted.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Box-Cox transformation — g_bc","text":"","code":"g_bc(t, lambda)"},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Box-Cox transformation — g_bc","text":"t argument(s) evaluate function lambda Box-Cox parameter","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Box-Cox transformation — g_bc","text":"evaluation(s) Box-Cox function given input(s) t.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Box-Cox transformation — g_bc","text":"Special cases include identity transformation (lambda = 1), square-root transformation (lambda = 1/2), log transformation (lambda = 0).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Box-Cox transformation — g_bc","text":"","code":"# Log-transformation: g_bc(1:5, lambda = 0); log(1:5) #> [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 #> [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379  # Square-root transformation: note the shift and scaling g_bc(1:5, lambda = 1/2); sqrt(1:5) #> [1] 0.0000000 0.8284271 1.4641016 2.0000000 2.4721360 #> [1] 1.000000 1.414214 1.732051 2.000000 2.236068"},{"path":"https://drkowal.github.io/SeBR/reference/g_fun.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the transformation — g_fun","title":"Compute the transformation — g_fun","text":"Given CDFs z y, compute smoothed function evaluate transformation","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_fun.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the transformation — g_fun","text":"","code":"g_fun(y, Fy_eval, z, Fz_eval)"},{"path":"https://drkowal.github.io/SeBR/reference/g_fun.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the transformation — g_fun","text":"y vector points CDF y evaluated Fy_eval CDF y evaluated y z vector points CDF z evaluated Fz_eval CDF z evaluated z","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_fun.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the transformation — g_fun","text":"smooth monotone function can used evaluations transformation.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_approx.html","id":null,"dir":"Reference","previous_headings":"","what":"Approximate inverse transformation — g_inv_approx","title":"Approximate inverse transformation — g_inv_approx","text":"Compute inverse function transformation g based grid search.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_approx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Approximate inverse transformation — g_inv_approx","text":"","code":"g_inv_approx(g, t_grid)"},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_approx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Approximate inverse transformation — g_inv_approx","text":"g transformation function t_grid grid arguments evaluate transformation function","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_approx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Approximate inverse transformation — g_inv_approx","text":"function can used evaluations (approximate) inverse transformation function.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_bc.html","id":null,"dir":"Reference","previous_headings":"","what":"Inverse Box-Cox transformation — g_inv_bc","title":"Inverse Box-Cox transformation — g_inv_bc","text":"Evaluate inverse Box-Cox transformation. Negative values permitted.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_bc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inverse Box-Cox transformation — g_inv_bc","text":"","code":"g_inv_bc(s, lambda)"},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_bc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inverse Box-Cox transformation — g_inv_bc","text":"s argument(s) evaluate function lambda Box-Cox parameter","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_bc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Inverse Box-Cox transformation — g_inv_bc","text":"evaluation(s) inverse Box-Cox function given input(s) s.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_bc.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Inverse Box-Cox transformation — g_inv_bc","text":"Special cases include identity transformation (lambda = 1), square-root transformation (lambda = 1/2), log transformation (lambda = 0).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_bc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inverse Box-Cox transformation — g_inv_bc","text":"","code":"# (Inverse) log-transformation: g_inv_bc(1:5, lambda = 0); exp(1:5) #> [1]   2.718282   7.389056  20.085537  54.598150 148.413159 #> [1]   2.718282   7.389056  20.085537  54.598150 148.413159  # (Inverse) square-root transformation: note the shift and scaling g_inv_bc(1:5, lambda = 1/2); (1:5)^2 #> [1]  2.25  4.00  6.25  9.00 12.25 #> [1]  1  4  9 16 25"},{"path":"https://drkowal.github.io/SeBR/reference/hbb.html","id":null,"dir":"Reference","previous_headings":"","what":"Hierarchical Bayesian bootstrap posterior sampler — hbb","title":"Hierarchical Bayesian bootstrap posterior sampler — hbb","text":"Compute one Monte Carlo draw hierarchical Bayesian bootstrap (HBB) posterior distribution cumulative distribution function (CDF) group. common (BB) group-specific (HBB) weights also returned.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/hbb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hierarchical Bayesian bootstrap posterior sampler — hbb","text":"","code":"hbb(   y,   groups,   sample_alphas = FALSE,   shape_alphas = NULL,   rate_alphas = NULL,   alphas = NULL,   M = 30 )"},{"path":"https://drkowal.github.io/SeBR/reference/hbb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hierarchical Bayesian bootstrap posterior sampler — hbb","text":"y data infer group-specific CDFs groups group assignment element y sample_alphas logical; TRUE, sample concentration hyperparameters marginal posterior distribution shape_alphas (optional) shape parameter Gamma prior alphas (sampled) rate_alphas (optional) rate parameter Gamma prior alphas (sampled) alphas (optional) vector fixed concentration hyperparameters corresponding unique levels groups (used sample_alphas = FALSE) M positive scaling term set default value alphas unspecified (alphas = NULL) sampled (sample_alphas = FALSE)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/hbb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Hierarchical Bayesian bootstrap posterior sampler — hbb","text":"list following elements: Fyc: list functions entry corresponds group  group-specific function can evaluate sampled CDF argument(s) weights_y: sampled weights common (BB) distribution (n-dimensional) weights_yc: sampled weights K groups (K x n) alphas: (fixed sampled) concentration hyperparameters","code":""},{"path":"https://drkowal.github.io/SeBR/reference/hbb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Hierarchical Bayesian bootstrap posterior sampler — hbb","text":"Assuming data y independent unknown, group-specific distributions, hierarchical Bayesian bootstrap (HBB) Oganisian et al. (https://doi.org/10.1515/ijb-2022-0051) nonparametric model distribution. HBB includes hierarchical shrinkage across groups toward common distribution (bb). HBB admits direct Monte Carlo (MCMC) sampling. shrinkage toward common distribution determined concentration hyperparameters alphas. component alphas corresponds one groups. Larger values encourage shrinkage toward common distribution, smaller values allow substantial deviations group. sample_alphas=TRUE, component alphas sampled marginal posterior distribution, assuming independent Gamma(shape_alphas, rate_alphas) priors. step uses simple grid approximation enable efficient sampling preserves joint Monte Carlo sampling group-specific common distributions. See concen_hbb details. Note diffuse priors alphas tends produce aggressive shrinkage toward common distribution (complete pooling). moderate shrinkage, use default values shape_alphas = 30*K rate_alphas = 1 K number groups. sample_alphas=FALSE, concentration hyperparameters fixed user-specified values. can done specifying alphas directly. Alternatively, alphas left unspecified (alphas = NULL), adopt default Oganisian et al. sets cth entry M*n/nc M user-specified nc number observations group c. guidance choice M: M = 0.01/K approximates separate BB's group (pooling); M 10 100 gives moderate shrinkage (partial pooling); M = 100*max(nc) approximates common BB (complete pooling).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/hbb.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Hierarchical Bayesian bootstrap posterior sampler — hbb","text":"supplying alphas distinct entries, make sure groups ordered properly; entries match sort(unique(groups)).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/hbb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Hierarchical Bayesian bootstrap posterior sampler — hbb","text":"Oganisian et al. (https://doi.org/10.1515/ijb-2022-0051)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/hbb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hierarchical Bayesian bootstrap posterior sampler — hbb","text":"","code":"# Sample size and number of groups: n = 500 K = 3  # Define the groups, then assign: ugroups = paste('g', 1:K, sep='') # groups groups = sample(ugroups, n, replace = TRUE) # assignments  # Simulate the data: iid normal, then add group-specific features y = rnorm(n = n) # data for(g in ugroups)   y[groups==g] = y[groups==g] + 3*rnorm(1) # group-specific  # One draw from the HBB posterior of the CDF: samp_hbb = hbb(y, groups)  names(samp_hbb) # items returned #> [1] \"Fyc\"        \"weights_y\"  \"weights_yc\" \"alphas\"     Fyc = samp_hbb$Fyc # list of CDFs class(Fyc) # this is a list #> [1] \"list\" class(Fyc[[1]]) # each element is a function #> [1] \"function\"  c = 1 # try: vary in 1:K Fyc[[c]](0) # some example use (for this one draw) #> [1] 0.04595643 Fyc[[c]](c(.5, 1.2)) #> [1] 0.07032374 0.10472581  # Plot several draws from the HBB posterior distribution: ys = seq(min(y), max(y), length.out=1000) plot(ys, ys, type='n', ylim = c(0,1),      main = 'Draws from HBB posteriors', xlab = 'y', ylab = 'F_c(y)') for(s in 1:50){ # some draws    # BB CDF:   Fy = bb(y)   lines(ys, Fy(ys), lwd=3) # plot CDF    # HBB:   Fyc = hbb(y, groups)$Fyc    # Plot CDFs by group:   for(c in 1:K) lines(ys, Fyc[[c]](ys), col=c+1, lwd=3) }  # For reference, add the ECDFs by group: for(c in 1:K) lines(ys, ecdf(y[groups==ugroups[c]])(ys), lty=2)  legend('bottomright', c('BB', paste('HBB:', ugroups)), col = 1:(K+1), lwd=3)"},{"path":"https://drkowal.github.io/SeBR/reference/plot_pptest.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot point and interval predictions on testing data — plot_pptest","title":"Plot point and interval predictions on testing data — plot_pptest","text":"Given posterior predictive samples X_test, plot point interval estimates compare actual testing data y_test.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/plot_pptest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot point and interval predictions on testing data — plot_pptest","text":"","code":"plot_pptest(post_ypred, y_test, alpha_level = 0.1)"},{"path":"https://drkowal.github.io/SeBR/reference/plot_pptest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot point and interval predictions on testing data — plot_pptest","text":"post_ypred nsave x n_test samples posterior predictive distribution test points X_test y_test n_test testing points alpha_level alpha-level prediction intervals","code":""},{"path":"https://drkowal.github.io/SeBR/reference/plot_pptest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot point and interval predictions on testing data — plot_pptest","text":"plot testing data, point interval predictions, summary empirical coverage","code":""},{"path":"https://drkowal.github.io/SeBR/reference/plot_pptest.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot point and interval predictions on testing data — plot_pptest","text":"","code":"# \\donttest{ # Simulate some data: dat = simulate_tlm(n = 100, p = 5, g_type = 'step')  # Fit a semiparametric Bayesian linear model: fit = sblm(y = dat$y, X = dat$X, X_test = dat$X_test) #> [1] \"3 seconds remaining\" #> [1] \"2 seconds remaining\" #> [1] \"1 seconds remaining\" #> [1] \"Total time:  3 seconds\"  # Evaluate posterior predictive means and intervals on the testing data: plot_pptest(fit$post_ypred, dat$y_test,             alpha_level = 0.10) # coverage should be about 90%  #> [1] 0.887 # }"},{"path":"https://drkowal.github.io/SeBR/reference/rank_approx.html","id":null,"dir":"Reference","previous_headings":"","what":"Rank-based estimation of the linear regression coefficients — rank_approx","title":"Rank-based estimation of the linear regression coefficients — rank_approx","text":"transformed Gaussian linear model, compute point estimates regression coefficients.  approach uses ranks data require transformation, must expand sample size n^2 thus can slow.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/rank_approx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rank-based estimation of the linear regression coefficients — rank_approx","text":"","code":"rank_approx(y, X)"},{"path":"https://drkowal.github.io/SeBR/reference/rank_approx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rank-based estimation of the linear regression coefficients — rank_approx","text":"y n x 1 response vector X n x p matrix predictors (include intercept!)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/rank_approx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rank-based estimation of the linear regression coefficients — rank_approx","text":"estimated linear coefficients","code":""},{"path":"https://drkowal.github.io/SeBR/reference/rank_approx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rank-based estimation of the linear regression coefficients — rank_approx","text":"","code":"# Simulate some data: dat = simulate_tlm(n = 200, p = 10, g_type = 'step')  # Point estimates for the linear coefficients: theta_hat = suppressWarnings(   rank_approx(y = dat$y,               X = dat$X[,-1]) # remove intercept ) # warnings occur from glm.fit (fitted probabilities 0 or 1)  # Check: correlation with true coefficients cor(dat$beta_true[-1], # excluding the intercept     theta_hat) #> [1] 0.9902873"},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":null,"dir":"Reference","previous_headings":"","what":"Semiparametric Bayesian Gaussian processes — sbgp","title":"Semiparametric Bayesian Gaussian processes — sbgp","text":"Monte Carlo sampling Bayesian Gaussian process regression unknown (nonparametric) transformation.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Semiparametric Bayesian Gaussian processes — sbgp","text":"","code":"sbgp(   y,   locs,   X = NULL,   covfun_name = \"matern_isotropic\",   locs_test = locs,   X_test = NULL,   nn = 30,   emp_bayes = TRUE,   approx_g = FALSE,   nsave = 1000,   ngrid = 100 )"},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Semiparametric Bayesian Gaussian processes — sbgp","text":"y n x 1 response vector locs n x d matrix locations X n x p design matrix; unspecified, use intercept covfun_name string name covariance function; see ?GpGp locs_test n_test x d matrix locations predictions needed; default locs X_test n_test x p design matrix test data; default X nn number nearest neighbors use; default 30 (larger values improve approximation increase computing cost) emp_bayes logical; TRUE, use (faster!) empirical Bayes approach estimating mean function approx_g logical; TRUE, apply large-sample approximation transformation nsave number Monte Carlo simulations ngrid number grid points inverse approximations","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Semiparametric Bayesian Gaussian processes — sbgp","text":"list following elements: coefficients estimated regression coefficients fitted.values posterior predictive mean test points locs_test fit_gp fitted GpGp_fit object, includes covariance parameter estimates model information post_ypred: nsave x ntest samples posterior predictive distribution locs_test post_g: nsave posterior samples transformation evaluated unique y values model: model fit (, sbgp) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Semiparametric Bayesian Gaussian processes — sbgp","text":"function provides Bayesian inference transformed Gaussian process model using Monte Carlo (MCMC) sampling. transformation modeled unknown learned jointly regression function (unless approx_g = TRUE, uses point approximation). model applies real-valued data, positive data, compactly-supported data (support automatically deduced observed y values). results typically unchanged whether laplace_approx TRUE/FALSE; setting TRUE may reduce sensitivity prior, setting FALSE may speed computations large datasets. computational efficiency, Gaussian process parameters fixed point estimates, latent Gaussian process sampled emp_bayes = FALSE. However, uncertainty term often negligible compared observation errors, transformation serves additional layer robustness.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Semiparametric Bayesian Gaussian processes — sbgp","text":"","code":"# \\donttest{ # Simulate some data: n = 200 # sample size x = seq(0, 1, length = n) # observation points  # Transform a noisy, periodic function: y = g_inv_bc(   sin(2*pi*x) + sin(4*pi*x) + rnorm(n, sd = .5),              lambda = .5) # Signed square-root transformation  # Package we use for fast computing w/ Gaussian processes: library(GpGp)  # Fit the semiparametric Bayesian Gaussian process: fit = sbgp(y = y, locs = x) #> [1] \"Initial GP fit...\" #> [1] \"Updated GP fit...\" #> [1] \"Sampling...\" #> [1] \"Done!\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"fit_gp\"        \"post_ypred\"    #>  [5] \"post_g\"        \"model\"         \"y\"             \"X\"             #>  [9] \"approx_g\"      \"sigma_epsilon\" coef(fit) # estimated regression coefficients (here, just an intercept) #> [1] -0.001659061 class(fit$fit_gp) # the GpGp object is also returned #> [1] \"GpGp_fit\"  # Plot the model predictions (point and interval estimates): pi_y = t(apply(fit$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI plot(x, y, type='n', ylim = range(pi_y,y),      xlab = 'x', ylab = 'y', main = paste('Fitted values and prediction intervals')) polygon(c(x, rev(x)),c(pi_y[,2], rev(pi_y[,1])),col='gray', border=NA) lines(x, y, type='p') lines(x, fitted(fit), lwd = 3)  # }"},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":null,"dir":"Reference","previous_headings":"","what":"Semiparametric Bayesian linear model — sblm","title":"Semiparametric Bayesian linear model — sblm","text":"Monte Carlo sampling Bayesian linear regression unknown (nonparametric) transformation. g-prior assumed regression coefficients.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Semiparametric Bayesian linear model — sblm","text":"","code":"sblm(   y,   X,   X_test = X,   psi = length(y),   laplace_approx = TRUE,   approx_g = FALSE,   nsave = 1000,   ngrid = 100,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Semiparametric Bayesian linear model — sblm","text":"y n x 1 response vector X n x p matrix predictors X_test n_test x p matrix predictors test data; default observed covariates X psi prior variance (g-prior) laplace_approx logical; TRUE, use normal approximation posterior definition transformation; otherwise prior used approx_g logical; TRUE, apply large-sample approximation transformation nsave number Monte Carlo simulations ngrid number grid points inverse approximations verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Semiparametric Bayesian linear model — sblm","text":"list following elements: coefficients posterior mean regression coefficients fitted.values posterior predictive mean test points X_test post_theta: nsave x p samples posterior distribution regression coefficients post_ypred: nsave x n_test samples posterior predictive distribution test points X_test post_g: nsave posterior samples transformation evaluated unique y values model: model fit (, sblm) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Semiparametric Bayesian linear model — sblm","text":"function provides fully Bayesian inference transformed linear model using Monte Carlo (MCMC) sampling. transformation modeled unknown learned jointly regression coefficients (unless approx_g = TRUE, uses point approximation). model applies real-valued data, positive data, compactly-supported data (support automatically deduced observed y values). results typically unchanged whether laplace_approx TRUE/FALSE; setting TRUE may reduce sensitivity prior, setting FALSE may speed computations large datasets.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Semiparametric Bayesian linear model — sblm","text":"","code":"# \\donttest{ # Simulate some data: dat = simulate_tlm(n = 100, p = 5, g_type = 'step') y = dat$y; X = dat$X # training data y_test = dat$y_test; X_test = dat$X_test # testing data  hist(y, breaks = 25) # marginal distribution   # Fit the semiparametric Bayesian linear model: fit = sblm(y = y, X = X, X_test = X_test) #> [1] \"5 seconds remaining\" #> [1] \"3 seconds remaining\" #> [1] \"1 seconds remaining\" #> [1] \"Total time:  4 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_g\"        \"model\"         \"y\"             \"X\"             #>  [9] \"X_test\"        \"psi\"           \"approx_g\"      \"sigma_epsilon\"  # Note: this is Monte Carlo sampling, so no need for MCMC diagnostics!  # Evaluate posterior predictive means and intervals on the testing data: plot_pptest(fit$post_ypred, y_test,             alpha_level = 0.10) # coverage should be about 90%  #> [1] 0.885  # Check: correlation with true coefficients cor(dat$beta_true[-1],     coef(fit)[-1]) # excluding the intercept #> [1] 0.9851408  # Summarize the transformation: y0 = sort(unique(y)) # posterior draws of g are evaluated at the unique y observations plot(y0, fit$post_g[1,], type='n', ylim = range(fit$post_g),      xlab = 'y', ylab = 'g(y)', main = \"Posterior draws of the transformation\") temp = sapply(1:nrow(fit$post_g), function(s)   lines(y0, fit$post_g[s,], col='gray')) # posterior draws lines(y0, colMeans(fit$post_g), lwd = 3) # posterior mean  # Add the true transformation, rescaled for easier comparisons: lines(y,       scale(dat$g_true)*sd(colMeans(fit$post_g)) + mean(colMeans(fit$post_g)), type='p', pch=2) legend('bottomright', c('Truth'), pch = 2) # annotate the true transformation   # Posterior predictive checks on testing data: empirical CDF y0 = sort(unique(y_test)) plot(y0, y0, type='n', ylim = c(0,1),      xlab='y', ylab='F_y', main = 'Posterior predictive ECDF') temp = sapply(1:nrow(fit$post_ypred), function(s)   lines(y0, ecdf(fit$post_ypred[s,])(y0), # ECDF of posterior predictive draws         col='gray', type ='s')) lines(y0, ecdf(y_test)(y0),  # ECDF of testing data      col='black', type = 's', lwd = 3)  # }"},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":null,"dir":"Reference","previous_headings":"","what":"Semiparametric Bayesian quantile regression — sbqr","title":"Semiparametric Bayesian quantile regression — sbqr","text":"MCMC sampling Bayesian quantile regression unknown (nonparametric) transformation. Like traditional Bayesian quantile regression, asymmetric Laplace distribution assumed errors, regression models targets specified quantile. However, models often woefully inadequate describing observed data. introduce nonparametric transformation improve model adequacy still providing inference regression coefficients specified quantile. g-prior assumed regression coefficients.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Semiparametric Bayesian quantile regression — sbqr","text":"","code":"sbqr(   y,   X,   tau = 0.5,   X_test = X,   psi = length(y),   laplace_approx = TRUE,   approx_g = FALSE,   nsave = 1000,   nburn = 100,   ngrid = 100,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Semiparametric Bayesian quantile regression — sbqr","text":"y n x 1 response vector X n x p matrix predictors tau target quantile (zero one) X_test n_test x p matrix predictors test data; default observed covariates X psi prior variance (g-prior) laplace_approx logical; TRUE, use normal approximation posterior definition transformation; otherwise prior used approx_g logical; TRUE, apply large-sample approximation transformation nsave number MCMC iterations save nburn number MCMC iterations discard ngrid number grid points inverse approximations verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Semiparametric Bayesian quantile regression — sbqr","text":"list following elements: coefficients posterior mean regression coefficients fitted.values estimated tauth quantile test points X_test post_theta: nsave x p samples posterior distribution regression coefficients post_ypred: nsave x n_test samples posterior predictive distribution test points X_test post_qtau: nsave x n_test samples tauth conditional quantile test points X_test post_g: nsave posterior samples transformation evaluated unique y values model: model fit (, sbqr) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Semiparametric Bayesian quantile regression — sbqr","text":"function provides fully Bayesian inference transformed quantile linear model. transformation modeled unknown learned jointly regression coefficients (unless approx_g = TRUE, uses point approximation). model applies real-valued data, positive data, compactly-supported data (support automatically deduced observed y values). results typically unchanged whether laplace_approx TRUE/FALSE; setting TRUE may reduce sensitivity prior, setting FALSE may speed computations large datasets.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Semiparametric Bayesian quantile regression — sbqr","text":"","code":"# \\donttest{ # Simulate some heteroskedastic data (no transformation): dat = simulate_tlm(n = 200, p = 10, g_type = 'box-cox', heterosked = TRUE, lambda = 1) y = dat$y; X = dat$X # training data y_test = dat$y_test; X_test = dat$X_test # testing data  # Target this quantile: tau = 0.05  # Fit the semiparametric Bayesian quantile regression model: fit = sbqr(y = y, X = X, tau = tau, X_test = X_test) #> [1] \"24 seconds remaining\" #> [1] \"30 seconds remaining\" #> [1] \"16 seconds remaining\" #> [1] \"0 seconds remaining\" #> [1] \"Total time:  55 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_qtau\"     \"post_g\"        \"model\"         \"y\"             #>  [9] \"X\"             \"X_test\"        \"psi\"           \"approx_g\"      #> [13] \"tau\"            # Posterior predictive checks on testing data: empirical CDF y0 = sort(unique(y_test)) plot(y0, y0, type='n', ylim = c(0,1),      xlab='y', ylab='F_y', main = 'Posterior predictive ECDF') temp = sapply(1:nrow(fit$post_ypred), function(s)   lines(y0, ecdf(fit$post_ypred[s,])(y0), # ECDF of posterior predictive draws         col='gray', type ='s')) lines(y0, ecdf(y_test)(y0),  # ECDF of testing data      col='black', type = 's', lwd = 3)  # }"},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":null,"dir":"Reference","previous_headings":"","what":"Semiparametric Bayesian spline model — sbsm","title":"Semiparametric Bayesian spline model — sbsm","text":"Monte Carlo sampling Bayesian spline regression unknown (nonparametric) transformation.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Semiparametric Bayesian spline model — sbsm","text":"","code":"sbsm(   y,   x = NULL,   x_test = NULL,   psi = NULL,   laplace_approx = TRUE,   approx_g = FALSE,   nsave = 1000,   ngrid = 100,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Semiparametric Bayesian spline model — sbsm","text":"y n x 1 response vector x n x 1 vector observation points; NULL, assume equally-spaced [0,1] x_test n_test x 1 vector testing points; NULL, assume equal x psi prior variance (inverse smoothing parameter); NULL, sample parameter laplace_approx logical; TRUE, use normal approximation posterior definition transformation; otherwise prior used approx_g logical; TRUE, apply large-sample approximation transformation nsave number Monte Carlo simulations ngrid number grid points inverse approximations verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Semiparametric Bayesian spline model — sbsm","text":"list following elements: coefficients posterior mean regression coefficients fitted.values posterior predictive mean test points x_test post_theta: nsave x p samples posterior distribution regression coefficients post_ypred: nsave x n_test samples posterior predictive distribution x_test post_g: nsave posterior samples transformation evaluated unique y values model: model fit (, sbsm) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Semiparametric Bayesian spline model — sbsm","text":"function provides fully Bayesian inference transformed spline regression model using Monte Carlo (MCMC) sampling. transformation modeled unknown learned jointly regression function (unless approx_g = TRUE, uses point approximation). model applies real-valued data, positive data, compactly-supported data (support automatically deduced observed y values). results typically unchanged whether laplace_approx TRUE/FALSE; setting TRUE may reduce sensitivity prior, setting FALSE may speed computations large datasets.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Semiparametric Bayesian spline model — sbsm","text":"","code":"# \\donttest{ # Simulate some data: n = 100 # sample size x = sort(runif(n)) # observation points  # Transform a noisy, periodic function: y = g_inv_bc(   sin(2*pi*x) + sin(4*pi*x) + rnorm(n, sd = .5),              lambda = .5) # Signed square-root transformation  # Fit the semiparametric Bayesian spline model: fit = sbsm(y = y, x = x) #> [1] \"2 seconds remaining\" #> [1] \"1 seconds remaining\" #> [1] \"0 seconds remaining\" #> [1] \"Total time:  2 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_g\"        \"post_psi\"      \"model\"         \"y\"             #>  [9] \"X\"             \"psi\"           \"approx_g\"      \"sigma_epsilon\"  # Note: this is Monte Carlo sampling, so no need for MCMC diagnostics!  # Plot the model predictions (point and interval estimates): pi_y = t(apply(fit$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI plot(x, y, type='n', ylim = range(pi_y,y),      xlab = 'x', ylab = 'y', main = paste('Fitted values and prediction intervals')) polygon(c(x, rev(x)),c(pi_y[,2], rev(pi_y[,1])),col='gray', border=NA) lines(x, y, type='p') lines(x, fitted(fit), lwd = 3)  # }"},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate a transformed linear model — simulate_tlm","title":"Simulate a transformed linear model — simulate_tlm","text":"Generate training data (X, y) testing data (X_test, y_test) transformed linear model. covariates correlated Gaussian variables. Half true regression coefficients zero half one. multiple options transformation, define support data (see ).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate a transformed linear model — simulate_tlm","text":"","code":"simulate_tlm(   n,   p,   g_type = \"beta\",   n_test = 1000,   heterosked = FALSE,   lambda = 1 )"},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate a transformed linear model — simulate_tlm","text":"n number observations training data p number covariates g_type type transformation; must one beta, step, box-cox n_test number observations testing data heterosked logical; TRUE, simulate latent data heteroskedasticity lambda Box-Cox parameter (applies g_type = 'box-cox')","code":""},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate a transformed linear model — simulate_tlm","text":"list following elements: y: response variable training data X: covariates training data y_test: response variable testing data X_test: covariates testing data beta_true: true regression coefficients g_true: true transformation, evaluated y","code":""},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulate a transformed linear model — simulate_tlm","text":"transformations vary complexity support observed data, include following options: beta yields marginally Beta(0.1, 0.5) data supported [0,1]; step generates locally-linear inverse transformation produces positive data; box-cox refers signed Box-Cox family indexed lambda, generates real-valued data examples including identity, square-root, log transformations.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate a transformed linear model — simulate_tlm","text":"","code":"# Simulate data: dat = simulate_tlm(n = 100, p = 5, g_type = 'beta') names(dat) # what is returned #> [1] \"y\"         \"X\"         \"y_test\"    \"X_test\"    \"beta_true\" \"g_true\"    hist(dat$y, breaks = 25) # marginal distribution"},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":null,"dir":"Reference","previous_headings":"","what":"Post-processing with importance sampling — sir_adjust","title":"Post-processing with importance sampling — sir_adjust","text":"Given Monte Carlo draws surrogate posterior, apply sampling importance reweighting (SIR) correct true model likelihood.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Post-processing with importance sampling — sir_adjust","text":"","code":"sir_adjust(fit, sir_frac = 0.3, nsims_prior = 100, verbose = TRUE)"},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Post-processing with importance sampling — sir_adjust","text":"fit fitted model object includes coefficients posterior mean regression coefficients post_theta: nsave x p samples posterior distribution regression coefficients post_ypred: nsave x n_test samples posterior predictive distribution test points X_test post_g: nsave posterior samples transformation evaluated unique y values model: model fit (sblm sbsm) sir_frac fraction draws sample SIR nsims_prior number draws prior verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Post-processing with importance sampling — sir_adjust","text":"fitted model object posterior draws subsampled based SIR adjustment","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Post-processing with importance sampling — sir_adjust","text":"Monte Carlo sampling sblm sbsm uses surrogate likelihood posterior inference, enables much faster easier computing. SIR provides correction actual (specified) likelihood. However, correction step quite slow typically produce noticeable discrepancies, even small sample sizes.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Post-processing with importance sampling — sir_adjust","text":"SIR sampling done WITHOUT replacement, sir_frac typically 0.1 0.5. nsims_priors draws used approximate prior expectation, larger values can significantly slow function.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Post-processing with importance sampling — sir_adjust","text":"","code":"# \\donttest{ # Simulate some data: dat = simulate_tlm(n = 50, p = 5, g_type = 'step') y = dat$y; X = dat$X # training data y_test = dat$y_test; X_test = dat$X_test # testing data  hist(y, breaks = 10) # marginal distribution   # Fit the semiparametric Bayesian linear model: fit = sblm(y = y, X = X, X_test = X_test) #> [1] \"4 seconds remaining\" #> [1] \"2 seconds remaining\" #> [1] \"1 seconds remaining\" #> [1] \"Total time:  3 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_g\"        \"model\"         \"y\"             \"X\"             #>  [9] \"X_test\"        \"psi\"           \"approx_g\"      \"sigma_epsilon\"  # Update with SIR: fit_sir = sir_adjust(fit) #> [1] \"26 seconds remaining\" #> [1] \"23 seconds remaining\" #> [1] \"20 seconds remaining\" #> [1] \"17 seconds remaining\" #> [1] \"14 seconds remaining\" #> [1] \"11 seconds remaining\" #> [1] \"9 seconds remaining\" #> [1] \"6 seconds remaining\" #> [1] \"3 seconds remaining\" #> [1] \"0 seconds remaining\" #> [1] \"Total time:  29 seconds\"  # Prediction: unadjusted vs. adjusted?  # Point estimates: y_hat = fitted(fit) y_hat_sir = fitted(fit_sir) cor(y_hat, y_hat_sir) # similar #> [1] 1  # Interval estimates: pi_y = t(apply(fit$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI pi_y_sir = t(apply(fit_sir$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI  # PI overlap (%): overlaps = 100*sapply(1:length(y_test), function(i){   # innermost part   (min(pi_y[i,2], pi_y_sir[i,2]) - max(pi_y[i,1], pi_y_sir[i,1]))/     # outermost part     (max(pi_y[i,2], pi_y_sir[i,2]) - min(pi_y[i,1], pi_y_sir[i,1])) }) summary(overlaps) # mostly close to 100% #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>   39.77   91.94   97.47   94.57   99.73  100.00   # Coverage of PIs on testing data (should be ~ 90%) mean((pi_y[,1] <= y_test)*(pi_y[,2] >= y_test)) # unadjusted #> [1] 0.903 mean((pi_y_sir[,1] <= y_test)*(pi_y_sir[,2] >= y_test)) # adjusted #> [1] 0.901  # Plot together with testing data: plot(y_test, y_test, type='n', ylim = range(pi_y, pi_y_sir, y_test),      xlab = 'y_test', ylab = 'y_hat', main = paste('Prediction intervals: testing data')) abline(0,1) # reference line suppressWarnings(   arrows(y_test, pi_y[,1], y_test, pi_y[,2],          length=0.15, angle=90, code=3, col='gray', lwd=2) ) # plot the PIs (unadjusted) suppressWarnings(   arrows(y_test, pi_y_sir[,1], y_test, pi_y_sir[,2],          length=0.15, angle=90, code=3, col='darkgray', lwd=2) ) # plot the PIs (adjusted) lines(y_test, y_hat, type='p', pch=2) # plot the means (unadjusted) lines(y_test, y_hat_sir, type='p', pch=3) # plot the means (adjusted)  # }"},{"path":"https://drkowal.github.io/SeBR/reference/uni.slice.html","id":null,"dir":"Reference","previous_headings":"","what":"Univariate Slice Sampler from Neal (2008) — uni.slice","title":"Univariate Slice Sampler from Neal (2008) — uni.slice","text":"Compute draw univariate distribution using code provided Radford M. Neal. documentation also reproduced Neal (2008).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/uni.slice.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Univariate Slice Sampler from Neal (2008) — uni.slice","text":"","code":"uni.slice(x0, g, w = 1, m = Inf, lower = -Inf, upper = +Inf, gx0 = NULL)"},{"path":"https://drkowal.github.io/SeBR/reference/uni.slice.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Univariate Slice Sampler from Neal (2008) — uni.slice","text":"x0 Initial point g Function returning log probability density (plus constant) w Size steps creating interval (default 1) m Limit steps (default infinite) lower Lower bound support distribution (default -Inf) upper Upper bound support distribution (default +Inf) gx0 Value g(x0), known (default known)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/uni.slice.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Univariate Slice Sampler from Neal (2008) — uni.slice","text":"point sampled, log density attached attribute.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/uni.slice.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Univariate Slice Sampler from Neal (2008) — uni.slice","text":"log density function may return -Inf points outside support distribution.  lower /upper bound specified support, log density function called outside limits.","code":""},{"path":"https://drkowal.github.io/SeBR/news/index.html","id":"sebr-100","dir":"Changelog","previous_headings":"","what":"SeBR 1.0.0","title":"SeBR 1.0.0","text":"CRAN release: 2023-07-03 Initial CRAN submission.","code":""}]
