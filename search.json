[{"path":"https://drkowal.github.io/SeBR/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 SeBR authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (â€œSoftwareâ€), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED â€œâ€, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://drkowal.github.io/SeBR/articles/SeBR.html","id":"background-semiparametric-regression-via-data-transformations","dir":"Articles","previous_headings":"","what":"Background: semiparametric regression via data transformations","title":"Introduction to SeBR","text":"Data transformations useful companion parametric regression models. well-chosen learned transformation can greatly enhance applicability given model, especially data irregular marginal features (e.g., multimodality, skewness) various data domains (e.g., real-valued, positive, compactly-supported data). interested providing fully Bayesian inference semiparametric regression models incorporate (1) unknown data transformation (2) useful parametric regression model. paired data {xi,yi}=1n\\{x_i, y_i\\}_{=1}^n xiâˆˆâ„px_i \\\\mathbb{R}^p yâˆˆð’´âŠ†â„y \\\\mathcal{Y} \\subseteq \\mathbb{R}, consider following class models: g(yi)=zi g(y_i) = z_i zi=fÎ¸(xi)+ÏƒÏµi z_i  = f_\\theta(x_i) + \\sigma \\epsilon_i  , gg (monotone increasing) data transformation learned, fÎ¸f_\\theta unknown regression function parametrized Î¸\\theta, Ïµi\\epsilon_i independent errors. Location scale restrictions (e.g., fÎ¸(0)=0f_\\theta(0) = 0 Ïƒ=1\\sigma =1) usually applied identifiability. Examples. focus following important special cases: linear model natural starting point: zi=xiâ€²Î¸+ÏƒÏµi,Ïµiâˆ¼iidN(0,1) z_i = x_i'\\theta + \\sigma\\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0, 1)  transformation gg broadens applicability useful class models, including positive compactly-supported data (see ). quantile regression model replaces Gaussian assumption linear model asymmetric Laplace distribution (ALD) zi=xiâ€²Î¸+ÏƒÏµi,Ïµiâˆ¼iidALD(Ï„) z_i = x_i'\\theta + \\sigma\\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} ALD(\\tau)  target Ï„\\tauth quantile zz xx, equivalently, gâˆ’1(Ï„)g^{-1}(\\tau)th quantile yy xx. ALD quite often poor model real data, especially Ï„\\tau near zero one. transformation gg offers pathway significantly improve model adequacy, still targeting desired quantile data. Gaussian process (GP) model generalizes linear model include nonparametric regression function, zi=fÎ¸(xi)+ÏƒÏµi,Ïµiâˆ¼iidN(0,1) z_i = f_\\theta(x_i) + \\sigma \\epsilon_i, \\quad  \\epsilon_i \\stackrel{iid}{\\sim} N(0, 1)  fÎ¸f_\\theta GP Î¸\\theta parameterizes mean covariance functions. Although GPs offer substantial flexibility regression function fÎ¸f_\\theta, default approach (without transformation) may inadequate yy irregular marginal features restricted domain (e.g., positive compact). Challenges: goal provide fully Bayesian posterior inference unknowns (g,Î¸)(g, \\theta) posterior predictive inference future/unobserved data yÌƒ(x)\\tilde y(x). prefer model algorithm offer () flexible modeling gg (ii) efficient posterior predictive computations. Innovations: approach (https://doi.org/10.1080/01621459.2024.2395586) specifies nonparametric model gg, yet also provides Monte Carlo (MCMC) sampling posterior predictive distributions. result, control approximation accuracy via number simulations, require lengthy runs, burn-periods, convergence diagnostics, inefficiency factors accompany MCMC. Monte Carlo sampling typically quite fast.","code":""},{"path":"https://drkowal.github.io/SeBR/articles/SeBR.html","id":"using-sebr","dir":"Articles","previous_headings":"","what":"Using SeBR","title":"Introduction to SeBR","text":"R package SeBR installed loaded follows: main functions SeBR : sblm(): Monte Carlo sampling posterior predictive inference semiparametric Bayesian linear model; sbsm(): Monte Carlo sampling posterior predictive inference semiparametric Bayesian spline model, replaces linear model spline nonlinear modeling xâˆˆâ„x \\\\mathbb{R}; sbqr(): blocked Gibbs sampling posterior predictive inference semiparametric Bayesian quantile regression; sbgp(): Monte Carlo sampling predictive inference semiparametric Bayesian Gaussian process model. function returns point estimate Î¸\\theta (coefficients), point predictions specified testing points (fitted.values), posterior samples transformation gg (post_g), posterior predictive samples yÌƒ(x)\\tilde y(x) testing points (post_ypred), well function-specific quantities (e.g., posterior draws Î¸\\theta, post_theta). calls coef() fitted() extract point estimates point predictions, respectively. Note: package also includes Box-Cox variants functions, .e., restricting gg (signed) Box-Cox parametric family g(t;Î»)={sign(t)|t|Î»âˆ’1}/Î»g(t; \\lambda) = \\{\\mbox{sign}(t) \\vert t \\vert^\\lambda - 1\\}/\\lambda known unknown Î»\\lambda. parametric transformation less flexible, especially irregular marginals restricted domains, requires MCMC sampling. functions (e.g., blm_bc(), etc.) primarily benchmarking.","code":"# install.packages(\"devtools\") # devtools::install_github(\"drkowal/SeBR\") library(SeBR)"},{"path":"https://drkowal.github.io/SeBR/articles/SeBR.html","id":"semiparametric-bayesian-linear-models-with-sblm","dir":"Articles","previous_headings":"","what":"Semiparametric Bayesian linear models with sblm","title":"Introduction to SeBR","text":"simulate data transformed linear model: sblm() quickly produces Monte Carlo samples (Î¸,g,yÌƒ(Xtest))(\\theta, g, \\tilde y(X_{test})) semiparametric Bayesian linear model: Monte Carlo (MCMC) samples, need perform MCMC diagnostics (e.g., verify convergence, inspect autocorrelations, discard burn-, re-run multiple chains, etc.). First, check model adequacy using posterior predictive diagnostics. Specifically, compute empirical CDF y_test (black) simulated testing predictive dataset post_ypred (gray):  Despite challenging features marginal distribution, proposed model appears adequate. Although gray lines clearly visible zero one, posterior predictive distribution indeed match support observed data. Remark: Posterior predictive diagnostics require training/testing splits typically performed -sample. X_test left unspecified sblm, posterior predictive draws given X can compared y. example uses --sample checks, rigorous less common. Next, evaluate predictive ability testing dataset computing plotting --sample prediction intervals X_test comparing y_test. built-function :  --sample predictive distributions well-calibrated. Finally, summarize posterior inference transformation gg regression coefficients Î¸\\theta compare ground truth values. First, plot posterior draws gg (gray), posterior mean gg (black), true transformation (triangles): posterior distribution gg accurately matches true transformation. Next, compute point interval summaries Î¸\\theta compare ground truth: point estimates Î¸\\theta closely track ground truth, inference based 95% credible intervals correctly selects truly nonzero regression coefficients. Remark: identifiability, location (intercept Î¸0=0\\theta_0= 0) scale (Ïƒ=1\\sigma =1) fixed regression model; otherwise identified location/scale transformation gg. Note: Try repeating exercise blm_bc() place sblm(). Box-Cox transformation recover transformation gg coefficients Î¸\\theta accurately, model diagnostics alarming, predictions deteriorate substantially.","code":"set.seed(123) # for reproducibility  # Simulate data from a transformed linear model: dat = simulate_tlm(n = 200,  # number of observations                    p = 10,   # number of covariates                     g_type = 'step' # type of transformation (here, positive data)                    ) # Training data: y = dat$y; X = dat$X   # Testing data: y_test = dat$y_test; X_test = dat$X_test # Fit the semiparametric Bayesian linear model: fit = sblm(y = y,             X = X,             X_test = X_test) #> [1] \"4 seconds remaining\" #> [1] \"Total time:  8 seconds\"  names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_g\"        \"model\"         \"y\"             \"X\"             #>  [9] \"X_test\"        \"psi\"           \"approx_g\" # Evaluate posterior predictive means and intervals on the testing data: plot_pptest(fit$post_ypred,              y_test,              alpha_level = 0.10) # coverage should be >= 90% #> [1] 0.93 # Summarize the parameters (regression coefficients):  # Posterior means: coef(fit) #>  [1]  0.94009109  0.87897201  0.95878521  0.67151935  0.77619097  0.15821173 #>  [7]  0.25339490 -0.09715452 -0.16245684  0.15840254  # Check: correlation with true coefficients cor(dat$beta_true, coef(fit)) #> [1] 0.9435587  # 95% credible intervals: theta_ci = t(apply(fit$post_theta, 2, quantile, c(.025, 0.975)))  # Check: agreement on nonzero coefficients? which(theta_ci[,1] >= 0 | theta_ci[,2] <=0) # 95% CI excludes zero #> [1] 1 2 3 4 5 which(dat$beta_true != 0) # truly nonzero #> [1] 1 2 3 4 5"},{"path":"https://drkowal.github.io/SeBR/articles/SeBR.html","id":"semiparametric-bayesian-quantile-regression-with-sbqr","dir":"Articles","previous_headings":"","what":"Semiparametric Bayesian quantile regression with sbqr","title":"Introduction to SeBR","text":"now consider Bayesian quantile regression, specifies linear model ALD errors. First, simulate data heteroskedastic linear model. Heteroskedasticity often produces conclusions differ traditional mean regression. , include transformation, data-generating process implicitly favor approach traditional Bayesian quantile regression (.e., g(t)=tg(t) = t identity). Next, load two packages â€™ll need: Now, fit two Bayesian quantile regression models: traditional version without transformation (bqr()) proposed alternative (sbqr()). target Ï„=0.05\\tau = 0.05 quantile. model fits, evaluate posterior predictive diagnostics . Specifically, compute empirical CDF y_test (black) simulated testing predictive dataset post_ypred sbqr (gray) bqr (red): Without transformation, Bayesian quantile regression model good model data. learned transformation completely resolves model inadequacyâ€”even though transformation present data-generating process. Finally, can asses quantile estimates testing data. First, consider bqr:  Recall quantile regression models Ï„\\tau, expect asymmetric y_test. --sample empirical quantile 0.026 (target Ï„=0.05\\tau = 0.05) 90% prediction interval coverage 0.978. Repeat evaluation sbqr:  Now --sample empirical quantile 0.044 90% prediction interval coverage 0.956. sbqr better calibrated Ï„\\tau, methods slightly overconservative prediction interval coverage. However, sbqr produce significantly smaller prediction intervals maintaining conservative coverage, thus provides powerful precise inference. Remark: point interval estimates quantile regression coefficients Î¸\\theta may computed exactly sblm() example. Note: try quantiles, Ï„âˆˆ{0.25,0.5}\\tau \\\\{0.25, 0.5\\}. Ï„\\tau approaches 0.5 (.e., median regression), problem becomes easier models better calibrated.","code":"# Simulate data from a heteroskedastic linear model (no transformation): dat = simulate_tlm(n = 200,  # number of observations                    p = 10,   # number of covariates                     g_type = 'box-cox', lambda = 1, # no transformation                    heterosked = TRUE # heteroskedastic errors                    ) # Training data: y = dat$y; X = dat$X   # Testing data: y_test = dat$y_test; X_test = dat$X_test library(quantreg) # traditional QR for initialization library(statmod) # for rinvgauss sampling # Quantile to target: tau = 0.05  # (Traditional) Bayesian quantile regression: fit_bqr = bqr(y = y,             X = X,             tau = tau,             X_test = X_test,            verbose = FALSE  # omit printout )  # Semiparametric Bayesian quantile regression: fit = sbqr(y = y,             X = X,             tau = tau,             X_test = X_test,            verbose = FALSE # omit printout )        names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_qtau\"     \"post_g\"        \"model\"         \"y\"             #>  [9] \"X\"             \"X_test\"        \"psi\"           \"approx_g\"      #> [13] \"tau\" # Quantile point estimates: q_hat_bqr = fitted(fit_bqr)   # Empirical quantiles on testing data: (emp_quant_bqr = mean(q_hat_bqr >= y_test)) #> [1] 0.026  # Evaluate posterior predictive means and intervals on the testing data: (emp_cov_bqr = plot_pptest(fit_bqr$post_ypred,                             y_test,                             alpha_level = 0.10)) #> [1] 0.978 # Quantile point estimates: q_hat = fitted(fit)   # Empirical quantiles on testing data: (emp_quant_sbqr = mean(q_hat >= y_test)) #> [1] 0.044  # Evaluate posterior predictive means and intervals on the testing data: (emp_cov_sbqr = plot_pptest(fit$post_ypred,                              y_test,                              alpha_level = 0.10)) #> [1] 0.956"},{"path":"https://drkowal.github.io/SeBR/articles/SeBR.html","id":"semiparametric-bayesian-gaussian-processes-with-sbgp","dir":"Articles","previous_headings":"","what":"Semiparametric Bayesian Gaussian processes with sbgp","title":"Introduction to SeBR","text":"Consider challenging scenario () nonlinear regression function xâˆˆâ„x \\\\mathbb{R} (ii) Beta marginals, support ð’´=[0,1]\\mathcal{Y} = [0,1]. Simulate data accordingly:  highlight challenges , first consider Box-Cox-transformed GP. well proposed model, require package: Now fit Box-Cox GP evaluate --sample predictive performance:  Box-Cox transformation adds flexibility GP, insufficient data. prediction intervals unnecessarily wide respect support ð’´=[0,1]\\mathcal{Y} = [0,1], estimated mean function fully capture trend data. Now fit semiparametric Bayesian GP model: Evaluate --sample predictive performance testing data:  Unlike Box-Cox version, sbgp respects support data ð’´=[0,1]\\mathcal{Y} = [0,1], captures trend, provides narrower intervals (average widths 0.233 compared 0.284) better coverage (0.91 sbgp 0.899 Box-Cox). Despite significant complexities data, sbgp performs quite well ---box: nonlinearity modeled adequately; support data enforced automatically; --sample prediction intervals sharp calibrated; computations fast. Note: sbgp also applies xâˆˆâ„px \\\\mathbb{R}^p p>1p >1, spatial spatio-temporal data. cases may require careful consideration mean covariance functions: default mean function linear regression intercept , default covariance function isotropic Matern function. However, many options available (inherited GpGp package).","code":"# Training data: n = 200 # sample size x = seq(0, 1, length = n) # observation points  # Testing data: n_test = 1000  x_test = seq(0, 1, length = n_test)   # True inverse transformation: g_inv_true = function(z)    qbeta(pnorm(z),          shape1 = 0.5,          shape2 = 0.1) # approx Beta(0.5, 0.1) marginals  # Training observations: y = g_inv_true(   sin(2*pi*x) + sin(4*pi*x) + .25*rnorm(n)              )   # Testing observations: y_test = g_inv_true(   sin(2*pi*x_test) + sin(4*pi*x_test) + .25*rnorm(n)              )   plot(x_test, y_test,       xlab = 'x', ylab = 'y',      main = \"Training (gray) and testing (black) data\") lines(x, y, type='p', col='gray', pch = 2) library(GpGp) # fast GP computing library(fields) # accompanies GpGp # Fit the Box-Cox Gaussian process model: fit_bc = bgp_bc(y = y,             locs = x,            locs_test = x_test) #> [1] \"Initial GP fit...\" #> [1] \"Updated GP fit...\"  # Fitted values on the testing data: y_hat_bc = fitted(fit_bc)  # 90% prediction intervals on the testing data: pi_y_bc = t(apply(fit_bc$post_ypred, 2, quantile, c(0.05, .95)))   # Average PI width: (width_bc = mean(pi_y_bc[,2] - pi_y_bc[,1])) #> [1] 0.284099  # Empirical PI coverage: (emp_cov_bc = mean((pi_y_bc[,1] <= y_test)*(pi_y_bc[,2] >= y_test))) #> [1] 0.899  # Plot these together with the actual testing points: plot(x_test, y_test, type='n',       ylim = range(pi_y_bc, y_test), xlab = 'x', ylab = 'y',       main = paste('Fitted values and prediction intervals: \\n Box-Cox Gaussian process'))  # Add the intervals: polygon(c(x_test, rev(x_test)),         c(pi_y_bc[,2], rev(pi_y_bc[,1])),         col='gray', border=NA) lines(x_test, y_test, type='p') # actual values lines(x_test, y_hat_bc, lwd = 3) # fitted values # library(GpGp) # loaded above  # Fit the semiparametric Gaussian process model: fit = sbgp(y = y,             locs = x,            locs_test = x_test) #> [1] \"Initial GP fit...\" #> [1] \"Updated GP fit...\" #> [1] \"Sampling...\" #> [1] \"Done!\"  names(fit) # what is returned #> [1] \"coefficients\"  \"fitted.values\" \"fit_gp\"        \"post_ypred\"    #> [5] \"post_g\"        \"model\"         \"y\"             \"X\"             #> [9] \"approx_g\"  coef(fit) # estimated regression coefficients (here, just an intercept) #> [1] -0.009660044 # Fitted values on the testing data: y_hat = fitted(fit)  # 90% prediction intervals on the testing data: pi_y = t(apply(fit$post_ypred, 2, quantile, c(0.05, .95)))   # Average PI width: (width = mean(pi_y[,2] - pi_y[,1])) #> [1] 0.2325542  # Empirical PI coverage: (emp_cov = mean((pi_y[,1] <= y_test)*(pi_y[,2] >= y_test))) #> [1] 0.91  # Plot these together with the actual testing points: plot(x_test, y_test, type='n',       ylim = range(pi_y, y_test), xlab = 'x', ylab = 'y',       main = paste('Fitted values and prediction intervals: \\n semiparametric Gaussian process'))  # Add the intervals: polygon(c(x_test, rev(x_test)),         c(pi_y[,2], rev(pi_y[,1])),         col='gray', border=NA) lines(x_test, y_test, type='p') # actual values lines(x_test, y_hat, lwd = 3) # fitted values"},{"path":"https://drkowal.github.io/SeBR/articles/SeBR.html","id":"references","dir":"Articles","previous_headings":"Semiparametric Bayesian Gaussian processes with sbgp","what":"References","title":"Introduction to SeBR","text":"Kowal, D. Wu, B. (2024). Monte Carlo inference semiparametric Bayesian regression. JASA. https://doi.org/10.1080/01621459.2024.2395586","code":""},{"path":"https://drkowal.github.io/SeBR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Dan Kowal. Author, maintainer, copyright holder.","code":""},{"path":"https://drkowal.github.io/SeBR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Kowal D (2025). SeBR: Semiparametric Bayesian Regression Analysis. R package version 1.0.0, https://github.com/drkowal/SeBR.","code":"@Manual{,   title = {SeBR: Semiparametric Bayesian Regression Analysis},   author = {Dan Kowal},   year = {2025},   note = {R package version 1.0.0},   url = {https://github.com/drkowal/SeBR}, }"},{"path":"https://drkowal.github.io/SeBR/index.html","id":"sebr-semiparametric-bayesian-regression","dir":"","previous_headings":"","what":"Semiparametric Bayesian Regression Analysis","title":"Semiparametric Bayesian Regression Analysis","text":"Overview. Data transformations useful companion parametric regression models. well-chosen learned transformation can greatly enhance applicability given model, especially data irregular marginal features (e.g., multimodality, skewness) various data domains (e.g., real-valued, positive, compactly-supported data). Given paired data (xi,yi)(x_i,y_i) =1,â€¦,ni=1,\\ldots,n, SeBR implements efficient fully Bayesian inference semiparametric regression models incorporate (1) unknown data transformation g(yi)=zi g(y_i) = z_i (2) useful parametric regression model zi=fÎ¸(xi)+ÏƒÏµi z_i  = f_\\theta(x_i) + \\sigma \\epsilon_i unknown parameters Î¸\\theta independent errors Ïµi\\epsilon_i. Examples. focus following important special cases: linear model natural starting point: zi=xiâ€²Î¸+ÏƒÏµi,Ïµiâˆ¼iidN(0,1) z_i = x_i'\\theta + \\sigma\\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0, 1) transformation gg broadens applicability useful class models, including positive compactly-supported data. quantile regression model replaces Gaussian assumption linear model asymmetric Laplace distribution (ALD) zi=xiâ€²Î¸+ÏƒÏµi,Ïµiâˆ¼iidALD(Ï„) z_i = x_i'\\theta + \\sigma\\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} ALD(\\tau) target Ï„\\tauth quantile zz xx, equivalently, gâˆ’1(Ï„)g^{-1}(\\tau)th quantile yy xx. ALD quite often poor model real data, especially Ï„\\tau near zero one. transformation gg offers pathway significantly improve model adequacy, still targeting desired quantile data. Gaussian process (GP) model generalizes linear model include nonparametric regression function, zi=fÎ¸(xi)+ÏƒÏµi,Ïµiâˆ¼iidN(0,1) z_i = f_\\theta(x_i) + \\sigma \\epsilon_i, \\quad  \\epsilon_i \\stackrel{iid}{\\sim} N(0, 1) fÎ¸f_\\theta GP Î¸\\theta parameterizes mean covariance functions. Although GPs offer substantial flexibility regression function fÎ¸f_\\theta, model may inadequate yy irregular marginal features restricted domain (e.g., positive compact). Challenges: goal provide fully Bayesian posterior inference unknowns (g,Î¸)(g, \\theta) posterior predictive inference future/unobserved data yÌƒ(x)\\tilde y(x). prefer model algorithm offer () flexible modeling gg (ii) efficient posterior predictive computations. Innovations: approach (https://doi.org/10.1080/01621459.2024.2395586) specifies nonparametric model gg, yet also provides Monte Carlo (MCMC) sampling posterior predictive distributions. result, control approximation accuracy via number simulations, require lengthy runs, burn-periods, convergence diagnostics, inefficiency factors accompany MCMC. Monte Carlo sampling typically quite fast.","code":""},{"path":"https://drkowal.github.io/SeBR/index.html","id":"using-sebr","dir":"","previous_headings":"","what":"Using SeBR","title":"Semiparametric Bayesian Regression Analysis","text":"package SeBR installed loaded follows: main functions SeBR : sblm(): Monte Carlo sampling posterior predictive inference semiparametric Bayesian linear model; sbsm(): Monte Carlo sampling posterior predictive inference semiparametric Bayesian spline model, replaces linear model spline nonlinear modeling xâˆˆâ„x \\\\mathbb{R}; sbqr(): blocked Gibbs sampling posterior predictive inference semiparametric Bayesian quantile regression; sbgp(): Monte Carlo sampling predictive inference semiparametric Bayesian Gaussian process model. function returns point estimate Î¸\\theta (coefficients), point predictions specified testing points (fitted.values), posterior samples transformation gg (post_g), posterior predictive samples yÌƒ(x)\\tilde y(x) testing points (post_ypred), well function-specific quantities (e.g., posterior draws Î¸\\theta, post_theta). calls coef() fitted() extract point estimates point predictions, respectively. Note: package also includes Box-Cox variants functions, .e., restricting gg (signed) Box-Cox parametric family g(t;Î»)={sign(t)|t|Î»âˆ’1}/Î»g(t; \\lambda) = \\{\\mbox{sign}(t) \\vert t \\vert^\\lambda - 1\\}/\\lambda known unknown Î»\\lambda. parametric transformation less flexible, especially irregular marginals restricted domains, requires MCMC sampling. functions (e.g., blm_bc(), etc.) primarily benchmarking. Detailed documentation examples available https://drkowal.github.io/SeBR/.","code":"# CRAN version: # install.packages(\"SeBR\")  # Development version:  # devtools::install_github(\"drkowal/SeBR\") library(SeBR)"},{"path":"https://drkowal.github.io/SeBR/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Semiparametric Bayesian Regression Analysis","text":"Kowal, D. Wu, B. (2024). Monte Carlo inference semiparametric Bayesian regression. JASA. https://doi.org/10.1080/01621459.2024.2395586","code":""},{"path":"https://drkowal.github.io/SeBR/reference/Fz_fun.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the latent data CDF â€” Fz_fun","title":"Compute the latent data CDF â€” Fz_fun","text":"Assuming Gaussian latent data distribution (given x), compute CDF grid points","code":""},{"path":"https://drkowal.github.io/SeBR/reference/Fz_fun.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the latent data CDF â€” Fz_fun","text":"","code":"Fz_fun(z, weights = NULL, mean_vec = NULL, sd_vec)"},{"path":"https://drkowal.github.io/SeBR/reference/Fz_fun.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the latent data CDF â€” Fz_fun","text":"z vector points CDF z evaluated weights n-dimensional vector weights; NULL, assume 1/n mean_vec n-dimensional vector means; NULL, assume mean zero sd_vec n-dimensional vector standard deviations","code":""},{"path":"https://drkowal.github.io/SeBR/reference/Fz_fun.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the latent data CDF â€” Fz_fun","text":"CDF z evaluated z","code":""},{"path":"https://drkowal.github.io/SeBR/reference/SSR_gprior.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the sum-squared-residuals term under Zellner's g-prior â€” SSR_gprior","title":"Compute the sum-squared-residuals term under Zellner's g-prior â€” SSR_gprior","text":"sum-squared-residuals (SSR) arise variance (precision) term 1) Zellner's g-prior coefficients Gamma prior error precision 2) marginalization coefficients.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/SSR_gprior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the sum-squared-residuals term under Zellner's g-prior â€” SSR_gprior","text":"","code":"SSR_gprior(y, X = NULL, psi)"},{"path":"https://drkowal.github.io/SeBR/reference/SSR_gprior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the sum-squared-residuals term under Zellner's g-prior â€” SSR_gprior","text":"y vector response variables X matrix covariates; NULL, return sum(y^2) psi prior variance (g-prior)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/SSR_gprior.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the sum-squared-residuals term under Zellner's g-prior â€” SSR_gprior","text":"positive scalar","code":""},{"path":"https://drkowal.github.io/SeBR/reference/all_subsets.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute all subsets of a set â€” all_subsets","title":"Compute all subsets of a set â€” all_subsets","text":"Given set variables, compute inclusion indicators possible subsets.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/all_subsets.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute all subsets of a set â€” all_subsets","text":"","code":"all_subsets(set)"},{"path":"https://drkowal.github.io/SeBR/reference/all_subsets.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute all subsets of a set â€” all_subsets","text":"set set compute subsets (e.g., 1:p)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/all_subsets.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute all subsets of a set â€” all_subsets","text":"data frame rows indicate 2^p different subsets columns indicate inclusion (logical) element subset","code":""},{"path":"https://drkowal.github.io/SeBR/reference/all_subsets.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute all subsets of a set â€” all_subsets","text":"Code adapted https://www.r-bloggers.com/2012/04/generating--subsets---set/","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bb.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian bootstrap posterior sampler for the CDF â€” bb","title":"Bayesian bootstrap posterior sampler for the CDF â€” bb","text":"Compute one Monte Carlo draw Bayesian bootstrap (BB) posterior distribution cumulative distribution function (CDF).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian bootstrap posterior sampler for the CDF â€” bb","text":"","code":"bb(y)"},{"path":"https://drkowal.github.io/SeBR/reference/bb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian bootstrap posterior sampler for the CDF â€” bb","text":"y data infer CDF (preferably sorted)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian bootstrap posterior sampler for the CDF â€” bb","text":"function can evaluate sampled CDF argument(s)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian bootstrap posterior sampler for the CDF â€” bb","text":"Assuming data y iid unknown distribution, Bayesian bootstrap (BB) nonparametric model distribution. BB limiting case Dirichlet process prior (without hyperparameters) admits direct Monte Carlo (MCMC) sampling. function computes one draw BB posterior distribution CDF Fy.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bb.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bayesian bootstrap posterior sampler for the CDF â€” bb","text":"code inspired ggdist::weighted_ecdf.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian bootstrap posterior sampler for the CDF â€” bb","text":"","code":"# Simulate data: y = rnorm(n = 100)  # One draw from the BB posterior: Fy = bb(y)  class(Fy) # this is a function #> [1] \"function\" Fy(0) # some example use (for this one draw) #> [1] 0.5173514 Fy(c(.5, 1.2)) #> [1] 0.6692355 0.8765828  # Plot several draws from the BB posterior distribution: ys = seq(-3, 3, length.out=1000) plot(ys, ys, type='n', ylim = c(0,1),      main = 'Draws from BB posterior', xlab = 'y', ylab = 'F(y)') for(s in 1:50) lines(ys, bb(y)(ys), col='gray')  # Add ECDF for reference: lines(ys, ecdf(y)(ys), lty=2)"},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian Gaussian processes with a Box-Cox transformation â€” bgp_bc","title":"Bayesian Gaussian processes with a Box-Cox transformation â€” bgp_bc","text":"MCMC sampling Bayesian Gaussian process regression (known unknown) Box-Cox transformation.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian Gaussian processes with a Box-Cox transformation â€” bgp_bc","text":"","code":"bgp_bc(   y,   locs,   X = NULL,   covfun_name = \"matern_isotropic\",   locs_test = locs,   X_test = NULL,   nn = 30,   emp_bayes = TRUE,   lambda = NULL,   sample_lambda = TRUE,   nsave = 1000,   nburn = 1000,   nskip = 0 )"},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian Gaussian processes with a Box-Cox transformation â€” bgp_bc","text":"y n x 1 response vector locs n x d matrix locations X n x p design matrix; unspecified, use intercept covfun_name string name covariance function; see ?GpGp locs_test n_test x d matrix locations predictions needed; default locs X_test n_test x p design matrix test data; default X nn number nearest neighbors use; default 30 (larger values improve approximation increase computing cost) emp_bayes logical; TRUE, use (faster!) empirical Bayes approach estimating mean function lambda Box-Cox transformation; NULL, estimate parameter sample_lambda logical; TRUE, sample lambda, otherwise use fixed value lambda MLE (lambda unspecified) nsave number MCMC iterations save nburn number MCMC iterations discard nskip number MCMC iterations skip saving iterations, .e., save every (nskip + 1)th draw","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian Gaussian processes with a Box-Cox transformation â€” bgp_bc","text":"list following elements: coefficients posterior mean regression coefficients fitted.values posterior predictive mean test points locs_test fit_gp fitted GpGp_fit object, includes covariance parameter estimates model information post_ypred: nsave x n_test samples posterior predictive distribution locs_test post_g: nsave posterior samples transformation evaluated unique y values post_lambda nsave posterior samples lambda model: model fit (, bgp_bc) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian Gaussian processes with a Box-Cox transformation â€” bgp_bc","text":"function provides Bayesian inference transformed Gaussian processes. transformation parametric Box-Cox family, one parameter lambda. parameter may fixed advanced learned data. computational efficiency, Gaussian process parameters fixed point estimates, latent Gaussian process sampled emp_bayes = FALSE.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bayesian Gaussian processes with a Box-Cox transformation â€” bgp_bc","text":"Box-Cox transformations may useful cases, general recommend nonparametric transformation (Monte Carlo, MCMC sampling) sbgp.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bgp_bc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian Gaussian processes with a Box-Cox transformation â€” bgp_bc","text":"","code":"# \\donttest{ # Simulate some data: n = 200 # sample size x = seq(0, 1, length = n) # observation points  # Transform a noisy, periodic function: y = g_inv_bc(   sin(2*pi*x) + sin(4*pi*x) + rnorm(n, sd = .5),              lambda = .5) # Signed square-root transformation  # Package we use for fast computing w/ Gaussian processes: library(GpGp)  # Fit a Bayesian Gaussian process with Box-Cox transformation: fit = bgp_bc(y = y, locs = x) #> [1] \"Initial GP fit...\" #> [1] \"Updated GP fit...\" names(fit) # what is returned #> [1] \"coefficients\"  \"fitted.values\" \"fit_gp\"        \"post_ypred\"    #> [5] \"post_g\"        \"post_lambda\"   \"model\"         \"y\"             #> [9] \"X\"             coef(fit) # estimated regression coefficients (here, just an intercept) #> [1] 0.3167961 class(fit$fit_gp) # the GpGp object is also returned #> [1] \"GpGp_fit\" round(quantile(fit$post_lambda), 3) # summary of unknown Box-Cox parameter #>    0%   25%   50%   75%  100%  #> 0.693 0.786 0.813 0.841 0.934   # Plot the model predictions (point and interval estimates): pi_y = t(apply(fit$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI plot(x, y, type='n', ylim = range(pi_y,y),      xlab = 'x', ylab = 'y', main = paste('Fitted values and prediction intervals')) polygon(c(x, rev(x)),c(pi_y[,2], rev(pi_y[,1])),col='gray', border=NA) lines(x, y, type='p') lines(x, fitted(fit), lwd = 3)  # }"},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian linear model with a Box-Cox transformation â€” blm_bc","title":"Bayesian linear model with a Box-Cox transformation â€” blm_bc","text":"MCMC sampling Bayesian linear regression (known unknown) Box-Cox transformation. g-prior assumed regression coefficients.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian linear model with a Box-Cox transformation â€” blm_bc","text":"","code":"blm_bc(   y,   X,   X_test = X,   psi = length(y),   lambda = NULL,   sample_lambda = TRUE,   nsave = 1000,   nburn = 1000,   nskip = 0,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian linear model with a Box-Cox transformation â€” blm_bc","text":"y n x 1 vector observed counts X n x p matrix predictors (intercept) X_test n_test x p matrix predictors test data; default observed covariates X psi prior variance (g-prior) lambda Box-Cox transformation; NULL, estimate parameter sample_lambda logical; TRUE, sample lambda, otherwise use fixed value lambda MLE (lambda unspecified) nsave number MCMC iterations save nburn number MCMC iterations discard nskip number MCMC iterations skip saving iterations, .e., save every (nskip + 1)th draw verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian linear model with a Box-Cox transformation â€” blm_bc","text":"list following elements: coefficients posterior mean regression coefficients fitted.values posterior predictive mean test points X_test post_theta: nsave x p samples posterior distribution regression coefficients post_ypred: nsave x n_test samples posterior predictive distribution test points X_test post_g: nsave posterior samples transformation evaluated unique y values post_lambda nsave posterior samples lambda post_sigma nsave posterior samples sigma model: model fit (, blm_bc) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian linear model with a Box-Cox transformation â€” blm_bc","text":"function provides fully Bayesian inference transformed linear model via MCMC sampling. transformation parametric Box-Cox family, one parameter lambda. parameter may fixed advanced learned data.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bayesian linear model with a Box-Cox transformation â€” blm_bc","text":"Box-Cox transformations may useful cases, general recommend nonparametric transformation (Monte Carlo, MCMC sampling) sblm. intercept automatically added X X_test. coefficients reported ** include intercept parameter, since identified general transformation models (e.g., sblm).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/blm_bc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian linear model with a Box-Cox transformation â€” blm_bc","text":"","code":"# Simulate some data: dat = simulate_tlm(n = 100, p = 5, g_type = 'step') y = dat$y; X = dat$X # training data y_test = dat$y_test; X_test = dat$X_test # testing data  hist(y, breaks = 25) # marginal distribution   # Fit the Bayesian linear model with a Box-Cox transformation: fit = blm_bc(y = y, X = X, X_test = X_test) #> [1] \"0 seconds remaining\" #> [1] \"Total time:  0 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_g\"        \"post_lambda\"   \"post_sigma\"    \"model\"         #>  [9] \"y\"             \"X\"             \"X_test\"        \"psi\"           round(quantile(fit$post_lambda), 3) # summary of unknown Box-Cox parameter #>    0%   25%   50%   75%  100%  #> 0.000 0.002 0.006 0.012 0.062"},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian quantile regression â€” bqr","title":"Bayesian quantile regression â€” bqr","text":"MCMC sampling Bayesian quantile regression. asymmetric Laplace distribution assumed errors, regression models targets specified quantile. g-prior assumed regression coefficients.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian quantile regression â€” bqr","text":"","code":"bqr(   y,   X,   tau = 0.5,   X_test = X,   psi = length(y),   nsave = 1000,   nburn = 1000,   nskip = 0,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian quantile regression â€” bqr","text":"y n x 1 vector observed counts X n x p matrix predictors (intercept) tau target quantile (zero one) X_test n_test x p matrix predictors test data; default observed covariates X psi prior variance (g-prior) nsave number MCMC iterations save nburn number MCMC iterations discard nskip number MCMC iterations skip saving iterations, .e., save every (nskip + 1)th draw verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian quantile regression â€” bqr","text":"list following elements: coefficients posterior mean regression coefficients fitted.values estimated tauth quantile test points X_test post_theta: nsave x p samples posterior distribution regression coefficients post_ypred: nsave x n_test samples posterior predictive distribution test points X_test post_qtau: nsave x n_test samples tauth conditional quantile test points X_test model: model fit (, bqr) well arguments passed","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bayesian quantile regression â€” bqr","text":"asymmetric Laplace distribution advantageous links regression model (X%*%theta) pre-specified quantile (tau). However, often poor model observed data, semiparametric version sbqr recommended general. intercept automatically added X X_test. coefficients reported ** include intercept parameter.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bqr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian quantile regression â€” bqr","text":"","code":"# Simulate some heteroskedastic data (no transformation): dat = simulate_tlm(n = 100, p = 5, g_type = 'box-cox', heterosked = TRUE, lambda = 1) y = dat$y; X = dat$X # training data y_test = dat$y_test; X_test = dat$X_test # testing data  # Target this quantile: tau = 0.05  # Fit the Bayesian quantile regression model: fit = bqr(y = y, X = X, tau = tau, X_test = X_test) #> [1] \"0 seconds remaining\" #> [1] \"Total time:  0 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_qtau\"     \"model\"         \"y\"             \"X\"             #>  [9] \"X_test\"        \"psi\"           \"tau\"            # Posterior predictive checks on testing data: empirical CDF y0 = sort(unique(y_test)) plot(y0, y0, type='n', ylim = c(0,1),      xlab='y', ylab='F_y', main = 'Posterior predictive ECDF') temp = sapply(1:nrow(fit$post_ypred), function(s)   lines(y0, ecdf(fit$post_ypred[s,])(y0), # ECDF of posterior predictive draws         col='gray', type ='s')) lines(y0, ecdf(y_test)(y0),  # ECDF of testing data      col='black', type = 's', lwd = 3)   # The posterior predictive checks usually do not pass! # try ?sbqr instead..."},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian spline model with a Box-Cox transformation â€” bsm_bc","title":"Bayesian spline model with a Box-Cox transformation â€” bsm_bc","text":"MCMC sampling Bayesian spline regression (known unknown) Box-Cox transformation.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian spline model with a Box-Cox transformation â€” bsm_bc","text":"","code":"bsm_bc(   y,   x = NULL,   x_test = NULL,   psi = NULL,   lambda = NULL,   sample_lambda = TRUE,   nsave = 1000,   nburn = 1000,   nskip = 0,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian spline model with a Box-Cox transformation â€” bsm_bc","text":"y n x 1 vector observed counts x n x 1 vector observation points; NULL, assume equally-spaced [0,1] x_test n_test x 1 vector testing points; NULL, assume equal x psi prior variance (inverse smoothing parameter); NULL, sample parameter lambda Box-Cox transformation; NULL, estimate parameter sample_lambda logical; TRUE, sample lambda, otherwise use fixed value lambda MLE (lambda unspecified) nsave number MCMC iterations save nburn number MCMC iterations discard nskip number MCMC iterations skip saving iterations, .e., save every (nskip + 1)th draw verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian spline model with a Box-Cox transformation â€” bsm_bc","text":"list following elements: coefficients posterior mean regression coefficients fitted.values posterior predictive mean test points x_test post_theta: nsave x p samples posterior distribution regression coefficients post_ypred: nsave x n_test samples posterior predictive distribution x_test post_g: nsave posterior samples transformation evaluated unique y values post_lambda nsave posterior samples lambda model: model fit (, sbsm_bc) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian spline model with a Box-Cox transformation â€” bsm_bc","text":"function provides fully Bayesian inference transformed spline model via MCMC sampling. transformation parametric Box-Cox family, one parameter lambda. parameter may fixed advanced learned data.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bayesian spline model with a Box-Cox transformation â€” bsm_bc","text":"Box-Cox transformations may useful cases, general recommend nonparametric transformation (Monte Carlo, MCMC sampling) sbsm.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/bsm_bc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian spline model with a Box-Cox transformation â€” bsm_bc","text":"","code":"# Simulate some data: n = 100 # sample size x = sort(runif(n)) # observation points  # Transform a noisy, periodic function: y = g_inv_bc(   sin(2*pi*x) + sin(4*pi*x) + rnorm(n, sd = .5),              lambda = .5) # Signed square-root transformation  # Fit the Bayesian spline model with a Box-Cox transformation: fit = bsm_bc(y = y, x = x) #> [1] \"0 seconds remaining\" #> [1] \"Total time:  0 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_g\"        \"post_lambda\"   \"model\"         \"y\"             #>  [9] \"X\"             \"psi\"           round(quantile(fit$post_lambda), 3) # summary of unknown Box-Cox parameter #>    0%   25%   50%   75%  100%  #> 0.418 0.544 0.579 0.616 0.749   # Plot the model predictions (point and interval estimates): pi_y = t(apply(fit$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI plot(x, y, type='n', ylim = range(pi_y,y),      xlab = 'x', ylab = 'y', main = paste('Fitted values and prediction intervals')) polygon(c(x, rev(x)),c(pi_y[,2], rev(pi_y[,1])),col='gray', border=NA) lines(x, y, type='p') lines(x, fitted(fit), lwd = 3)"},{"path":"https://drkowal.github.io/SeBR/reference/computeTimeRemaining.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate the remaining time in the MCMC based on previous samples â€” computeTimeRemaining","title":"Estimate the remaining time in the MCMC based on previous samples â€” computeTimeRemaining","text":"Estimate remaining time MCMC based previous samples","code":""},{"path":"https://drkowal.github.io/SeBR/reference/computeTimeRemaining.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate the remaining time in the MCMC based on previous samples â€” computeTimeRemaining","text":"","code":"computeTimeRemaining(nsi, timer0, nsims, nrep = 1000, ninit = 500)"},{"path":"https://drkowal.github.io/SeBR/reference/computeTimeRemaining.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate the remaining time in the MCMC based on previous samples â€” computeTimeRemaining","text":"nsi Current iteration timer0 Initial timer value, returned proc.time()[3] nsims Total number simulations nrep Print estimated time remaining every nrep iterations ninit Print first estimated time remaining ninit","code":""},{"path":"https://drkowal.github.io/SeBR/reference/computeTimeRemaining.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate the remaining time in the MCMC based on previous samples â€” computeTimeRemaining","text":"Table summary statistics using function summary","code":""},{"path":"https://drkowal.github.io/SeBR/reference/concen_hbb.html","id":null,"dir":"Reference","previous_headings":"","what":"Posterior sampling algorithm for the HBB concentration hyperparameters â€” concen_hbb","title":"Posterior sampling algorithm for the HBB concentration hyperparameters â€” concen_hbb","text":"Compute Monte Carlo draws (marginal) posterior distribution concentration hyperparameters hierarchical Bayesian bootstrap (hbb). HBB nonparametric model group-specific distributions; group concentration parameter, larger values encourage shrinkage toward common distribution.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/concen_hbb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Posterior sampling algorithm for the HBB concentration hyperparameters â€” concen_hbb","text":"","code":"concen_hbb(   groups,   shape_alphas = NULL,   rate_alphas = NULL,   nsave = 1000,   ngrid = 500 )"},{"path":"https://drkowal.github.io/SeBR/reference/concen_hbb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Posterior sampling algorithm for the HBB concentration hyperparameters â€” concen_hbb","text":"groups group assignments observed data shape_alphas (optional) shape parameter Gamma prior rate_alphas (optional) rate parameter Gamma prior nsave (optional) number Monte Carlo simulations ngrid (optional) number grid points","code":""},{"path":"https://drkowal.github.io/SeBR/reference/concen_hbb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Posterior sampling algorithm for the HBB concentration hyperparameters â€” concen_hbb","text":"nsave x K samples concentration hyperparameters corresponding K groups","code":""},{"path":"https://drkowal.github.io/SeBR/reference/concen_hbb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Posterior sampling algorithm for the HBB concentration hyperparameters â€” concen_hbb","text":"concentration hyperparameters assigned independent Gamma(shape_alphas, rate_alphas) priors. function uses grid approximation marginal posterior goal producing simple algorithm. *marginal* posterior sampler, can used hbb sampler (conditions alphas) provide joint Monte Carlo (MCMC) sampling algorithm concentration hyperparameters, group-specific CDFs, common CDF. Note diffuse priors alphas tend put posterior mass large values, leads aggressive shrinkage toward common distribution (complete pooling). moderate shrinkage, use default values shape_alphas = 30*K rate_alphas = 1, K number groups.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/concen_hbb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Posterior sampling algorithm for the HBB concentration hyperparameters â€” concen_hbb","text":"Oganisian et al. (https://doi.org/10.1515/ijb-2022-0051)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/concen_hbb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Posterior sampling algorithm for the HBB concentration hyperparameters â€” concen_hbb","text":"","code":"# Dimensions: n = 500 # number of observations K = 3 # number of groups  # Assign groups w/ unequal probabilities: ugroups = paste('g', 1:K, sep='') # groups groups = sample(ugroups,                 size = n,                 replace = TRUE,                 prob = 1:K) # unequally weighted (unnormalized)  # Summarize: table(groups)/n #> groups #>    g1    g2    g3  #> 0.196 0.336 0.468   # Marginal posterior sampling for alpha: post_alpha = concen_hbb(groups)  # Summarize: posterior distributions for(c in 1:K) {   hist(post_alpha[,c],        main = paste(\"Concentration parameter: group\", ugroups[c]),        xlim = range(post_alpha))   abline(v = mean(post_alpha[,c]), lwd=3) # posterior mean }"},{"path":"https://drkowal.github.io/SeBR/reference/contract_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Grid contraction â€” contract_grid","title":"Grid contraction â€” contract_grid","text":"Contract grid evaluation points exceed threshold. removes corresponding z values. can add points back achieve (approximate) length.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/contract_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Grid contraction â€” contract_grid","text":"","code":"contract_grid(z, Fz, lower, upper, add_back = TRUE, monotone = TRUE)"},{"path":"https://drkowal.github.io/SeBR/reference/contract_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Grid contraction â€” contract_grid","text":"z grid points (ordered) Fz function evaluated grid points lower lower threshold check Fz upper upper threshold check Fz add_back logical; true, expand grid () original size monotone logical; true, enforce monotonicity expanded grid","code":""},{"path":"https://drkowal.github.io/SeBR/reference/contract_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Grid contraction â€” contract_grid","text":"list containing grid points z (interpolated) function Fz points","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":null,"dir":"Reference","previous_headings":"","what":"Box-Cox transformation â€” g_bc","title":"Box-Cox transformation â€” g_bc","text":"Evaluate Box-Cox transformation, scaled power transformation preserve continuity index lambda zero. Negative values permitted.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Box-Cox transformation â€” g_bc","text":"","code":"g_bc(t, lambda)"},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Box-Cox transformation â€” g_bc","text":"t argument(s) evaluate function lambda Box-Cox parameter","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Box-Cox transformation â€” g_bc","text":"evaluation(s) Box-Cox function given input(s) t.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Box-Cox transformation â€” g_bc","text":"Special cases include identity transformation (lambda = 1), square-root transformation (lambda = 1/2), log transformation (lambda = 0).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_bc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Box-Cox transformation â€” g_bc","text":"","code":"# Log-transformation: g_bc(1:5, lambda = 0); log(1:5) #> [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 #> [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379  # Square-root transformation: note the shift and scaling g_bc(1:5, lambda = 1/2); sqrt(1:5) #> [1] 0.0000000 0.8284271 1.4641016 2.0000000 2.4721360 #> [1] 1.000000 1.414214 1.732051 2.000000 2.236068"},{"path":"https://drkowal.github.io/SeBR/reference/g_fun.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the transformation â€” g_fun","title":"Compute the transformation â€” g_fun","text":"Given CDFs z y, compute smoothed function evaluate transformation","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_fun.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the transformation â€” g_fun","text":"","code":"g_fun(y, Fy_eval, z, Fz_eval)"},{"path":"https://drkowal.github.io/SeBR/reference/g_fun.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the transformation â€” g_fun","text":"y vector points CDF y evaluated Fy_eval CDF y evaluated y z vector points CDF z evaluated Fz_eval CDF z evaluated z","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_fun.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the transformation â€” g_fun","text":"smooth monotone function can used evaluations transformation.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_approx.html","id":null,"dir":"Reference","previous_headings":"","what":"Approximate inverse transformation â€” g_inv_approx","title":"Approximate inverse transformation â€” g_inv_approx","text":"Compute inverse function transformation g based grid search.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_approx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Approximate inverse transformation â€” g_inv_approx","text":"","code":"g_inv_approx(g, t_grid)"},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_approx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Approximate inverse transformation â€” g_inv_approx","text":"g transformation function t_grid grid arguments evaluate transformation function","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_approx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Approximate inverse transformation â€” g_inv_approx","text":"function can used evaluations (approximate) inverse transformation function.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_bc.html","id":null,"dir":"Reference","previous_headings":"","what":"Inverse Box-Cox transformation â€” g_inv_bc","title":"Inverse Box-Cox transformation â€” g_inv_bc","text":"Evaluate inverse Box-Cox transformation. Negative values permitted.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_bc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inverse Box-Cox transformation â€” g_inv_bc","text":"","code":"g_inv_bc(s, lambda)"},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_bc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inverse Box-Cox transformation â€” g_inv_bc","text":"s argument(s) evaluate function lambda Box-Cox parameter","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_bc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Inverse Box-Cox transformation â€” g_inv_bc","text":"evaluation(s) inverse Box-Cox function given input(s) s.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_bc.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Inverse Box-Cox transformation â€” g_inv_bc","text":"Special cases include identity transformation (lambda = 1), square-root transformation (lambda = 1/2), log transformation (lambda = 0).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/g_inv_bc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inverse Box-Cox transformation â€” g_inv_bc","text":"","code":"# (Inverse) log-transformation: g_inv_bc(1:5, lambda = 0); exp(1:5) #> [1]   2.718282   7.389056  20.085537  54.598150 148.413159 #> [1]   2.718282   7.389056  20.085537  54.598150 148.413159  # (Inverse) square-root transformation: note the shift and scaling g_inv_bc(1:5, lambda = 1/2); (1:5)^2 #> [1]  2.25  4.00  6.25  9.00 12.25 #> [1]  1  4  9 16 25"},{"path":"https://drkowal.github.io/SeBR/reference/hbb.html","id":null,"dir":"Reference","previous_headings":"","what":"Hierarchical Bayesian bootstrap posterior sampler â€” hbb","title":"Hierarchical Bayesian bootstrap posterior sampler â€” hbb","text":"Compute one Monte Carlo draw hierarchical Bayesian bootstrap (HBB) posterior distribution cumulative distribution function (CDF) group. common (BB) group-specific (HBB) weights also returned.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/hbb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hierarchical Bayesian bootstrap posterior sampler â€” hbb","text":"","code":"hbb(   y,   groups,   sample_alphas = FALSE,   shape_alphas = NULL,   rate_alphas = NULL,   alphas = NULL,   M = 30 )"},{"path":"https://drkowal.github.io/SeBR/reference/hbb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hierarchical Bayesian bootstrap posterior sampler â€” hbb","text":"y data infer group-specific CDFs groups group assignment element y sample_alphas logical; TRUE, sample concentration hyperparameters marginal posterior distribution shape_alphas (optional) shape parameter Gamma prior alphas (sampled) rate_alphas (optional) rate parameter Gamma prior alphas (sampled) alphas (optional) vector fixed concentration hyperparameters corresponding unique levels groups (used sample_alphas = FALSE) M positive scaling term set default value alphas unspecified (alphas = NULL) sampled (sample_alphas = FALSE)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/hbb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Hierarchical Bayesian bootstrap posterior sampler â€” hbb","text":"list following elements: Fyc: list functions entry corresponds group  group-specific function can evaluate sampled CDF argument(s) weights_y: sampled weights common (BB) distribution (n-dimensional) weights_yc: sampled weights K groups (K x n) alphas: (fixed sampled) concentration hyperparameters","code":""},{"path":"https://drkowal.github.io/SeBR/reference/hbb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Hierarchical Bayesian bootstrap posterior sampler â€” hbb","text":"Assuming data y independent unknown, group-specific distributions, hierarchical Bayesian bootstrap (HBB) Oganisian et al. (https://doi.org/10.1515/ijb-2022-0051) nonparametric model distribution. HBB includes hierarchical shrinkage across groups toward common distribution (bb). HBB admits direct Monte Carlo (MCMC) sampling. shrinkage toward common distribution determined concentration hyperparameters alphas. component alphas corresponds one groups. Larger values encourage shrinkage toward common distribution, smaller values allow substantial deviations group. sample_alphas=TRUE, component alphas sampled marginal posterior distribution, assuming independent Gamma(shape_alphas, rate_alphas) priors. step uses simple grid approximation enable efficient sampling preserves joint Monte Carlo sampling group-specific common distributions. See concen_hbb details. Note diffuse priors alphas tends produce aggressive shrinkage toward common distribution (complete pooling). moderate shrinkage, use default values shape_alphas = 30*K rate_alphas = 1 K number groups. sample_alphas=FALSE, concentration hyperparameters fixed user-specified values. can done specifying alphas directly. Alternatively, alphas left unspecified (alphas = NULL), adopt default Oganisian et al. sets cth entry M*n/nc M user-specified nc number observations group c. guidance choice M: M = 0.01/K approximates separate BB's group (pooling); M 10 100 gives moderate shrinkage (partial pooling); M = 100*max(nc) approximates common BB (complete pooling).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/hbb.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Hierarchical Bayesian bootstrap posterior sampler â€” hbb","text":"supplying alphas distinct entries, make sure groups ordered properly; entries match sort(unique(groups)).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/hbb.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Hierarchical Bayesian bootstrap posterior sampler â€” hbb","text":"Oganisian et al. (https://doi.org/10.1515/ijb-2022-0051)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/hbb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hierarchical Bayesian bootstrap posterior sampler â€” hbb","text":"","code":"# Sample size and number of groups: n = 500 K = 3  # Define the groups, then assign: ugroups = paste('g', 1:K, sep='') # groups groups = sample(ugroups, n, replace = TRUE) # assignments  # Simulate the data: iid normal, then add group-specific features y = rnorm(n = n) # data for(g in ugroups)   y[groups==g] = y[groups==g] + 3*rnorm(1) # group-specific  # One draw from the HBB posterior of the CDF: samp_hbb = hbb(y, groups)  names(samp_hbb) # items returned #> [1] \"Fyc\"        \"weights_y\"  \"weights_yc\" \"alphas\"     Fyc = samp_hbb$Fyc # list of CDFs class(Fyc) # this is a list #> [1] \"list\" class(Fyc[[1]]) # each element is a function #> [1] \"function\"  c = 1 # try: vary in 1:K Fyc[[c]](0) # some example use (for this one draw) #> [1] 0.04595643 Fyc[[c]](c(.5, 1.2)) #> [1] 0.07032374 0.10472581  # Plot several draws from the HBB posterior distribution: ys = seq(min(y), max(y), length.out=1000) plot(ys, ys, type='n', ylim = c(0,1),      main = 'Draws from HBB posteriors', xlab = 'y', ylab = 'F_c(y)') for(s in 1:50){ # some draws    # BB CDF:   Fy = bb(y)   lines(ys, Fy(ys), lwd=3) # plot CDF    # HBB:   Fyc = hbb(y, groups)$Fyc    # Plot CDFs by group:   for(c in 1:K) lines(ys, Fyc[[c]](ys), col=c+1, lwd=3) }  # For reference, add the ECDFs by group: for(c in 1:K) lines(ys, ecdf(y[groups==ugroups[c]])(ys), lty=2)  legend('bottomright', c('BB', paste('HBB:', ugroups)), col = 1:(K+1), lwd=3)"},{"path":"https://drkowal.github.io/SeBR/reference/plot_pptest.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot point and interval predictions on testing data â€” plot_pptest","title":"Plot point and interval predictions on testing data â€” plot_pptest","text":"Given posterior predictive samples X_test, plot point interval estimates compare actual testing data y_test.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/plot_pptest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot point and interval predictions on testing data â€” plot_pptest","text":"","code":"plot_pptest(post_ypred, y_test, alpha_level = 0.1)"},{"path":"https://drkowal.github.io/SeBR/reference/plot_pptest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot point and interval predictions on testing data â€” plot_pptest","text":"post_ypred nsave x n_test samples posterior predictive distribution test points X_test y_test n_test testing points alpha_level alpha-level prediction intervals","code":""},{"path":"https://drkowal.github.io/SeBR/reference/plot_pptest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot point and interval predictions on testing data â€” plot_pptest","text":"plot testing data, point interval predictions, summary empirical coverage","code":""},{"path":"https://drkowal.github.io/SeBR/reference/plot_pptest.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot point and interval predictions on testing data â€” plot_pptest","text":"","code":"# \\donttest{ # Simulate some data: dat = simulate_tlm(n = 100, p = 5, g_type = 'step')  # Fit a semiparametric Bayesian linear model: fit = sblm(y = dat$y, X = dat$X, X_test = dat$X_test) #> [1] \"2 seconds remaining\" #> [1] \"Total time:  4 seconds\"  # Evaluate posterior predictive means and intervals on the testing data: plot_pptest(fit$post_ypred, dat$y_test,             alpha_level = 0.10) # coverage should be about 90%  #> [1] 0.887 # }"},{"path":"https://drkowal.github.io/SeBR/reference/rank_approx.html","id":null,"dir":"Reference","previous_headings":"","what":"Rank-based estimation of the linear regression coefficients â€” rank_approx","title":"Rank-based estimation of the linear regression coefficients â€” rank_approx","text":"transformed Gaussian linear model, compute point estimates regression coefficients.  approach uses ranks data require transformation, must expand sample size n^2 thus can slow.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/rank_approx.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rank-based estimation of the linear regression coefficients â€” rank_approx","text":"","code":"rank_approx(y, X)"},{"path":"https://drkowal.github.io/SeBR/reference/rank_approx.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rank-based estimation of the linear regression coefficients â€” rank_approx","text":"y n x 1 response vector X n x p matrix predictors (include intercept!)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/rank_approx.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rank-based estimation of the linear regression coefficients â€” rank_approx","text":"estimated linear coefficients","code":""},{"path":"https://drkowal.github.io/SeBR/reference/rank_approx.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rank-based estimation of the linear regression coefficients â€” rank_approx","text":"","code":"# Simulate some data: dat = simulate_tlm(n = 200, p = 10, g_type = 'step')  # Point estimates for the linear coefficients: theta_hat = suppressWarnings(   rank_approx(y = dat$y,               X = dat$X[,-1]) # remove intercept ) # warnings occur from glm.fit (fitted probabilities 0 or 1)  # Check: correlation with true coefficients cor(dat$beta_true[-1], # excluding the intercept     theta_hat) #> [1] 0.9455397"},{"path":"https://drkowal.github.io/SeBR/reference/sampleFastGaussian.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample a Gaussian vector using the fast sampler of BHATTACHARYA et al. â€” sampleFastGaussian","title":"Sample a Gaussian vector using the fast sampler of BHATTACHARYA et al. â€” sampleFastGaussian","text":"Sample N(mu, Sigma) Sigma = solve(crossprod(Phi) + solve(D)) mu = Sigma*crossprod(Phi, alpha):","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sampleFastGaussian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample a Gaussian vector using the fast sampler of BHATTACHARYA et al. â€” sampleFastGaussian","text":"","code":"sampleFastGaussian(Phi, Ddiag, alpha)"},{"path":"https://drkowal.github.io/SeBR/reference/sampleFastGaussian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample a Gaussian vector using the fast sampler of BHATTACHARYA et al. â€” sampleFastGaussian","text":"Phi n x p matrix (predictors) Ddiag p x 1 vector diagonal components (prior variance) alpha n x 1 vector (data, scaled variance)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sampleFastGaussian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample a Gaussian vector using the fast sampler of BHATTACHARYA et al. â€” sampleFastGaussian","text":"Draw N(mu, Sigma), p x 1, computed O(n^2*p)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sampleFastGaussian.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Sample a Gaussian vector using the fast sampler of BHATTACHARYA et al. â€” sampleFastGaussian","text":"Assumes D diagonal, extensions available","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":null,"dir":"Reference","previous_headings":"","what":"Semiparametric Bayesian Gaussian processes â€” sbgp","title":"Semiparametric Bayesian Gaussian processes â€” sbgp","text":"Monte Carlo sampling Bayesian Gaussian process regression unknown (nonparametric) transformation.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Semiparametric Bayesian Gaussian processes â€” sbgp","text":"","code":"sbgp(   y,   locs,   X = NULL,   covfun_name = \"matern_isotropic\",   locs_test = locs,   X_test = NULL,   nn = 30,   emp_bayes = TRUE,   fixedX = FALSE,   approx_g = FALSE,   nsave = 1000,   ngrid = 100 )"},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Semiparametric Bayesian Gaussian processes â€” sbgp","text":"y n x 1 response vector locs n x d matrix locations X n x p design matrix; unspecified, use intercept covfun_name string name covariance function; see ?GpGp locs_test n_test x d matrix locations predictions needed; default locs X_test n_test x p design matrix test data; default X nn number nearest neighbors use; default 30 (larger values improve approximation increase computing cost) emp_bayes logical; TRUE, use (faster!) empirical Bayes approach estimating mean function fixedX logical; TRUE, treat design fixed (non-random) sampling transformation; otherwise treat covariates random unknown distribution approx_g logical; TRUE, apply large-sample approximation transformation nsave number Monte Carlo simulations ngrid number grid points inverse approximations","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Semiparametric Bayesian Gaussian processes â€” sbgp","text":"list following elements: coefficients estimated regression coefficients fitted.values posterior predictive mean test points locs_test fit_gp fitted GpGp_fit object, includes covariance parameter estimates model information post_ypred: nsave x ntest samples posterior predictive distribution locs_test post_g: nsave posterior samples transformation evaluated unique y values model: model fit (, sbgp) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Semiparametric Bayesian Gaussian processes â€” sbgp","text":"function provides Bayesian inference transformed Gaussian process model using Monte Carlo (MCMC) sampling. transformation modeled unknown learned jointly regression function (unless approx_g = TRUE, uses point approximation). model applies real-valued data, positive data, compactly-supported data (support automatically deduced observed y values). results typically unchanged whether laplace_approx TRUE/FALSE; setting TRUE may reduce sensitivity prior, setting FALSE may speed computations large datasets. computational efficiency, Gaussian process parameters fixed point estimates, latent Gaussian process sampled emp_bayes = FALSE. However, uncertainty term often negligible compared observation errors, transformation serves additional layer robustness.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbgp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Semiparametric Bayesian Gaussian processes â€” sbgp","text":"","code":"# \\donttest{ # Simulate some data: n = 200 # sample size x = seq(0, 1, length = n) # observation points  # Transform a noisy, periodic function: y = g_inv_bc(   sin(2*pi*x) + sin(4*pi*x) + rnorm(n),              lambda = .5) # Signed square-root transformation  # Package we use for fast computing w/ Gaussian processes: library(GpGp)  # Fit the semiparametric Bayesian Gaussian process: fit = sbgp(y = y, locs = x) #> [1] \"Initial GP fit...\" #> [1] \"Updated GP fit...\" #> [1] \"Sampling...\" #> [1] \"Done!\" names(fit) # what is returned #> [1] \"coefficients\"  \"fitted.values\" \"fit_gp\"        \"post_ypred\"    #> [5] \"post_g\"        \"model\"         \"y\"             \"X\"             #> [9] \"approx_g\"      coef(fit) # estimated regression coefficients (here, just an intercept) #> [1] -0.02613754 class(fit$fit_gp) # the GpGp object is also returned #> [1] \"GpGp_fit\"  # Plot the model predictions (point and interval estimates): pi_y = t(apply(fit$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI plot(x, y, type='n', ylim = range(pi_y,y),      xlab = 'x', ylab = 'y', main = paste('Fitted values and prediction intervals')) polygon(c(x, rev(x)),c(pi_y[,2], rev(pi_y[,1])),col='gray', border=NA) lines(x, y, type='p') # observed points lines(x, fitted(fit), lwd = 3) # fitted curve  # }"},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":null,"dir":"Reference","previous_headings":"","what":"Semiparametric Bayesian linear model â€” sblm","title":"Semiparametric Bayesian linear model â€” sblm","text":"Monte Carlo sampling Bayesian linear regression unknown (nonparametric) transformation. g-prior assumed regression coefficients.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Semiparametric Bayesian linear model â€” sblm","text":"","code":"sblm(   y,   X,   X_test = X,   psi = length(y),   laplace_approx = TRUE,   fixedX = FALSE,   approx_g = FALSE,   nsave = 1000,   ngrid = 100,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Semiparametric Bayesian linear model â€” sblm","text":"y n x 1 response vector X n x p matrix predictors (intercept) X_test n_test x p matrix predictors test data; default observed covariates X psi prior variance (g-prior) laplace_approx logical; TRUE, use normal approximation posterior definition transformation; otherwise prior used fixedX logical; TRUE, treat design fixed (non-random) sampling transformation; otherwise treat covariates random unknown distribution approx_g logical; TRUE, apply large-sample approximation transformation nsave number Monte Carlo simulations ngrid number grid points inverse approximations verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Semiparametric Bayesian linear model â€” sblm","text":"list following elements: coefficients posterior mean regression coefficients fitted.values posterior predictive mean test points X_test post_theta: nsave x p samples posterior distribution regression coefficients post_ypred: nsave x n_test samples posterior predictive distribution test points X_test post_g: nsave posterior samples transformation evaluated unique y values model: model fit (, sblm) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Semiparametric Bayesian linear model â€” sblm","text":"function provides fully Bayesian inference transformed linear model using Monte Carlo (MCMC) sampling. transformation modeled unknown learned jointly regression coefficients (unless approx_g = TRUE, uses point approximation). model applies real-valued data, positive data, compactly-supported data (support automatically deduced observed y values). results typically unchanged whether laplace_approx TRUE/FALSE; setting TRUE may reduce sensitivity prior, setting FALSE may speed computations large datasets.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Semiparametric Bayesian linear model â€” sblm","text":"location (intercept) scale (sigma_epsilon) identified, intercepts X X_test removed. model-fitting ** include internal location-scale adjustment, function outputs inferential summaries identifiable parameters.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Semiparametric Bayesian linear model â€” sblm","text":"","code":"# \\donttest{ # Simulate some data: dat = simulate_tlm(n = 100, p = 5, g_type = 'step') y = dat$y; X = dat$X # training data y_test = dat$y_test; X_test = dat$X_test # testing data  hist(y, breaks = 25) # marginal distribution   # Fit the semiparametric Bayesian linear model: fit = sblm(y = y, X = X, X_test = X_test) #> [1] \"2 seconds remaining\" #> [1] \"Total time:  3 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_g\"        \"model\"         \"y\"             \"X\"             #>  [9] \"X_test\"        \"psi\"           \"approx_g\"       # Note: this is Monte Carlo sampling...no need for MCMC diagnostics!  # Evaluate posterior predictive means and intervals on the testing data: plot_pptest(fit$post_ypred, y_test,             alpha_level = 0.10) # coverage should be about 90%  #> [1] 0.885  # Check: correlation with true coefficients cor(dat$beta_true, coef(fit)) #> [1] 0.985565  # Summarize the transformation: y0 = sort(unique(y)) # posterior draws of g are evaluated at the unique y observations plot(y0, fit$post_g[1,], type='n', ylim = range(fit$post_g),      xlab = 'y', ylab = 'g(y)', main = \"Posterior draws of the transformation\") temp = sapply(1:nrow(fit$post_g), function(s)   lines(y0, fit$post_g[s,], col='gray')) # posterior draws lines(y0, colMeans(fit$post_g), lwd = 3) # posterior mean lines(y, dat$g_true, type='p', pch=2) # true transformation legend('bottomright', c('Truth'), pch = 2) # annotate the true transformation   # Posterior predictive checks on testing data: empirical CDF y0 = sort(unique(y_test)) plot(y0, y0, type='n', ylim = c(0,1),      xlab='y', ylab='F_y', main = 'Posterior predictive ECDF') temp = sapply(1:nrow(fit$post_ypred), function(s)   lines(y0, ecdf(fit$post_ypred[s,])(y0), # ECDF of posterior predictive draws         col='gray', type ='s')) lines(y0, ecdf(y_test)(y0),  # ECDF of testing data      col='black', type = 's', lwd = 3)  # }"},{"path":"https://drkowal.github.io/SeBR/reference/sblm_hs.html","id":null,"dir":"Reference","previous_headings":"","what":"Semiparametric Bayesian linear model with horseshoe priors for high-dimensional data â€” sblm_hs","title":"Semiparametric Bayesian linear model with horseshoe priors for high-dimensional data â€” sblm_hs","text":"MCMC sampling semiparametric Bayesian linear regression 1) unknown (nonparametric) transformation 2) horseshoe prior (possibly high-dimensional) regression coefficients. , unlike sblm, Gibbs sampling needed regression coefficients horseshoe prior variance components. transformation g still sampled unconditionally regression coefficients, provides efficient blocking within Gibbs sampler.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm_hs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Semiparametric Bayesian linear model with horseshoe priors for high-dimensional data â€” sblm_hs","text":"","code":"sblm_hs(   y,   X,   X_test = X,   fixedX = FALSE,   approx_g = FALSE,   init_screen = NULL,   pilot_hs = FALSE,   nsave = 1000,   nburn = 1000,   ngrid = 100,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/sblm_hs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Semiparametric Bayesian linear model with horseshoe priors for high-dimensional data â€” sblm_hs","text":"y n x 1 response vector X n x p matrix predictors (intercept) X_test n_test x p matrix predictors test data; default observed covariates X fixedX logical; TRUE, treat design fixed (non-random) sampling transformation; otherwise treat covariates random unknown distribution approx_g logical; TRUE, apply large-sample approximation transformation init_screen initial approximation, number covariates pre-screen (necessary p > n); NULL, use n/log(n) pilot_hs logical; TRUE, use short pilot run horseshoe prior estimate marginal CDF latent z (otherwise, use Laplace approximation) nsave number MCMC simulations save nburn number MCMC iterations discard ngrid number grid points inverse approximations verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm_hs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Semiparametric Bayesian linear model with horseshoe priors for high-dimensional data â€” sblm_hs","text":"list following elements: coefficients posterior mean regression coefficients fitted.values posterior predictive mean test points X_test post_theta: nsave x p samples posterior distribution regression coefficients post_ypred: nsave x n_test samples posterior predictive distribution test points X_test post_g: nsave posterior samples transformation evaluated unique y values model: model fit (, sblm_hs) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm_hs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Semiparametric Bayesian linear model with horseshoe priors for high-dimensional data â€” sblm_hs","text":"function provides fully Bayesian inference transformed linear model horseshoe priors using efficiently-blocked Gibbs sampling. transformation modeled unknown learned jointly regression coefficients (unless approx_g = TRUE, uses point approximation). model applies real-valued data, positive data, compactly-supported data (support automatically deduced observed y values). horseshoe prior especially useful high-dimensional settings many (possibly correlated) covariates. Compared sparse spike--slab alternatives (see sblm_ssvs), horseshoe prior delivers scalable computing p. function uses fast Cholesky-forward/backward sampler p < n Bhattacharya et al. (https://doi.org/10.1093/biomet/asw042) sampler p > n. Thus, sampler can scale linear n (fixed/small p) linear p (fixed/small n). Empirically, horseshoe prior performs best sparse regimes, .e., number true signals (nonzero regression coefficients) small fraction total number variables. learn transformation, SeBR infers marginal CDF latent data model Fz averaging covariates X (via Bayesian bootstrap, bb) coefficients theta. pilot_hs = TRUE, algorithm fits initial linear regression model horseshoe prior fit transformed data (preliminary point estimate transformation) uses posterior distribution marginalize theta. Otherwise, marginalization done using Laplace approximation speed simplicity.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm_hs.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Semiparametric Bayesian linear model with horseshoe priors for high-dimensional data â€” sblm_hs","text":"location (intercept) scale (sigma_epsilon) identified, intercepts X X_test removed. model-fitting ** include internal location-scale adjustment, function outputs inferential summaries identifiable parameters.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm_hs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Semiparametric Bayesian linear model with horseshoe priors for high-dimensional data â€” sblm_hs","text":"","code":"# \\donttest{ # Simulate some data: dat = simulate_tlm(n = 100, p = 50, g_type = 'step', prop_sig = 0.1) y = dat$y; X = dat$X # training data y_test = dat$y_test; X_test = dat$X_test # testing data  hist(y, breaks = 25) # marginal distribution   # Fit the semiparametric Bayesian linear model with horseshoe priors: fit = sblm_hs(y = y, X = X, X_test = X_test) #> [1] \"3 seconds remaining\" #> [1] \"Total time:  9 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_g\"        \"model\"         \"y\"             \"X\"             #>  [9] \"X_test\"        \"init_screen\"   \"approx_g\"       # Evaluate posterior predictive means and intervals on the testing data: plot_pptest(fit$post_ypred, y_test,             alpha_level = 0.10) # coverage should be about 90%  #> [1] 0.969  # Check: correlation with true coefficients cor(dat$beta_true, coef(fit)) #> [1] 0.9737789  # Compute 95% credible intervals for the coefficients: ci_theta = t(apply(fit$post_theta, 2, quantile, c(0.05/2, 1 - 0.05/2)))  # True positive/negative rates for \"selected\" coefficients: selected = ((ci_theta[,1] >0 | ci_theta[,2] < 0)) # intervals exclude zero sigs_true = dat$beta_true != 0 # true signals (TPR = sum(selected & sigs_true)/sum(sigs_true)) #> [1] 1 (TNR = sum(!selected & !sigs_true)/sum(!sigs_true)) #> [1] 1  # Summarize the transformation: y0 = sort(unique(y)) # posterior draws of g are evaluated at the unique y observations plot(y0, fit$post_g[1,], type='n', ylim = range(fit$post_g),      xlab = 'y', ylab = 'g(y)', main = \"Posterior draws of the transformation\") temp = sapply(1:nrow(fit$post_g), function(s)   lines(y0, fit$post_g[s,], col='gray')) # posterior draws lines(y0, colMeans(fit$post_g), lwd = 3) # posterior mean  # Add the true transformation, rescaled for easier comparisons: lines(y,       scale(dat$g_true)*sd(colMeans(fit$post_g)) + mean(colMeans(fit$post_g)), type='p', pch=2) legend('bottomright', c('Truth'), pch = 2) # annotate the true transformation   # Posterior predictive checks on testing data: empirical CDF y0 = sort(unique(y_test)) plot(y0, y0, type='n', ylim = c(0,1),      xlab='y', ylab='F_y', main = 'Posterior predictive ECDF') temp = sapply(1:nrow(fit$post_ypred), function(s)   lines(y0, ecdf(fit$post_ypred[s,])(y0), # ECDF of posterior predictive draws         col='gray', type ='s')) lines(y0, ecdf(y_test)(y0),  # ECDF of testing data      col='black', type = 's', lwd = 3)  # }"},{"path":"https://drkowal.github.io/SeBR/reference/sblm_modelsel.html","id":null,"dir":"Reference","previous_headings":"","what":"Model selection for semiparametric Bayesian linear regression â€” sblm_modelsel","title":"Model selection for semiparametric Bayesian linear regression â€” sblm_modelsel","text":"Compute model probabilities semiparametric Bayesian linear regression 1) unknown (nonparametric) transformation 2) sparsity prior regression coefficients. model probabilities computed using direct Monte Carlo (MCMC) sampling.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm_modelsel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model selection for semiparametric Bayesian linear regression â€” sblm_modelsel","text":"","code":"sblm_modelsel(   y,   X,   prob_inclusion = 0.5,   psi = length(y),   fixedX = FALSE,   init_screen = NULL,   nsave = 1000,   override = FALSE,   ngrid = 100,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/sblm_modelsel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model selection for semiparametric Bayesian linear regression â€” sblm_modelsel","text":"y n x 1 response vector X n x p matrix predictors (intercept) prob_inclusion prior inclusion probability variable psi prior variance (g-prior) fixedX logical; TRUE, treat design fixed (non-random) sampling transformation; otherwise treat covariates random unknown distribution init_screen initial approximation, number covariates pre-screen (necessary p > n); NULL, use n/log(n) nsave number Monte Carlo simulations override logical; TRUE, user may override default cancellation function call p > 15 ngrid number grid points inverse approximations verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm_modelsel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model selection for semiparametric Bayesian linear regression â€” sblm_modelsel","text":"list following elements: post_probs posterior probabilities model all_models: 2^p x p matrix row corresponds model post_probs column indicates inclusion (TRUE) exclusion (FALSE) variable model: model fit (, sblm_modelsel) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm_modelsel.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Model selection for semiparametric Bayesian linear regression â€” sblm_modelsel","text":"function provides fully Bayesian model selection transformed linear model sparse g-priors regression coefficients. transformation modeled unknown learned jointly model probabilities. model applies real-valued data, positive data, compactly-supported data (support automatically deduced observed y values). Enumeration possible subsets computationally demanding reserved small p. function exit p > 15 unless override command given (override = TRUE). function exclusively computes model probabilities provide coefficient inference prediction. straightforward, omitted save compute time. prediction, coefficient inference, computation moderate large p, use sblm_ssvs.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm_modelsel.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Model selection for semiparametric Bayesian linear regression â€” sblm_modelsel","text":"location (intercept) scale (sigma_epsilon) identified, intercept X removed. model-fitting ** include internal location-scale adjustment, model probabilities refer non-intercept variables X.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm_modelsel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model selection for semiparametric Bayesian linear regression â€” sblm_modelsel","text":"","code":"# \\donttest{ # Simulate some data: dat = simulate_tlm(n = 100, p = 5, g_type = 'beta') y = dat$y; X = dat$X  hist(y, breaks = 25) # marginal distribution   # Package for conveniently computing all subsets: library(plyr)  # Fit the semiparametric Bayesian linear model with model selection: fit = sblm_modelsel(y = y, X = X) #> [1] \"10 seconds remaining\" #> [1] \"Total time:  19 seconds\" names(fit) # what is returned #> [1] \"post_probs\"  \"all_models\"  \"model\"       \"y\"           \"X\"           #> [6] \"init_screen\"  # Summarize the probabilities of each model (by size): plot(rowSums(fit$all_models), fit$post_probs,      xlab = 'Model sizes', ylab = 'p(model | data)',      main = 'Posterior model probabilities', pch = 2, ylim = c(0,1))   # Highest probability model: hpm = which.max(fit$post_probs) fit$post_probs[hpm] # probability #> [1] 0.9404899 which(fit$all_models[hpm,]) # which variables #> X1 X2 X3  #>  1  2  3  which(dat$beta_true != 0) # ground truth #> [1] 1 2 3  # }"},{"path":"https://drkowal.github.io/SeBR/reference/sblm_ssvs.html","id":null,"dir":"Reference","previous_headings":"","what":"Semiparametric Bayesian linear model with stochastic search variable selection â€” sblm_ssvs","title":"Semiparametric Bayesian linear model with stochastic search variable selection â€” sblm_ssvs","text":"MCMC sampling semiparametric Bayesian linear regression 1) unknown (nonparametric) transformation 2) sparsity prior (possibly high-dimensional) regression coefficients. , unlike sblm, Gibbs sampling used variable inclusion indicator variables gamma, referred stochastic search variable selection (SSVS). remaining termsâ€“including transformation g, regression coefficients theta, predictive drawsâ€“drawn directly joint posterior (predictive) distribution.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm_ssvs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Semiparametric Bayesian linear model with stochastic search variable selection â€” sblm_ssvs","text":"","code":"sblm_ssvs(   y,   X,   X_test = X,   psi = length(y),   fixedX = FALSE,   approx_g = FALSE,   init_screen = NULL,   a_pi = 1,   b_pi = 1,   nsave = 1000,   nburn = 1000,   ngrid = 100,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/sblm_ssvs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Semiparametric Bayesian linear model with stochastic search variable selection â€” sblm_ssvs","text":"y n x 1 response vector X n x p matrix predictors (intercept) X_test n_test x p matrix predictors test data; default observed covariates X psi prior variance (g-prior) fixedX logical; TRUE, treat design fixed (non-random) sampling transformation; otherwise treat covariates random unknown distribution approx_g logical; TRUE, apply large-sample approximation transformation init_screen initial approximation, number covariates pre-screen (necessary p > n); NULL, use n/log(n) a_pi shape1 parameter (Beta) prior inclusion probability b_pi shape2 parameter (Beta) prior inclusion probability nsave number MCMC simulations save nburn number MCMC iterations discard ngrid number grid points inverse approximations verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm_ssvs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Semiparametric Bayesian linear model with stochastic search variable selection â€” sblm_ssvs","text":"list following elements: coefficients posterior mean regression coefficients fitted.values posterior predictive mean test points X_test selected: variables (indices) selected median probability model post_theta: nsave x p samples posterior distribution regression coefficients post_gamma: nsave x p samples posterior distribution variable inclusion indicators post_ypred: nsave x n_test samples posterior predictive distribution test points X_test post_g: nsave posterior samples transformation evaluated unique y values model: model fit (, sblm_ssvs) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm_ssvs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Semiparametric Bayesian linear model with stochastic search variable selection â€” sblm_ssvs","text":"function provides fully Bayesian inference transformed linear model sparse g-priors regression coefficients. transformation modeled unknown learned jointly regression coefficients (unless approx_g = TRUE, uses point approximation). model applies real-valued data, positive data, compactly-supported data (support automatically deduced observed y values). sparsity prior especially useful variable selection. Compared horseshoe prior version (sblm_hs), sparse g-prior advantageous 1) truly allows sparse (.e., exactly zero) coefficients prior posterior, 2) incorporates covariate dependencies via g-prior structure, 3) tends perform well sparse non-sparse regimes, horseshoe version performs well sparse regimes. disadvantage SSVS scale nearly well p. Following Scott Berger (https://doi.org/10.1214/10-AOS792), include Beta prior prior inclusion probability. term sampled variable inclusion indicators gamma Gibbs sampling block. terms sampled using direct Monte Carlo (MCMC) sampling. Alternatively, model probabilities can computed directly (Monte Carlo, MCMC/Gibbs sampling) using sblm_modelsel.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm_ssvs.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Semiparametric Bayesian linear model with stochastic search variable selection â€” sblm_ssvs","text":"location (intercept) scale (sigma_epsilon) identified, intercepts X X_test removed. model-fitting ** include internal location-scale adjustment, function outputs inferential summaries identifiable parameters.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sblm_ssvs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Semiparametric Bayesian linear model with stochastic search variable selection â€” sblm_ssvs","text":"","code":"# \\donttest{ # Simulate some data: dat = simulate_tlm(n = 100, p = 15, g_type = 'step') y = dat$y; X = dat$X # training data y_test = dat$y_test; X_test = dat$X_test # testing data  hist(y, breaks = 25) # marginal distribution   # Fit the semiparametric Bayesian linear model with sparsity priors: fit = sblm_ssvs(y = y, X = X, X_test = X_test) #> [1] \"28 seconds remaining\" #> [1] \"Total time:  49 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"selected\"      \"post_theta\"    #>  [5] \"post_gamma\"    \"post_ypred\"    \"post_g\"        \"model\"         #>  [9] \"y\"             \"X\"             \"X_test\"        \"init_screen\"   #> [13] \"approx_g\"       # Evaluate posterior predictive means and intervals on the testing data: plot_pptest(fit$post_ypred, y_test,             alpha_level = 0.10) # coverage should be about 90%  #> [1] 0.877  # Check: correlation with true coefficients cor(dat$beta_true, coef(fit)) #> [1] 0.9702049  # Selected coefficients under median probability model: fit$selected #> [1] 1 2 3 4 5 6 7 8  # True signals: which(dat$beta_true != 0) #> [1] 1 2 3 4 5 6 7 8  # Summarize the transformation: y0 = sort(unique(y)) # posterior draws of g are evaluated at the unique y observations plot(y0, fit$post_g[1,], type='n', ylim = range(fit$post_g),      xlab = 'y', ylab = 'g(y)', main = \"Posterior draws of the transformation\") temp = sapply(1:nrow(fit$post_g), function(s)   lines(y0, fit$post_g[s,], col='gray')) # posterior draws lines(y0, colMeans(fit$post_g), lwd = 3) # posterior mean  # Add the true transformation, rescaled for easier comparisons: lines(y,       scale(dat$g_true)*sd(colMeans(fit$post_g)) + mean(colMeans(fit$post_g)), type='p', pch=2) legend('bottomright', c('Truth'), pch = 2) # annotate the true transformation   # Posterior predictive checks on testing data: empirical CDF y0 = sort(unique(y_test)) plot(y0, y0, type='n', ylim = c(0,1),      xlab='y', ylab='F_y', main = 'Posterior predictive ECDF') temp = sapply(1:nrow(fit$post_ypred), function(s)   lines(y0, ecdf(fit$post_ypred[s,])(y0), # ECDF of posterior predictive draws         col='gray', type ='s')) lines(y0, ecdf(y_test)(y0),  # ECDF of testing data      col='black', type = 's', lwd = 3)  # }"},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":null,"dir":"Reference","previous_headings":"","what":"Semiparametric Bayesian quantile regression â€” sbqr","title":"Semiparametric Bayesian quantile regression â€” sbqr","text":"MCMC sampling Bayesian quantile regression unknown (nonparametric) transformation. Like traditional Bayesian quantile regression, asymmetric Laplace distribution assumed errors, regression models targets specified quantile. However, models often woefully inadequate describing observed data. introduce nonparametric transformation improve model adequacy still providing inference regression coefficients specified quantile. g-prior assumed regression coefficients.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Semiparametric Bayesian quantile regression â€” sbqr","text":"","code":"sbqr(   y,   X,   tau = 0.5,   X_test = X,   psi = length(y),   laplace_approx = TRUE,   fixedX = TRUE,   approx_g = FALSE,   nsave = 1000,   nburn = 100,   ngrid = 100,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Semiparametric Bayesian quantile regression â€” sbqr","text":"y n x 1 response vector X n x p matrix predictors (intercept) tau target quantile (zero one) X_test n_test x p matrix predictors test data; default observed covariates X psi prior variance (g-prior) laplace_approx logical; TRUE, use normal approximation posterior definition transformation; otherwise prior used fixedX logical; TRUE, treat design fixed (non-random) sampling transformation; otherwise treat covariates random unknown distribution approx_g logical; TRUE, apply large-sample approximation transformation nsave number MCMC iterations save nburn number MCMC iterations discard ngrid number grid points inverse approximations verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Semiparametric Bayesian quantile regression â€” sbqr","text":"list following elements: coefficients posterior mean regression coefficients fitted.values estimated tauth quantile test points X_test post_theta: nsave x p samples posterior distribution regression coefficients post_ypred: nsave x n_test samples posterior predictive distribution test points X_test post_qtau: nsave x n_test samples tauth conditional quantile test points X_test post_g: nsave posterior samples transformation evaluated unique y values model: model fit (, sbqr) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Semiparametric Bayesian quantile regression â€” sbqr","text":"function provides fully Bayesian inference transformed quantile linear model. transformation modeled unknown learned jointly regression coefficients (unless approx_g = TRUE, uses point approximation). model applies real-valued data, positive data, compactly-supported data (support automatically deduced observed y values). results typically unchanged whether laplace_approx TRUE/FALSE; setting TRUE may reduce sensitivity prior, setting FALSE may speed computations large datasets. Similarly, treating covariates fixed (fixedX = TRUE) can substantially improve computing efficiency, make default.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Semiparametric Bayesian quantile regression â€” sbqr","text":"location (intercept) identified, intercepts X X_test removed. model-fitting ** include internal location-scale adjustment, function outputs inferential summaries identifiable parameters.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbqr.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Semiparametric Bayesian quantile regression â€” sbqr","text":"","code":"# \\donttest{ # Simulate some heteroskedastic data (no transformation): dat = simulate_tlm(n = 200, p = 10, g_type = 'box-cox', heterosked = TRUE, lambda = 1) y = dat$y; X = dat$X # training data y_test = dat$y_test; X_test = dat$X_test # testing data  # Target this quantile: tau = 0.05  # Fit the semiparametric Bayesian quantile regression model: fit = sbqr(y = y, X = X, tau = tau, X_test = X_test) #> [1] \"3 seconds remaining\" #> [1] \"Total time:  6 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_qtau\"     \"post_g\"        \"model\"         \"y\"             #>  [9] \"X\"             \"X_test\"        \"psi\"           \"approx_g\"      #> [13] \"tau\"            # Posterior predictive checks on testing data: empirical CDF y0 = sort(unique(y_test)) plot(y0, y0, type='n', ylim = c(0,1),      xlab='y', ylab='F_y', main = 'Posterior predictive ECDF') temp = sapply(1:nrow(fit$post_ypred), function(s)   lines(y0, ecdf(fit$post_ypred[s,])(y0), # ECDF of posterior predictive draws         col='gray', type ='s')) lines(y0, ecdf(y_test)(y0),  # ECDF of testing data      col='black', type = 's', lwd = 3)  # }"},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":null,"dir":"Reference","previous_headings":"","what":"Semiparametric Bayesian spline model â€” sbsm","title":"Semiparametric Bayesian spline model â€” sbsm","text":"Monte Carlo sampling Bayesian spline regression unknown (nonparametric) transformation. Cubic B-splines used prior penalizes roughness.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Semiparametric Bayesian spline model â€” sbsm","text":"","code":"sbsm(   y,   x = NULL,   x_test = NULL,   psi = NULL,   laplace_approx = TRUE,   fixedX = FALSE,   approx_g = FALSE,   nsave = 1000,   ngrid = 100,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Semiparametric Bayesian spline model â€” sbsm","text":"y n x 1 response vector x n x 1 vector observation points; NULL, assume equally-spaced [0,1] x_test n_test x 1 vector testing points; NULL, assume equal x psi prior variance (inverse smoothing parameter); NULL, sample parameter laplace_approx logical; TRUE, use normal approximation posterior definition transformation; otherwise prior used fixedX logical; TRUE, treat design fixed (non-random) sampling transformation; otherwise treat covariates random unknown distribution approx_g logical; TRUE, apply large-sample approximation transformation nsave number Monte Carlo simulations ngrid number grid points inverse approximations verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Semiparametric Bayesian spline model â€” sbsm","text":"list following elements: coefficients posterior mean regression coefficients fitted.values posterior predictive mean test points x_test post_theta: nsave x p samples posterior distribution regression coefficients post_ypred: nsave x n_test samples posterior predictive distribution x_test post_g: nsave posterior samples transformation evaluated unique y values model: model fit (, sbsm) well arguments passed .","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Semiparametric Bayesian spline model â€” sbsm","text":"function provides fully Bayesian inference transformed spline regression model using Monte Carlo (MCMC) sampling. transformation modeled unknown learned jointly regression function (unless approx_g = TRUE, uses point approximation). model applies real-valued data, positive data, compactly-supported data (support automatically deduced observed y values). results typically unchanged whether laplace_approx TRUE/FALSE; setting TRUE may reduce sensitivity prior, setting FALSE may speed computations large datasets.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sbsm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Semiparametric Bayesian spline model â€” sbsm","text":"","code":"# \\donttest{ # Simulate some data: n = 200 # sample size x = sort(runif(n)) # observation points  # Transform a noisy, periodic function: y = g_inv_bc(   sin(2*pi*x) + sin(4*pi*x) + rnorm(n),              lambda = .5) # Signed square-root transformation  # Fit the semiparametric Bayesian spline model: fit = sbsm(y = y, x = x) #> [1] \"2 seconds remaining\" #> [1] \"Total time:  4 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_g\"        \"post_psi\"      \"model\"         \"y\"             #>  [9] \"X\"             \"approx_g\"       # Note: this is Monte Carlo sampling...no need for MCMC diagnostics!  # Plot the model predictions (point and interval estimates): pi_y = t(apply(fit$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI plot(x, y, type='n', ylim = range(pi_y,y),      xlab = 'x', ylab = 'y', main = paste('Fitted values and prediction intervals')) polygon(c(x, rev(x)),c(pi_y[,2], rev(pi_y[,1])),col='gray', border=NA) lines(x, y, type='p') # observed points lines(x, fitted(fit), lwd = 3) # fitted curve   # Summarize the transformation: y0 = sort(unique(y)) # posterior draws of g are evaluated at the unique y observations plot(y0, fit$post_g[1,], type='n', ylim = range(fit$post_g),      xlab = 'y', ylab = 'g(y)', main = \"Posterior draws of the transformation\") temp = sapply(1:nrow(fit$post_g), function(s)   lines(y0, fit$post_g[s,], col='gray')) # posterior draws lines(y0, colMeans(fit$post_g), lwd = 3) # posterior mean lines(y, g_bc(y, 0.5), type='p', pch=2) # true transformation legend('bottomright', c('Truth'), pch = 2) # annotate the true transformation   # }"},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate a transformed linear model â€” simulate_tlm","title":"Simulate a transformed linear model â€” simulate_tlm","text":"Generate training data (X, y) testing data (X_test, y_test) transformed linear model. covariates correlated Gaussian variables. user-specified proportion (prop_sig) regression coefficients nonozero (= 1) rest zero. multiple options transformation, define support data (see ).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate a transformed linear model â€” simulate_tlm","text":"","code":"simulate_tlm(   n,   p,   g_type = \"beta\",   n_test = 1000,   heterosked = FALSE,   lambda = 1,   prop_sig = 0.5 )"},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate a transformed linear model â€” simulate_tlm","text":"n number observations training data p number covariates g_type type transformation; must one beta, step, box-cox n_test number observations testing data heterosked logical; TRUE, simulate latent data heteroskedasticity lambda Box-Cox parameter (applies g_type = 'box-cox') prop_sig proportion signals (nonzero coefficients)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate a transformed linear model â€” simulate_tlm","text":"list following elements: y: response variable training data X: covariates training data y_test: response variable testing data X_test: covariates testing data beta_true: true regression coefficients g_true: true transformation, evaluated y","code":""},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulate a transformed linear model â€” simulate_tlm","text":"transformations vary complexity support observed data, include following options: beta yields marginally Beta(0.1, 0.5) data supported [0,1]; step generates locally-linear inverse transformation produces positive data; box-cox refers signed Box-Cox family indexed lambda, generates real-valued data examples including identity, square-root, log transformations.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Simulate a transformed linear model â€” simulate_tlm","text":"design matrices X X_test include intercept intercept parameter beta_true. location/scale data identified general transformed regression models, recovering goal.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/simulate_tlm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate a transformed linear model â€” simulate_tlm","text":"","code":"# Simulate data: dat = simulate_tlm(n = 100, p = 5, g_type = 'beta') names(dat) # what is returned #> [1] \"y\"         \"X\"         \"y_test\"    \"X_test\"    \"beta_true\" \"g_true\"    hist(dat$y, breaks = 25) # marginal distribution"},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":null,"dir":"Reference","previous_headings":"","what":"Post-processing with importance sampling â€” sir_adjust","title":"Post-processing with importance sampling â€” sir_adjust","text":"Given Monte Carlo draws surrogate posterior, apply sampling importance reweighting (SIR) correct true model likelihood.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Post-processing with importance sampling â€” sir_adjust","text":"","code":"sir_adjust(   fit,   sir_frac = 0.3,   nsims_prior = 100,   marg_x = FALSE,   verbose = TRUE )"},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Post-processing with importance sampling â€” sir_adjust","text":"fit fitted model object includes coefficients posterior mean regression coefficients post_theta: nsave x p samples posterior distribution regression coefficients post_ypred: nsave x n_test samples posterior predictive distribution test points X_test post_g: nsave posterior samples transformation evaluated unique y values model: model fit (sblm sbsm) sir_frac fraction draws sample SIR nsims_prior number draws prior marg_x logical; TRUE, compute weights marginal covariates verbose logical; TRUE, print time remaining","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Post-processing with importance sampling â€” sir_adjust","text":"fitted model object posterior draws subsampled based SIR adjustment","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Post-processing with importance sampling â€” sir_adjust","text":"Monte Carlo sampling sblm sbsm uses surrogate likelihood posterior inference, enables much faster easier computing. SIR provides correction actual (specified) likelihood. However, correction step typically produce noticeable discrepancies, even small sample sizes.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Post-processing with importance sampling â€” sir_adjust","text":"SIR sampling done *without* replacement, sir_frac typically 0.1 0.5. nsims_priors draws used approximate prior expectation, larger values can significantly slow function. importance weights can computed conditionally (marg_x = FALSE) unconditionally (marg_x = TRUE) covariates, corresponding whether covariates marginalized likelihood. conditional version much faster.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/sir_adjust.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Post-processing with importance sampling â€” sir_adjust","text":"","code":"# \\donttest{ # Simulate some data: dat = simulate_tlm(n = 50, p = 5, g_type = 'step') y = dat$y; X = dat$X # training data y_test = dat$y_test; X_test = dat$X_test # testing data  hist(y, breaks = 10) # marginal distribution   # Fit the semiparametric Bayesian linear model: fit = sblm(y = y, X = X, X_test = X_test) #> [1] \"1 seconds remaining\" #> [1] \"Total time:  3 seconds\" names(fit) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_g\"        \"model\"         \"y\"             \"X\"             #>  [9] \"X_test\"        \"psi\"           \"approx_g\"       # Update with SIR: fit_sir = sir_adjust(fit) #> [1] \"0 seconds remaining\" #> [1] \"Total time:  1 seconds\" names(fit_sir) # what is returned #>  [1] \"coefficients\"  \"fitted.values\" \"post_theta\"    \"post_ypred\"    #>  [5] \"post_g\"        \"model\"         \"y\"             \"X\"             #>  [9] \"X_test\"        \"psi\"           \"approx_g\"       # Prediction: unadjusted vs. adjusted?  # Point estimates: y_hat = fitted(fit) y_hat_sir = fitted(fit_sir) cor(y_hat, y_hat_sir) # similar #> [1] 0.9991993  # Interval estimates: pi_y = t(apply(fit$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI pi_y_sir = t(apply(fit_sir$post_ypred, 2, quantile, c(0.05, .95))) # 90% PI  # PI overlap (%): overlaps = 100*sapply(1:length(y_test), function(i){   # innermost part   (min(pi_y[i,2], pi_y_sir[i,2]) - max(pi_y[i,1], pi_y_sir[i,1]))/     # outermost part     (max(pi_y[i,2], pi_y_sir[i,2]) - min(pi_y[i,1], pi_y_sir[i,1])) }) summary(overlaps) # mostly close to 100% #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's  #>    5.00   80.05   88.25   86.13   94.22  100.00       1   # Coverage of PIs on testing data (should be ~ 90%) mean((pi_y[,1] <= y_test)*(pi_y[,2] >= y_test)) # unadjusted #> [1] 0.903 mean((pi_y_sir[,1] <= y_test)*(pi_y_sir[,2] >= y_test)) # adjusted #> [1] 0.849  # Plot together with testing data: plot(y_test, y_test, type='n', ylim = range(pi_y, pi_y_sir, y_test),      xlab = 'y_test', ylab = 'y_hat', main = paste('Prediction intervals: testing data')) abline(0,1) # reference line suppressWarnings(   arrows(y_test, pi_y[,1], y_test, pi_y[,2],          length=0.15, angle=90, code=3, col='gray', lwd=2) ) # plot the PIs (unadjusted) suppressWarnings(   arrows(y_test, pi_y_sir[,1], y_test, pi_y_sir[,2],          length=0.15, angle=90, code=3, col='darkgray', lwd=2) ) # plot the PIs (adjusted) lines(y_test, y_hat, type='p', pch=2) # plot the means (unadjusted) lines(y_test, y_hat_sir, type='p', pch=3) # plot the means (adjusted)  # }"},{"path":"https://drkowal.github.io/SeBR/reference/square_stabilize.html","id":null,"dir":"Reference","previous_headings":"","what":"Numerically stabilize the squared elements â€” square_stabilize","title":"Numerically stabilize the squared elements â€” square_stabilize","text":"Given vector squared, add numeric buffer elements close zero.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/square_stabilize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Numerically stabilize the squared elements â€” square_stabilize","text":"","code":"square_stabilize(vec)"},{"path":"https://drkowal.github.io/SeBR/reference/square_stabilize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Numerically stabilize the squared elements â€” square_stabilize","text":"vec vector inputs squared","code":""},{"path":"https://drkowal.github.io/SeBR/reference/square_stabilize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Numerically stabilize the squared elements â€” square_stabilize","text":"vector length `vec`","code":""},{"path":"https://drkowal.github.io/SeBR/reference/uni.slice.html","id":null,"dir":"Reference","previous_headings":"","what":"Univariate Slice Sampler from Neal (2008) â€” uni.slice","title":"Univariate Slice Sampler from Neal (2008) â€” uni.slice","text":"Compute draw univariate distribution using code provided Radford M. Neal. documentation also reproduced Neal (2008).","code":""},{"path":"https://drkowal.github.io/SeBR/reference/uni.slice.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Univariate Slice Sampler from Neal (2008) â€” uni.slice","text":"","code":"uni.slice(x0, g, w = 1, m = Inf, lower = -Inf, upper = +Inf, gx0 = NULL)"},{"path":"https://drkowal.github.io/SeBR/reference/uni.slice.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Univariate Slice Sampler from Neal (2008) â€” uni.slice","text":"x0 Initial point g Function returning log probability density (plus constant) w Size steps creating interval (default 1) m Limit steps (default infinite) lower Lower bound support distribution (default -Inf) upper Upper bound support distribution (default +Inf) gx0 Value g(x0), known (default known)","code":""},{"path":"https://drkowal.github.io/SeBR/reference/uni.slice.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Univariate Slice Sampler from Neal (2008) â€” uni.slice","text":"point sampled, log density attached attribute.","code":""},{"path":"https://drkowal.github.io/SeBR/reference/uni.slice.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Univariate Slice Sampler from Neal (2008) â€” uni.slice","text":"log density function may return -Inf points outside support distribution.  lower /upper bound specified support, log density function called outside limits.","code":""},{"path":"https://drkowal.github.io/SeBR/news/index.html","id":"sebr-100","dir":"Changelog","previous_headings":"","what":"SeBR 1.0.0","title":"SeBR 1.0.0","text":"CRAN release: 2023-07-03 Initial CRAN submission.","code":""}]
